

# Welcome {-}


These notes by Léo Belzile (HEC Montréal) are licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).

This course is about statistical modelling.

A famous quote attributed to George Box claims that

> All models are wrong, but some are useful.

This standpoint is reductive: Peter McCullagh and John Nelder wrote in the preamble of their book (emphasis mine)

> Modelling in science remains, partly at least, an art. Some principles do exist, however, to guide the modeller. The first is that all models are wrong; **some, though, are better** than others and we can **search for the better ones**. At the same time we must recognize that eternal truth is not within our grasp.

And this quote by David R. Cox adds to the point:

> ...it does not seem helpful just to say that all models are wrong. The very word model implies simplification and idealization. The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd. The construction of idealized representations that **capture important stable aspects of such systems** is, however, a vital part of general scientific analysis and statistical models, especially substantive ones, do not seem essentially different from other kinds of model.


Why use models? [Paul Krugman wrote in 2010 in his blog](https://krugman.blogs.nytimes.com/2010/11/18/debt-deleveraging-and-the-liquidity-trap/)

> The answer I’d give is that models are an enormously important tool for clarifying your thought. You don’t have to literally believe your model — in fact, you’re a fool if you do — to believe that putting together a simplified but complete account of how things work, with all the eyes crossed and teas dotted or something, helps you gain a much more sophisticated understanding of the real situation. People who don’t use models end up relying on slogans that are much more simplistic than the models

## Course content


The purpose of statistical inference is to draw conclusions based on data. Scientific research relies on hypothesis testing: once an hypothesis is formulated, the researcher collects data, performs a test and concludes as to whether there is evidence for the proposed theory.

There are two main data type: **experimental** data are typically collected in a control environment following a research protocol with a particular experimental design: they serve to answer questions specified ahead of time. This approach is highly desirable to avoid the garden of forking paths [(researchers unfortunately tend to refine or change their hypothesis in light of data, which invalidates their findings](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf) --- preregistration alleviates this somewhat). While experimental data are highly desirable, it is not always possible to collect experimental data: for example, an economist cannot modify interest rates to see how it impacts consumer savings. When data have been collected beforehand without intervention (for other purposes), these are called **observational**. These will be the ones most frequently encountered.


A stochastic model will comprise two ingredients: a distribution for the random data and a formula linking the parameters or the conditional expectation of a response variable $Y$ to a set of explanatories $\mathbf{X}$. A model can serve to either predict new outcomes (predictive modelling) or else to test research hypothesis about the effect of the explanatory variables on the response (explanatory model). These two objectives are of course not mutually exclusive even if we distinguish in practice inference and prediction.

A predictive model gives predictions of $Y$ for different combinations of explanatory variables or future data. For example, one could try to forecast the enery consumption of a house as a function of weather, the number of inhabitants and its size. Black boxes used in machine learning are often used solely for prediction: these models are not easily interpreted and they often ignore the data structure.

By constrast, explicative models are often simple and interpretable: regression models are often used for inference purpose and we will focus on these. The following examples will be covered in class or as part of the exercices:

- Are sequential decisions in online shop (buying or not, then selecting the quantity) preferable to integrated decisions [@Duke.Amir:2023]?
- Determining what is the most distracting for road users: talking on a cellphone, texting or checking your smartwatch [@Brodeur:2021]?
- What is the impact of inconsistencies between product description and the displayed image [@Lee.Choi:2019]?
- Is the price of gasoline more expansive in the Gaspé peninsula than in the rest of Quebec? [A report of the *Régie de l'énergie* examines the question](http://www.regie-energie.qc.ca/energie/rapports/Rapport_priceGasp%C3%A9sie_20191219.pdf)
- Qu'est-ce qui explique que les prix de l'essence soient plus élevés en Gaspésie qu'ailleurs au Québec? [Un rapport de surveillance des prix de l'essence en Gaspésie par la Régie de l'énergie se penche sur la question.](http://www.regie-energie.qc.ca/energie/rapports/
Rapport_PrixGasp%C3%A9sie_20191219.pdf)
- Are driving tests in the UK easier if you live in a rural area? [An analysis of *The Guardian*](https://www.theguardian.com/world/2019/aug/23/an-easy-ride-scottish-village-fuels-debate-driving-test-pass-rates) hints that it is the case.
- What is the environmental perception of a package that includes cardboard over a plastic container [@Sokolova:2023]?
- What is the psychological impact of suggested amounts on donations [@Moon.VanEpps:2023]?
- What are the benefits of face-to-face meetings, rather than via videoconference tools? @Brucks.Levav:2022 suggests a decrease in the number of creative ideas and interactions when meeting online.
