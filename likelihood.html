<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>likelihood</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="likelihood_files/libs/clipboard/clipboard.min.js"></script>
<script src="likelihood_files/libs/quarto-html/quarto.js"></script>
<script src="likelihood_files/libs/quarto-html/popper.min.js"></script>
<script src="likelihood_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="likelihood_files/libs/quarto-html/anchor.min.js"></script>
<link href="likelihood_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="likelihood_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="likelihood_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="likelihood_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="likelihood_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="likelihood" class="level1">
<h1>Likelihood-based inference</h1>
<p>This chapter is dedicated to the basics of statistical modelling using likelihood-based inference, arguably the most popular estimation paradigm in statistics.</p>
<p>A statistical model starts with the specification of a data generating mechanism. We postulate that the data has been generated from a probability distribution with <span class="math inline">\(p\)</span>-dimensional parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>. The sample space is the set in which the <span class="math inline">\(n\)</span> vector observations lie, while the parameter space <span class="math inline">\(\boldsymbol{\Theta} \subseteq \mathbb{R}^p\)</span> is the set in which the parameter takes values.</p>
<p>As motivating example, consider the time a passenger must wait at the Edouard-Montpetit station if that person arrives at 17:59 sharp every weekday, just in time for the metro train. The measurements in <code>waiting</code> represent the time in seconds before the next train leaves the station. The data were collected over three months and can be treated as an independent sample. The left panel of <a href="#fig-waiting-hist" class="quarto-xref">Figure&nbsp;1</a> shows an histogram of the <span class="math inline">\(n=62\)</span> observations, which range from <span class="math inline">\(4\)</span> to <span class="math inline">\(57\)</span> seconds. The data are positive, so our model must account for this feature.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-waiting-hist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-waiting-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="likelihood_files/figure-html/fig-waiting-hist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-waiting-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Histogram of waiting time with rugs for the observations (left) and exponential log likelihood function for the waiting time, with the maximum likelihood estimate at dashed vertical line (right).
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-exponential-model" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Exponential model for waiting times)</strong></span> To model the waiting time, we may consider for example an exponential distribution with scale <span class="math inline">\(\lambda\)</span> <strong>?@def-exponentialdist</strong>, which represents the theoretical mean. Under independence<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, the joint density for the observations <span class="math inline">\(y_1, \ldots, y_n\)</span> is <span class="math display">\[\begin{align*}
f(\boldsymbol{y}) = \prod_{i=1}^n f(y_i) =\prod_{i=1}^n  \lambda^{-1} \exp(- y_i/\lambda) = \lambda^{-n} \exp\left(- \sum_{i=1}^n y_i/\lambda\right)
\end{align*}\]</span> The sample space is <span class="math inline">\(\mathbb{R}_{+}^n = [0, \infty)^n\)</span>, while the parameter space is <span class="math inline">\((0, \infty)\)</span>.</p>
</div>
<p>To estimate the scale parameter <span class="math inline">\(\lambda\)</span> and obtain suitable uncertainty measures, we need a modelling framework. We turn to likelihood-based inference.</p>
<section id="maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum likelihood estimation</h2>
<p>For any given value of <span class="math inline">\(\boldsymbol{\theta}\)</span>, we can obtain the probability mass or density of the sample observations, and we use this to derive an objective function for the estimation.</p>
<div id="def-likelihood" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Likelihood)</strong></span> The <strong>likelihood</strong> <span class="math inline">\(L(\boldsymbol{\theta})\)</span> is a function of the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> that gives the probability (or density) of observing a sample under a postulated distribution, treating the observations as fixed, <span class="math display">\[\begin{align*}
L(\boldsymbol{\theta}; \boldsymbol{y}) = f(\boldsymbol{y}; \boldsymbol{\theta}),
\end{align*}\]</span> where <span class="math inline">\(f(\boldsymbol{y}; \boldsymbol{\theta})\)</span> denotes the joint density or mass function of the <span class="math inline">\(n\)</span>-vector containing the observations.</p>
<p>If the latter are independent, the joint density factorizes as the product of the density of individual observations, and the likelihood becomes <span class="math display">\[\begin{align*}
L(\boldsymbol{\theta}; \boldsymbol{y})=\prod_{i=1}^n f_i(y_i; \boldsymbol{\theta}) = f_1(y_1; \boldsymbol{\theta}) \times \cdots \times f_n(y_n; \boldsymbol{\theta}).
\end{align*}\]</span> The corresponding log likelihood function for independent and identically distributions observations is <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}; \boldsymbol{y}) = \sum_{i=1}^n \log f(y_i; \boldsymbol{\theta})
\end{align*}\]</span></p>
</div>
<div id="exm-markov" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Dependent data)</strong></span> The joint density function only factorizes for independent data, but alternative sequential decompositions can be helpful. For example, we can write the joint density <span class="math inline">\(f(y_1, \ldots, y_n)\)</span> using the factorization <span class="math display">\[\begin{align*}
f(\boldsymbol{y}) = f(y_1) \times f(y_2 \mid y_1) \times \ldots f(y_n \mid y_1, \ldots, y_n)
\end{align*}\]</span> in terms of conditional. Such a decomposition is particularly useful in the context of time series, where data are ordered from time <span class="math inline">\(1\)</span> until time <span class="math inline">\(n\)</span> and models typically relate observation <span class="math inline">\(y_n\)</span> to it’s past. For example, the <span class="math inline">\(\textsc{ar}(1)\)</span> process, states that <span class="math inline">\(Y_t \mid Y_{t-1}=y_{t-1} \sim \mathsf{normal}(\alpha + \beta y_{t-1}, \sigma^2)\)</span> and we can simplify the log likelihood using the Markov property, which states that the current realization depends on the past, <span class="math inline">\(Y_t \mid Y_1, \ldots, Y_{t-1}\)</span>, only through the most recent value <span class="math inline">\(Y_{t-1}\)</span>. The log likelihood thus becomes <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \log f(y_1) + \sum_{i=2}^n f(y_i \mid y_{i-1}).
\end{align*}\]</span></p>
</div>
<div id="def-mle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (Maximum likelihood estimator)</strong></span> The <strong>maximum likelihood estimator</strong> <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the vector value that maximizes the likelihood, <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\theta}} = \mathrm{arg max}_{\boldsymbol{\theta} \in \boldsymbol{\Theta}} L(\boldsymbol{\theta}; \boldsymbol{y}).
\end{align*}\]</span></p>
<p>The natural logarithm <span class="math inline">\(\log\)</span> is a monotonic transformation, so the maximum likelihood estimator <span class="math inline">\(\boldsymbol{\theta}\)</span> for likelihood <span class="math inline">\(L(\boldsymbol{\theta}; \boldsymbol{y})\)</span> is the same as that of the log likelihood <span class="math inline">\(\ell(\boldsymbol{\theta}; \boldsymbol{y}) = \log L(\boldsymbol{\theta}; \boldsymbol{y})\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</div>
<p>If we suppose that our model is correct, than we expect to observe whatever was realized, so we find the parameter vector that makes the sample the most likely to have been generated by our model.</p>
<p>We can resort to numerical optimization routines to find the value of the maximum likelihood estimate, or sometimes derive closed-form expressions for the estimator, starting from the log likelihood. The right panel of <a href="#fig-waiting-hist" class="quarto-xref">Figure&nbsp;1</a> shows the exponential log likelihood, which attains a maximum at <span class="math inline">\(\widehat{\lambda}=28.935\)</span> second, the sample mean of the observations. The function decreases to either side of these values as the data become less compatible with the model. Given the values achieved here with a small sample, it is easy to see that direct optimization of the likelihood function (rather than it’s natural logarithm) could lead to numerical underflow, since already <span class="math inline">\(\exp(-270) \approx 5.5 \times 10^{-118}\)</span>, and log values smaller than <span class="math inline">\(-746\)</span> would be rounded to zero.</p>
<div id="exm-exponential-mle" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (Calculation of the maximum likelihood of an exponential distribution)</strong></span> As <a href="#fig-waiting-hist" class="quarto-xref">Figure&nbsp;1</a> reveals that the exponential log likelihood function is unimodal and thus achieves a single maximum, we can use calculus to derive an explicit expression for <span class="math inline">\(\widehat{\lambda}\)</span> based on the log likelihood <span class="math display">\[\begin{align*}
\ell(\lambda) = -n \log\lambda -\frac{1}{\lambda} \sum_{i=1}^n y_i.
\end{align*}\]</span> Taking first derivative and setting the result to zero, we find <span class="math display">\[\begin{align*}
\frac{\mathrm{d} \ell(\lambda)}{\mathrm{d} \lambda}  = -\frac{n}{\lambda} + \frac{1}{\lambda^2} \sum_{i=1}^n y_i = 0.
\end{align*}\]</span> Rearranging this expression by taking <span class="math inline">\(-n/\lambda\)</span> to the right hand side of the equality and multiplying both sides by <span class="math inline">\(\lambda^2&gt;0\)</span>, we find that <span class="math inline">\(\widehat{\lambda} = \sum_{i=1}^n y_i / n\)</span>. The second derivative of the log likelihood is <span class="math inline">\(\mathrm{d}^2 \ell(\lambda)/\mathrm{d} \lambda^2 = n(\lambda^{-2} - 2\lambda^{-3}\overline{y})\)</span>, and plugging <span class="math inline">\(\lambda = \overline{y}\)</span> gives <span class="math inline">\(-n/\overline{y}^2\)</span>, which is negative. Therefore, <span class="math inline">\(\widehat{\lambda}\)</span> is indeed a maximizer.</p>
</div>
<div id="prp-invariance-mle" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Invariance of maximum likelihood estimators)</strong></span> If <span class="math inline">\(g(\boldsymbol{\theta}): \mathbb{R}^p \mapsto \mathbb{R}^k\)</span> for <span class="math inline">\(k \leq p\)</span> is a function of the parameter vector, then <span class="math inline">\(g(\widehat{\boldsymbol{\theta}})\)</span> is the maximum likelihood estimator of the function.</p>
</div>
<p>The invariance property explains the widespread use of maximum likelihood estimation. For example, having estimated the parameter <span class="math inline">\(\lambda\)</span>, we can now use the model to derive other quantities of interest and get the “best” estimates for free. For example, we could compute the maximum likelihood estimate of the probability of waiting more than one minute, <span class="math inline">\(\Pr(T&gt;60) = \exp(-60/\widehat{\lambda})= 0.126\)</span>, or using <strong>R</strong> built-in distribution function <code>pexp</code>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: default R parametrization for the exponential is </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in terms of rate, i.e., the inverse scale parameter</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">pexp</span>(<span class="at">q =</span> <span class="dv">60</span>, <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">mean</span>(waiting), <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.126</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Another appeal of the invariance property is the possibility to compute the MLE in the most suitable parametrization, which is convenient if the support is restricted. If <span class="math inline">\(g\)</span> is a one-to-one function of <span class="math inline">\(\boldsymbol{\theta}\)</span>, for example if <span class="math inline">\(\theta &gt;0\)</span>, taking <span class="math inline">\(g(\theta) = \log \theta\)</span> or, if <span class="math inline">\(0 \leq \theta \leq 1\)</span>, by maximizing <span class="math inline">\(g(\theta) = \log(\theta) - \log(1-\theta) \in \mathbb{R}\)</span> removes the support constraints for the numerical optimization.</p>
<div id="def-information" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (Score and information matrix)</strong></span> Let <span class="math inline">\(\ell(\boldsymbol{\theta})\)</span>, <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta} \subseteq \mathbb{R}^p\)</span>, be the log likelihood function. The gradient of the log likelihood <span class="math inline">\(U(\boldsymbol{\theta}) = \partial \ell(\boldsymbol{\theta}) / \partial \boldsymbol{\theta}\)</span> is termed <strong>score</strong> function.</p>
<p>The <strong>observed information matrix</strong> is the hessian of the negative log likelihood <span class="math display">\[\begin{align*}
j(\boldsymbol{\theta}; \boldsymbol{y})=-\frac{\partial^2 \ell(\boldsymbol{\theta}; \boldsymbol{y})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}
\end{align*}\]</span>, evaluated at the maximum likelihood estimate <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>, so <span class="math inline">\(j(\widehat{\boldsymbol{\theta}})\)</span>. Under regularity conditions, the <strong>expected information</strong>, also called <strong>Fisher information</strong> matrix, is <span class="math display">\[\begin{align*}
i(\boldsymbol{\theta}) = \mathsf{E}\left\{U(\boldsymbol{\theta}; \boldsymbol{Y}) U(\boldsymbol{\theta}; \boldsymbol{Y})^\top\right\} = \mathsf{E}\left\{j(\boldsymbol{\theta}; \boldsymbol{Y})\right\}
\end{align*}\]</span> Both the Fisher (or expected) and the observed information matrices are symmetric and encode the curvature of the log likelihood and provide information about the variability of <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>.</p>
</div>
<div id="exm-exponential" class="theorem example">
<p><span class="theorem-title"><strong>Example 4 (Information for the exponential model)</strong></span> The observed and expected information of the exponential model for a random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, parametrized in terms of scale <span class="math inline">\(\lambda\)</span>, is <span class="math display">\[\begin{align*}
j(\lambda; \boldsymbol{y}) &amp;= -\frac{\partial^2 \ell(\lambda)}{\partial \lambda^2} = \frac{n}{\lambda^{2}} + \frac{2}{n\lambda^{3}}\sum_{i=1}^n y_i \\
i(\lambda) &amp;= \frac{n}{\lambda^{2}} + \frac{2}{n\lambda^{3}}\sum_{i=1}^n \mathsf{E}(Y_i)  = \frac{n}{\lambda^{2}}
\end{align*}\]</span> since <span class="math inline">\(\mathsf{E}(Y_i) = \lambda\)</span> and expectation is a linear operator. The observed information is <span class="math inline">\(j(\widehat{\lambda}) = n/\overline{y}^2\)</span> and the Fisher information is <span class="math inline">\({n}/{\lambda^{2}}\)</span>: both coincide when evaluated at the maximum likelihood estimator <span class="math inline">\(\widehat{\lambda}=\overline{y}\)</span> (i.e., the sample mean), but this isn’t the case in general.</p>
</div>
<div id="prp-gradient" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2 (Gradient-based optimization)</strong></span> To obtain the maximum likelihood estimator, we will typically find the value of the vector <span class="math inline">\(\boldsymbol{\theta}\)</span> that solves the score vector, meaning <span class="math inline">\(U(\widehat{\boldsymbol{\theta}})=\boldsymbol{0}_p\)</span>. This amounts to solving simultaneously a <span class="math inline">\(p\)</span>-system of equations by setting the derivative with respect to each element of <span class="math inline">\(\boldsymbol{\theta}\)</span> to zero. If <span class="math inline">\(j(\widehat{\boldsymbol{\theta}})\)</span> is a positive definite matrix (i.e., all of it’s eigenvalues are positive), then the vector <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the maximum likelihood estimator.</p>
<p>We can use a variant of Newton–Raphson algorithm if the likelihood is thrice differentiable and the maximum likelihood estimator does not lie on the boundary of the parameter space. If we consider an initial value <span class="math inline">\(\boldsymbol{\theta}^{\dagger}\)</span>, then a first order Taylor series expansion of the score likelihood in a neighborhood <span class="math inline">\(\boldsymbol{\theta}^{\dagger}\)</span> of the MLE <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> gives <span class="math display">\[\begin{align*}
\boldsymbol{0}_p &amp; = U(\widehat{\boldsymbol{\theta}}) \stackrel{\cdot}{\simeq} \left.
\frac{\partial \ell(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} \right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\dagger}} + \left.
\frac{\partial^2 \ell(\boldsymbol{\theta})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\top}\right|_{\boldsymbol{\theta} = \boldsymbol{\theta}^{\dagger}}(\widehat{\boldsymbol{\theta}} - \boldsymbol{\theta}^{\dagger})\\&amp;=U(\boldsymbol{\theta}^{\dagger}) - j(\boldsymbol{\theta}^{\dagger})(\widehat{\boldsymbol{\theta}} - \boldsymbol{\theta}^{\dagger})
\end{align*}\]</span> and solving this for <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> (provided the <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(j(\widehat{\boldsymbol{\theta}})\)</span> is invertible), we get <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\theta}} \stackrel{\cdot}{\simeq} \boldsymbol{\theta}^{\dagger} +
j^{-1}(\boldsymbol{\theta}^{\dagger})U(\boldsymbol{\theta}^{\dagger}),
\end{align*}\]</span> which suggests an iterative procedure from a starting value <span class="math inline">\(\boldsymbol{\theta}^{\dagger}\)</span> in the vicinity of the mode until the gradient is approximately zero. If the value is far from the mode, then the algorithm may diverge to infinity. To avoid this, we may multiply the term <span class="math inline">\(j^{-1}(\boldsymbol{\theta}^{\dagger})U(\boldsymbol{\theta}^{\dagger})\)</span> by a damping factor <span class="math inline">\(c&lt;1\)</span>. A variant of the algorithm, termed Fisher scoring, uses the expected or Fisher information <span class="math inline">\(i(\boldsymbol{\theta})\)</span> in place of the observed information, <span class="math inline">\(j(\boldsymbol{\theta})\)</span>, for numerical stability and to avoid situations where the latter is not positive definite. This is the optimization routine used in the <code>glm</code> function in <strong>R</strong>.</p>
</div>
<!-- Similar quantities can be obtained via Monte Carlo methods by simulation from the model, or through analytical derivations.  -->
<p>The exponential model may be restrictive for our purposes, so we consider for the purpose of illustration and as a generalization a Weibull distribution.</p>
<div id="def-weibull" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (Weibull distribution)</strong></span> The distribution function of a <strong>Weibull</strong> random variable with scale <span class="math inline">\(\lambda&gt;0\)</span> and shape <span class="math inline">\(\alpha&gt;0\)</span> is <span class="math display">\[\begin{align*}
F(x; \lambda, \alpha) &amp;= 1 − \exp\left\{−(x/\lambda)^\alpha\right\}, \qquad x \geq 0, \lambda&gt;0, \alpha&gt;0,
\end{align*}\]</span> while the corresponding density is <span class="math display">\[\begin{align*}
f(x; \lambda, \alpha) &amp;= \frac{\alpha}{\lambda^\alpha} x^{\alpha-1}\exp\left\{−(x/\lambda)^\alpha\right\}, \qquad x \geq 0, \lambda&gt;0, \alpha&gt;0.
\end{align*}\]</span> The quantile function, the inverse of the distribution function, is <span class="math inline">\(Q(p) = \lambda\{-\log(1-p)\}^{1/\alpha}\)</span>. The Weibull distribution includes the exponential as special case when <span class="math inline">\(\alpha=1\)</span>. The expected value of <span class="math inline">\(Y \sim \mathsf{Weibull}(\lambda, \alpha)\)</span> is <span class="math inline">\(\mathsf{E}(Y) = \lambda \Gamma(1+1/\alpha)\)</span>.</p>
</div>
<div id="exm-weibull-mle" class="theorem example">
<p><span class="theorem-title"><strong>Example 5 (Maximum likelihood of a Weibull sample)</strong></span> The log likelihood for a simple random sample whose realizations are <span class="math inline">\(y_1, \ldots, y_n\)</span> of size <span class="math inline">\(n\)</span> from a <span class="math inline">\(\mathsf{Weibull}(\lambda, \alpha)\)</span> model is <span class="math display">\[\begin{align*}
\ell(\lambda, \alpha) = n \log(\alpha) - n\alpha\log(\lambda) + (\alpha-1) \sum_{i=1}^n \log y_i  - \lambda^{-\alpha}\sum_{i=1}^n y_i^\alpha.
\end{align*}\]</span> The gradient of this function is easily obtained by differentiation<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math display">\[\begin{align*}
\frac{\partial \ell(\lambda, \alpha)}{\partial \lambda} &amp;= -\frac{n\alpha}{\lambda} +\alpha\lambda^{-\alpha-1}\sum_{i=1}^n y_i^\alpha \\
\frac{\partial \ell(\lambda, \alpha)}{\partial \alpha} &amp;= \frac{n}{\alpha} - n \log(\lambda) + \sum_{i=1}^n \log y_i + \lambda^{-\alpha}\log(\alpha)\sum_{i=1}^n y_i^\alpha - \sum_{i=1}^n \left(\frac{y_i}{\lambda}\right)^{\alpha} \times\log\left(\frac{y_i}{\lambda}\right).
\end{align*}\]</span> We turn to numerical optimization to obtain the maximum likelihood estimate of the Weibull distribution, in the absence of closed-form expression for the MLE. To this end, we create functions that encode the log likelihood, here taken as the sum of log density contributions. The function <code>nll_weibull</code> below takes as argument the vector of parameters, <code>pars</code>, and returns the negative of the log likelihood which we wish to minimize<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> We also code the gradient, although we can resort to numerical differentiation at little additional costs. We then use <code>optim</code>, the default optimization routine in <strong>R</strong>, to minimize <code>nll_weibull</code>. The function returns a list containing a convergence code (<code>0</code> indicating convergence), the MLE in <code>par</code>, the log likelihood <span class="math inline">\(\ell(\widehat{\boldsymbol{\theta}})\)</span> and the Hessian matrix, which is the matrix of second derivatives of the negative log likelihood evaluated at <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>. The log likelihood surface, for pairs of scale and shape vectors <span class="math inline">\(\boldsymbol{\theta} = (\lambda, \alpha)\)</span>, are displayed in <a href="#fig-weibull-loglik-surf" class="quarto-xref">Figure&nbsp;2</a>. We can see that the maximum likelihood value has converged, and check that the score satisfies <span class="math inline">\(U(\widehat{\boldsymbol{\theta}}) = 0\)</span> at the returned optimum value.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data vector</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(waiting, <span class="at">package =</span> <span class="st">"hecstatmod"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Negative log likelihood for a Weibull sample</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>nll_weibull <span class="ot">&lt;-</span> <span class="cf">function</span>(pars, y){</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Handle the case of negative parameter values</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">isTRUE</span>(<span class="fu">any</span>(pars <span class="sc">&lt;=</span> <span class="dv">0</span>))){ <span class="co"># parameters must be positive</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fl">1e10</span>) <span class="co"># large value (not infinite, to avoid warning messages)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dweibull</span>(<span class="at">x =</span> y, <span class="at">scale =</span> pars[<span class="dv">1</span>], <span class="at">shape =</span> pars[<span class="dv">2</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of the negative Weibull log likelihood</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>gr_nll_weibull <span class="ot">&lt;-</span> <span class="cf">function</span>(pars, y){</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  scale <span class="ot">&lt;-</span> pars[<span class="dv">1</span>] </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  shape <span class="ot">&lt;-</span> pars[<span class="dv">2</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  grad_ll <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="at">scale =</span> <span class="sc">-</span>n<span class="sc">*</span>shape<span class="sc">/</span>scale <span class="sc">+</span> shape<span class="sc">*</span>scale<span class="sc">^</span>(<span class="sc">-</span>shape<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">sum</span>(y<span class="sc">^</span>shape),</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>               <span class="at">shape =</span> n<span class="sc">/</span>shape <span class="sc">-</span> n<span class="sc">*</span><span class="fu">log</span>(scale) <span class="sc">+</span> <span class="fu">sum</span>(<span class="fu">log</span>(y)) <span class="sc">-</span> </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">sum</span>(<span class="fu">log</span>(y<span class="sc">/</span>scale)<span class="sc">*</span>(y<span class="sc">/</span>scale)<span class="sc">^</span>shape))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span> grad_ll)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Use exponential submodel MLE as starting parameters</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>start <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(waiting), <span class="dv">1</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Check gradient function is correctly coded! </span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Returns TRUE if numerically equal to tolerance</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="fu">isTRUE</span>(<span class="fu">all.equal</span>(numDeriv<span class="sc">::</span><span class="fu">grad</span>(nll_weibull, <span class="at">x =</span> start, <span class="at">y =</span> waiting),</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">gr_nll_weibull</span>(<span class="at">pars =</span> start, <span class="at">y =</span> waiting),</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>                 <span class="at">check.attributes =</span> <span class="cn">FALSE</span>))</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] TRUE</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Numerical minimization using optim</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>opt_weibull <span class="ot">&lt;-</span> <span class="fu">optim</span>(</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">par =</span> start,  <span class="co"># starting values</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">fn =</span> nll_weibull,  <span class="co"># pass function, whose first argument is the parameter vector</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">gr =</span> gr_nll_weibull, <span class="co"># optional (if missing, numerical derivative)</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"BFGS"</span>, <span class="co"># gradient-based algorithm, common alternative is "Nelder"</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> waiting, <span class="co"># vector of observations, passed as additional argument to fn</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">hessian =</span> <span class="cn">TRUE</span>) <span class="co"># return matrix of second derivatives evaluated at MLE</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative using pure Newton</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># nlm(f = nll_weibull, p = start, hessian = TRUE, y = waiting)</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameter estimates - MLE</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>(mle_weibull <span class="ot">&lt;-</span> opt_weibull<span class="sc">$</span>par)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 32.6  2.6</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Check gradient for convergence</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="fu">gr_nll_weibull</span>(mle_weibull, <span class="at">y =</span> waiting)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     scale     shape </span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.0000142 0.0001136</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Is the Hessian of the negative positive definite (all eigenvalues are positive)</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co"># If so, we found a maximum and the matrix is invertible</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="fu">isTRUE</span>(<span class="fu">all</span>(<span class="fu">eigen</span>(opt_weibull<span class="sc">$</span>hessian)<span class="sc">$</span>values <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] TRUE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-weibull-loglik-surf" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weibull-loglik-surf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="likelihood_files/figure-html/fig-weibull-loglik-surf-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weibull-loglik-surf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Log likelhood surface for the Weibull model applied to the <code>waiting</code> data, with white contour curves given 10%, 20%, , 90% confidence sets. Higher log likelihood values are indicated by darker colors. The cross indicates the maximum likelihood estimate.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="sampling-distribution" class="level2">
<h2 class="anchored" data-anchor-id="sampling-distribution">Sampling distribution</h2>
<p>The <strong>sampling distribution</strong> of an estimator <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the probability distribution induced by the underlying data, given that the latter inputs are random.</p>
<p>For simplicity, suppose we have a simple random sample, so the log likelihood is a sum of <span class="math inline">\(n\)</span> terms and information accumulates linearly with the sample size: the data carry more information about the unknown parameter vector, whose true value we denote <span class="math inline">\(\boldsymbol{\theta}_0\)</span>. Under suitable regularity conditions, cf.&nbsp;Section 4.4.2 of <span class="citation" data-cites="Davison:2003">@Davison:2003</span>, for large sample size <span class="math inline">\(n\)</span>, we can perform a Taylor series of the score vector and apply the central limit theorem. Since <span class="math inline">\(U(\boldsymbol{\theta})\)</span> and <span class="math inline">\(i(\boldsymbol{\theta})\)</span> are the sum of <span class="math inline">\(n\)</span> independent random variables, and that <span class="math inline">\(\mathsf{E}\{U(\boldsymbol{\theta})\}=\boldsymbol{0}_p\)</span>, and <span class="math inline">\(\mathsf{Var}\{U(\boldsymbol{\theta})\}=i(\boldsymbol{\theta})\)</span>, application of the central limit theorem yields <span class="math inline">\(i(\boldsymbol{\theta}_0)^{-1/2}U(\boldsymbol{\theta}_0) \stackrel{\cdot}{\sim}\mathsf{normal}(0,1)\)</span>. We can use this to obtain approximations to the sampling distribution of <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> and also for the log likelihood ratio <span class="math display">\[\begin{align*}
R(\boldsymbol{\theta}_0)=2\{\ell(\widehat{\boldsymbol{\theta}}) - \ell(\boldsymbol{\theta}_0)\}
\end{align*}\]</span> where <span class="math inline">\(R(\boldsymbol{\theta}_0) \stackrel{\cdot}{\sim} \chi^2_p\)</span>.</p>
<p>As the sample size grows, the <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> becomes centered around the value <span class="math inline">\(\boldsymbol{\theta}_0\)</span> that minimizes the discrepancy between the model and the true data generating process. In large samples, the MLE estimator is approximately multivariate normal with</p>
<p><span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\theta}} \stackrel{\cdot}{\sim} \mathsf{normal}_p\{\boldsymbol{\theta}_0, i^{-1}(\boldsymbol{\theta})\}
\end{align*}\]</span> where the covariance matrix is the inverse of the Fisher information. In practice, since the true parameter value <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is unknown, we replace it with either <span class="math inline">\(i^{-1}(\widehat{\boldsymbol{\theta}})\)</span> or the inverse of the observed information <span class="math inline">\(j^{-1}(\widehat{\boldsymbol{\theta}})\)</span>, as both of these converge to the true value.</p>
<div id="exm-Weibull-se" class="theorem example">
<p><span class="theorem-title"><strong>Example 6 (Covariance matrix and standard errors for the Weibull distribution)</strong></span> We use the output of our optimization procedure to get the observed information matrix and the standard errors for the parameters of the Weibull model. The latter are simply the square root of the diagonal entries of the inverse Hessian matrix.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The Hessian matrix of the negative log likelihood evaluated at the MLE</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># is the inverse of the observed information matrix</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>obsinfo_weibull <span class="ot">&lt;-</span> opt_weibull<span class="sc">$</span>hessian</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>vmat_weibull <span class="ot">&lt;-</span> <span class="fu">solve</span>(obsinfo_weibull)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard errors</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>se_weibull <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(vmat_weibull))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>We can use this result to construct confidence intervals for parameters from <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<div id="prp-transformation" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3 (Asymptotic normality and transformations)</strong></span> The asymptotic normality result can be used to derive standard errors for other quantities of interest. If <span class="math inline">\(\phi = g(\boldsymbol{\theta})\)</span>, then <span class="math inline">\(\widehat{\phi} \stackrel{\cdot}{\sim}\mathsf{normal}(\phi_0, \mathrm{V}_\phi)\)</span>, with <span class="math inline">\(\mathrm{V}_\phi = \nabla \phi^\top \mathbf{V}_{\boldsymbol{\theta}} \nabla \phi\)</span>, where <span class="math inline">\(\nabla \phi=[\partial \phi/\partial \theta_1, \ldots, \partial \phi/\partial \theta_p]^\top\)</span>. The variance matrix and the gradient vector are evaluated at the maximum likelihood estimate <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>. This result readily extends to vector <span class="math inline">\(\boldsymbol{\phi} \in \mathbb{R}^k\)</span> for <span class="math inline">\(k \leq p\)</span>.</p>
</div>
<div id="exm-transformation-exp" class="theorem example">
<p><span class="theorem-title"><strong>Example 7 (Probability of waiting for exponential model.)</strong></span> To illustrate the difference between likelihood ratio and Wald tests (and their respective confidence intervals), we consider the metro waiting time data and consider the probability of waiting more than one minute, <span class="math inline">\(\phi=g(\lambda) = \exp(-60/\lambda)\)</span>. The maximum likelihood estimate is, by invariance, <span class="math inline">\(0.126\)</span> and the gradient of <span class="math inline">\(g\)</span> with respect to the scale parameter is <span class="math inline">\(\nabla \phi = \partial \phi / \partial \lambda = 60\exp(-60/\lambda)/\lambda^2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>lambda_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(waiting)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>phi_hat <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">60</span><span class="sc">/</span>lambda_hat)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>dphi <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda){<span class="dv">60</span><span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">60</span><span class="sc">/</span>lambda)<span class="sc">/</span>(lambda<span class="sc">^</span><span class="dv">2</span>)}</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>V_lambda <span class="ot">&lt;-</span> lambda_hat<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="fu">length</span>(waiting)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>V_phi <span class="ot">&lt;-</span> <span class="fu">dphi</span>(lambda_hat)<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> V_lambda</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>(se_phi <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(V_phi))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.0331</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="liktests" class="level2">
<h2 class="anchored" data-anchor-id="liktests">Likelihood-based tests</h2>
<p>Oftentimes, we wish to compare two models: the model implied by the null hypothesis, which is a restriction or simpler version of the full model. Models are said to be <strong>nested</strong> if we can obtain one from the other by imposing restrictions on the parameters.</p>
<p>We consider a null hypothesis <span class="math inline">\(\mathscr{H}_0\)</span> that imposes restrictions on the possible values of <span class="math inline">\(\boldsymbol{\theta}\)</span> can take, relative to an unconstrained alternative <span class="math inline">\(\mathscr{H}_1\)</span>. We need two <strong>nested</strong> models: a <em>full</em> model, and a <em>reduced</em> model that is a subset of the full model where we impose <span class="math inline">\(q\)</span> restrictions. For example, the full model could be a regression model with four predictor variables and the reduced model could include only the first two predictor variables, which is equivalent to setting <span class="math inline">\(\mathscr{H}_0: \beta_3=\beta_4=0\)</span>. The testing procedure involves fitting the two models and obtaining the maximum likelihood estimators of each of <span class="math inline">\(\mathscr{H}_1\)</span> and <span class="math inline">\(\mathscr{H}_0\)</span>, respectively <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\theta}}_0\)</span> for the parameters under <span class="math inline">\(\mathscr{H}_0\)</span>. The null hypothesis <span class="math inline">\(\mathscr{H}_0\)</span> tested is: `the reduced model is an <strong>adequate simplification</strong> of the full model’ and the likelihood provides three main classes of statistics for testing this hypothesis: these are</p>
<ul>
<li>likelihood ratio tests statistics, denoted <span class="math inline">\(R\)</span>, which measure the drop in log likelihood (vertical distance) from <span class="math inline">\(\ell(\widehat{\boldsymbol{\theta}})\)</span> and <span class="math inline">\(\ell(\widehat{\boldsymbol{\theta}}_0)\)</span>.</li>
<li>Wald tests statistics, denoted <span class="math inline">\(W\)</span>, which consider the standardized horizontal distance between <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\theta}}_0\)</span>.</li>
<li>score tests statistics, denoted <span class="math inline">\(S\)</span>, which looks at the scaled gradient of <span class="math inline">\(\ell\)</span>, evaluated <em>only</em> at <span class="math inline">\(\widehat{\boldsymbol{\theta}}_0\)</span> (derivative of <span class="math inline">\(\ell\)</span>).</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/likelihood_tests.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Log-likelihood curve: the three likelihood-based tests, namely Wald, likelihood ratio and score tests, are shown on the curve. The tests use different information about the function.</figcaption>
</figure>
</div>
</div>
</div>
<p>The three main classes of statistics for testing a simple null hypothesis <span class="math inline">\(\mathscr{H}_0: \boldsymbol{\theta}=\boldsymbol{\theta}_0\)</span> against the alternative <span class="math inline">\(\mathscr{H}_a: \boldsymbol{\theta} \neq \boldsymbol{\theta}_0\)</span> are the likelihood ratio, the score and the Wald statistics, defined respectively as <span class="math display">\[\begin{align*}
R &amp;= 2 \left\{ \ell(\widehat{\boldsymbol{\theta}})-\ell(\boldsymbol{\theta}_0)\right\}, \\
S &amp;= U^\top(\boldsymbol{\theta}_0)i^{-1}(\boldsymbol{\theta}_0)U(\boldsymbol{\theta}_0), \\
W &amp;= (\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}_0)^\top j(\widehat{\boldsymbol{\theta}})(\widehat{\boldsymbol{\theta}}-\boldsymbol{\theta}_0),
\end{align*}\]</span> where <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span> is the maximum likelihood estimate under the alternative and <span class="math inline">\(\boldsymbol{\theta}_0\)</span> is the null value of the parameter vector. Asymptotically, all the test statistics are equivalent (in the sense that they lead to the same conclusions about <span class="math inline">\(\mathscr{H}_0\)</span>). If <span class="math inline">\(\mathscr{H}_0\)</span> is true, the three test statistics follow asymptotically a <span class="math inline">\(\chi^2_q\)</span> distribution under a null hypothesis <span class="math inline">\(\mathscr{H}_0\)</span>, where the degrees of freedom <span class="math inline">\(q\)</span> are the number of restrictions.</p>
<p>For scalar <span class="math inline">\(\theta\)</span> with <span class="math inline">\(q=1\)</span>, signed versions of these statistics exist, e.g., <span class="math display">\[\begin{align*}
W(\theta_0)=(\widehat{\theta}-\theta_0)/\mathsf{se}(\widehat{\theta})\stackrel{\cdot}{\sim} \mathsf{normal}(0,1)
\end{align*}\]</span> for the Wald statistic or the directed likelihood root <span class="math display">\[\begin{align*}
R({\theta_0}) = \mathrm{sign}(\widehat{\theta}-\theta)\left[2
\left\{\ell(\widehat{\theta})-\ell(\theta)\right\}\right]^{1/2} \stackrel{\cdot}{\sim} \mathsf{normal}(0,1).
\end{align*}\]</span> The likelihood ratio test statistic is normally the most powerful of the three likelihood tests. The score statistic <span class="math inline">\(S\)</span> only requires calculation of the score and information under <span class="math inline">\(\mathscr{H}_0\)</span> (because by definition <span class="math inline">\(U(\widehat{\theta})=0\)</span>), so it can be useful in problems where calculations of the maximum likelihood estimator under the alternative is costly or impossible.</p>
<p>The Wald statistic <span class="math inline">\(W\)</span> is the most widely encountered statistic and two-sided 95% confidence intervals for a single parameter <span class="math inline">\(\theta\)</span> are of the form <span class="math display">\[\begin{align*}
\widehat{\theta} \pm \mathfrak{z}_{1-\alpha/2}\mathrm{se}(\widehat{\theta}),
\end{align*}\]</span> where <span class="math inline">\(\mathfrak{z}_{1-\alpha/2}\)</span> is the <span class="math inline">\(1-\alpha/2\)</span> quantile of the standard normal distribution; for a <span class="math inline">\(95\)</span>% confidence interval, the <span class="math inline">\(0.975\)</span> quantile of the normal distribution is <span class="math inline">\(\mathfrak{z}_{0.975}=1.96\)</span>. The Wald-based confidence intervals are by construction <strong>symmetric</strong>: they may include implausible values (e.g., negative values for if the parameter of interest <span class="math inline">\(\theta\)</span> is positive, such as variances). The Wald-based confidence intervals are not parametrization invariant: if we want intervals for a nonlinear continuous function <span class="math inline">\(g(\theta)\)</span>, then in general <span class="math inline">\(\mathsf{CI}_{W}\{g(\theta)\} \neq g\{\mathsf{CI}_{W}(\theta)\}.\)</span></p>
<p>These confidence intervals can be contrasted with the (better) ones derived using the likelihood ratio test: these are found through a numerical search to find the limits of <span class="math display">\[\begin{align*}
\theta: 2\{\ell(\widehat{\theta}) - \ell(\theta)\} \leq \chi^2_1(1-\alpha),
\end{align*}\]</span> where <span class="math inline">\(\chi^2_1(1-\alpha)\)</span> is the <span class="math inline">\((1-\alpha)\)</span> quantile of the <span class="math inline">\(\chi^2_1\)</span> distribution. If <span class="math inline">\(\boldsymbol{\theta}\)</span> is multidimensional, confidence intervals for <span class="math inline">\(\theta_i\)</span> are derived using the profile likelihood. Likelihood ratio-based confidence intervals are <strong>parametrization invariant</strong>, so <span class="math inline">\(\mathsf{CI}_{R}\{g(\theta)\} = g\{\mathsf{CI}_{R}(\theta)\}\)</span>. Because the likelihood is zero if a parameter value falls outside the range of possible values for the parameter, the intervals only include plausible values of <span class="math inline">\(\theta\)</span>. In general, the intervals are asymmetric and have better coverage properties.</p>
<p>We can invert the Wald test statistic to get a symmetric 95% confidence interval for <span class="math inline">\(\phi\)</span>, <span class="math inline">\([0.061\)</span>, <span class="math inline">\(0.191]\)</span>. If we were to naively transform the confidence interval for <span class="math inline">\(\lambda\)</span> into one for <span class="math inline">\(\phi\)</span>, we would get <span class="math inline">\([0.063\)</span>, <span class="math inline">\(0.19]\)</span>, which highlights the invariance although the difference here is subtle. The Gaussian approximation underlying the Wald test is reliable if the sampling distribution of the likelihood is near quadratic, which happens when the likelihood function is roughly symmetric on either side of the maximum likelihood estimator.</p>
<p>By contrast, the likelihood ratio test is invariant to interest-preserving reparametrizations, so the test statistic for <span class="math inline">\(\mathscr{H}_0: \phi=\phi_0\)</span> and <span class="math inline">\(\mathscr{H}_0: \lambda = -60/\log(\phi_0)\)</span> are the same.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>ll_exp <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda){<span class="fu">sum</span>(<span class="fu">dexp</span>(waiting, <span class="at">rate =</span> <span class="dv">1</span><span class="sc">/</span>lambda, <span class="at">log =</span> <span class="cn">TRUE</span>))}</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>lambda_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(waiting)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>lrt_lb <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(<span class="at">f =</span> <span class="cf">function</span>(r){<span class="dv">2</span><span class="sc">*</span>(<span class="fu">ll_exp</span>(lambda_hat) <span class="sc">-</span> <span class="fu">ll_exp</span>(r)) <span class="sc">-</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="dv">1</span>)}, </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">interval =</span> <span class="fu">c</span>(<span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">min</span>(waiting), <span class="fu">mean</span>(waiting)))<span class="sc">$</span>root</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>lrt_ub <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(<span class="at">f =</span> <span class="cf">function</span>(r){<span class="dv">2</span><span class="sc">*</span>(<span class="fu">ll_exp</span>(lambda_hat) <span class="sc">-</span> <span class="fu">ll_exp</span>(r)) <span class="sc">-</span> <span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="dv">1</span>)}, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">interval =</span> <span class="fu">c</span>(<span class="dv">2</span> <span class="sc">*</span> <span class="fu">max</span>(waiting), <span class="fu">mean</span>(waiting)))<span class="sc">$</span>root</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The likelihood ratio statistic 95% confidence interval for <span class="math inline">\(\phi\)</span> can be found by using a root finding algorithm: the 95% confidence interval for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\([22.784\)</span>, <span class="math inline">\(37.515]\)</span>. By invariance, the 95% confidence interval for <span class="math inline">\(\phi\)</span> is <span class="math inline">\([0.072, 0.202]\)</span>.</p>
</section>
<section id="profile-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="profile-likelihood">Profile likelihood</h2>
<p>Sometimes, we may want to perform hypothesis test or derive confidence intervals for selected components of the model. For example, we may be interested in obtaining confidence intervals for a single <span class="math inline">\(\beta_j\)</span> in a logistic regression, treating the other parameters <span class="math inline">\(\boldsymbol{\beta}_{-j}\)</span> as nuisance In this case, the null hypothesis only restricts part of the space and the other parameters, termed nuisance, are left unspecified — the question then is what values to use for comparison with the full model. It turns out that the values that maximize the constrained log likelihood are what one should use for the test, and the particular function in which these nuisance parameters are integrated out is termed a profile likelihood.</p>
<p>Consider a parametric model with log likelihood function <span class="math inline">\(\ell(\boldsymbol{\theta})\)</span> whose <span class="math inline">\(p\)</span>-dimensional parameter vector <span class="math inline">\(\boldsymbol{\theta}=(\boldsymbol{\psi}, \boldsymbol{\lambda})\)</span> can be decomposed into a <span class="math inline">\(q\)</span>-dimensional parameter of interest <span class="math inline">\(\boldsymbol{\psi}\)</span> and a <span class="math inline">\((p-q)\)</span>-dimensional nuisance vector <span class="math inline">\(\boldsymbol{\lambda}\)</span>.</p>
<!-- The score vector, the information matrix and its inverse are partitioned accordingly as -->
<!-- \begin{align*} -->
<!-- U(\boldsymbol{\theta})=\ell_{\boldsymbol{\theta}} = \begin{pmatrix} -->
<!-- \ell_{\boldsymbol{\psi}} \\ \ell_{\boldsymbol{\lambda}} -->
<!-- \end{pmatrix}, \qquad  -->
<!-- i(\boldsymbol{\theta}) = \begin{pmatrix} -->
<!-- i_{\boldsymbol{\psi\psi}} & i_{\boldsymbol{\psi\lambda}}\\ -->
<!-- i_{\boldsymbol{\lambda\psi}} & i_{\boldsymbol{\lambda\lambda}}\\ -->
<!-- \end{pmatrix}, \qquad  -->
<!-- i^{-1}(\boldsymbol{\theta}) = \begin{pmatrix} -->
<!-- i^{\boldsymbol{\psi\psi}} & i^{\boldsymbol{\psi\lambda}}\\ -->
<!-- i^{\boldsymbol{\lambda\psi}} & i^{\boldsymbol{\lambda\lambda}}\\ -->
<!-- \end{pmatrix}.       -->
<!-- \end{align*} -->
<p>We can consider the profile likelihood <span class="math inline">\(\ell_{\mathsf{p}}\)</span>, a function of <span class="math inline">\(\boldsymbol{\psi}\)</span> alone, which is obtained by maximizing the likelihood pointwise at each fixed value <span class="math inline">\(\boldsymbol{\psi}_0\)</span> over the nuisance vector <span class="math inline">\(\boldsymbol{\varphi}_{\psi_0}\)</span>, <span class="math display">\[\begin{align*}
\ell_{\mathsf{p}}(\boldsymbol{\psi})=\max_{\boldsymbol{\varphi}}\ell(\boldsymbol{\psi}, \boldsymbol{\varphi})=\ell(\boldsymbol{\psi}, \widehat{\boldsymbol{\varphi}}_{\boldsymbol{\psi}}).
\end{align*}\]</span> Figure @ref(fig:profile3d) shows a fictional log likelihood contour plot with the resulting profile curve (in black), where the log likelihood value is mapped to colors. If one thinks of these contours lines as those of a topographic map, the profile likelihood corresponds in this case to walking along the ridge of both mountains along the <span class="math inline">\(\psi\)</span> direction, with the right panel showing the elevation gain/loss.</p>
<p>The maximum profile likelihood estimator behaves like a regular likelihood for most quantities of interest and we can derive test statistics and confidence intervals in the usual way. One famous example of profile likelihood is the Cox proportional hazard covered in <a href="#survival">Chapter 7</a>.</p>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>Several properties of maximum likelihood estimator makes it appealing for inference.</p>
<ul>
<li>The maximum likelihood estimator is <strong>consistent</strong>, i.e., it converges to the correct value as the sample size increase (asymptotically unbiased).</li>
<li>The maximum likelihood estimator is invariant to reparametrizations.</li>
<li>Under regularity conditions, the maximum likelihood estimator is asymptotically normal, so we can obtain the null distribution of classes of hypothesis tests and derive confidence intervals based on <span class="math inline">\(\widehat{\boldsymbol{\theta}}\)</span>.</li>
<li>The maximum likelihood estimator is efficient, meaning it has the smallest asymptotic mean squared error.</li>
</ul>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Recall that, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent random variables, the joint probability is the product of the probability of the events, <span class="math inline">\(\Pr(A \cup B) = \Pr(A)\Pr(B)\)</span>. The same holds for density or mass function, since the latter are defined as the derivative of the distribution function.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Since in most instances we deal with a product of densities, taking the log leads to a sum of log density contributions, which facilitates optimization.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Using for example a symbolic calculator.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Most optimization algorithms minimize functions with respect to their arguments, so we minimize the negative log likelihood, which is equivalent to maximizing the log likelihood.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>