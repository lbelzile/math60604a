<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Linear regression | Statistical Modelling</title>
  <meta name="description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Linear regression | Statistical Modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="github-repo" content="lbelzile/math60604a" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Linear regression | Statistical Modelling" />
  
  <meta name="twitter:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="likelihood.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary remarks</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to statistical inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#tests"><i class="fa fa-check"></i><b>1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#eda"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>2.2</b> Ordinary least squares</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#interpretation-of-the-model-parameters"><i class="fa fa-check"></i><b>2.3</b> Interpretation of the model parameters</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#test-lm"><i class="fa fa-check"></i><b>2.4</b> Tests for parameters of the linear model</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#coefR2"><i class="fa fa-check"></i><b>2.5</b> Coefficient of determination</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#predictions-lm"><i class="fa fa-check"></i><b>2.6</b> Predictions</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i><b>2.7</b> Interactions</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>2.8</b> Collinearity</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#graphical-analysis-of-residuals"><i class="fa fa-check"></i><b>2.9</b> Graphical analysis of residuals</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#transformation-response"><i class="fa fa-check"></i><b>2.10</b> Transformation of the response</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>3</b> Likelihood-based inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="likelihood.html"><a href="likelihood.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.1</b> Maximum likelihood</a></li>
<li class="chapter" data-level="3.2" data-path="likelihood.html"><a href="likelihood.html#liktests"><i class="fa fa-check"></i><b>3.2</b> Likelihood-based tests</a></li>
<li class="chapter" data-level="3.3" data-path="likelihood.html"><a href="likelihood.html#profile-likelihood"><i class="fa fa-check"></i><b>3.3</b> Profile likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="likelihood.html"><a href="likelihood.html#information-criteria"><i class="fa fa-check"></i><b>3.4</b> Information criteria</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basic-principles"><i class="fa fa-check"></i><b>4.1</b> Basic principles</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#theory-of-generalized-linear-models"><i class="fa fa-check"></i><b>4.2</b> Theory of generalized linear models</a></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>4.3</b> Binary responses</a></li>
<li class="chapter" data-level="4.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-data"><i class="fa fa-check"></i><b>4.4</b> Count data</a></li>
<li class="chapter" data-level="4.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#modelling-proportions"><i class="fa fa-check"></i><b>4.5</b> Modelling proportions</a></li>
<li class="chapter" data-level="4.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#rates"><i class="fa fa-check"></i><b>4.6</b> Rates</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="correlated-longitudinal-data.html"><a href="correlated-longitudinal-data.html"><i class="fa fa-check"></i><b>5</b> Correlated and longitudinal data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="correlated-longitudinal-data.html"><a href="correlated-longitudinal-data.html#longitudinal-data"><i class="fa fa-check"></i><b>5.1</b> Longitudinal data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Linear mixed models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#partial-pooling"><i class="fa fa-check"></i><b>6.1</b> Partial pooling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>7</b> Survival analysis</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="complement.html"><a href="complement.html"><i class="fa fa-check"></i><b>A</b> Complements</a>
<ul>
<li class="chapter" data-level="A.1" data-path="complement.html"><a href="complement.html#population-sample"><i class="fa fa-check"></i><b>A.1</b> Population and samples</a></li>
<li class="chapter" data-level="A.2" data-path="complement.html"><a href="complement.html#random-variable"><i class="fa fa-check"></i><b>A.2</b> Random variable</a></li>
<li class="chapter" data-level="A.3" data-path="complement.html"><a href="complement.html#law-large-numbers"><i class="fa fa-check"></i><b>A.3</b> Laws of large numbers</a></li>
<li class="chapter" data-level="A.4" data-path="complement.html"><a href="complement.html#CLT"><i class="fa fa-check"></i><b>A.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>B</b> Supplementary material</a>
<ul>
<li class="chapter" data-level="B.1" data-path="math.html"><a href="math.html#ols"><i class="fa fa-check"></i><b>B.1</b> Derivation of the ordinary least squares estimator</a></li>
<li class="chapter" data-level="B.2" data-path="math.html"><a href="math.html#derivationR2"><i class="fa fa-check"></i><b>B.2</b> Derivation of the coefficient of determination</a></li>
<li class="chapter" data-level="B.3" data-path="math.html"><a href="math.html#restricted-estimation-maximum-likelihood"><i class="fa fa-check"></i><b>B.3</b> Restricted estimation maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="r.html"><a href="r.html"><i class="fa fa-check"></i><b>C</b> <strong>R</strong></a>
<ul>
<li class="chapter" data-level="C.1" data-path="r.html"><a href="r.html#basics-of-r"><i class="fa fa-check"></i><b>C.1</b> Basics of <strong>R</strong></a></li>
<li class="chapter" data-level="C.2" data-path="r.html"><a href="r.html#rlmfunc"><i class="fa fa-check"></i><b>C.2</b> Linear models in <strong>R</strong> using the <code>lm</code> function</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Linear regression<a href="linear-regression.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A linear regression is a model for the conditional mean of a response variable <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(p\)</span> explanatory variables (also termed regressors or covariates),
<span class="math display" id="eq:linearreg">\[\begin{align}
\mathsf{E}(Y \mid \mathbf{X})=\beta_0 + \beta_1\mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}. \tag{2.1}
\end{align}\]</span>
The mean of <span class="math inline">\(Y\)</span> is conditional on the values of the observed covariate <span class="math inline">\(\mathbf{X}\)</span>; this amounts to treating them as non-random, known in advance.</p>
<p>In practice, any model is an approximation of reality. An error term is included to take into account the fact that no exact linear relationship links <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> (otherwise this wouldn’t be a statistical problem), or that measurements of <span class="math inline">\(Y\)</span> are subject to error. The random error term <span class="math inline">\(\varepsilon\)</span> will be the source of information for our inference, as it will quantify the goodness of fit of the model.</p>
<p>We can rewrite the linear model in terms of the error for a random sample of size <span class="math inline">\(n\)</span>: denote by <span class="math inline">\(Y_i\)</span> the value of the response for observation <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mathrm{X}_{ij}\)</span> the value of of the <span class="math inline">\(j\)</span>th explanatory variable of observation <span class="math inline">\(i\)</span>. The model is
<span class="math display" id="eq:olsmean">\[\begin{align}
Y_i = \beta_0 + \beta_1 \mathrm{X}_{i1} + \ldots + \beta_p \mathrm{X}_{ip} +\varepsilon_{i}, \qquad i =1, \ldots, n, \tag{2.2}
\end{align}\]</span>
where <span class="math inline">\(\varepsilon_i\)</span> is the additive error term specific to observation <span class="math inline">\(i\)</span>. While we may avoid making distributional assumption about <span class="math inline">\(\varepsilon_i\)</span>, we nevertheless fix its expectation to zero to encode the fact we do not believe the model is systematically off,, so <span class="math inline">\(\mathsf{E}(\varepsilon_i \mid \boldsymbol{X}_i)=0\)</span> <span class="math inline">\((i=1, \ldots, n)\)</span>.</p>
<p>One important remark is that the model is linear in the coefficients <span class="math inline">\(\boldsymbol{\beta}\in \mathbb{R}_{p+1}\)</span>, not in the explanatory variables! the latter are arbitrary and could be (nonlinear) functions of other explanatory variables, for example <span class="math inline">\(\mathrm{X}=\ln(\texttt{years})\)</span>, <span class="math inline">\(\mathrm{X}=\texttt{horsepower}^2\)</span> or <span class="math inline">\(\mathrm{X}= \mathsf{I}_{\texttt{man}}\cdot\mathsf{I}_{\texttt{full}}\)</span>. The mean of the response is specified as a <strong>linear combination of explanatory variables</strong>. This is at the core of the flexibility of the linear regression, which is used mainly for the following purposes:</p>
<ol style="list-style-type: decimal">
<li>Evaluate the effects of covariates <span class="math inline">\(\mathbf{X}\)</span> on the mean response
of <span class="math inline">\(Y\)</span>.</li>
<li>Quantify the influence of the explanatories <span class="math inline">\(\mathbf{X}\)</span> on the
response and test for their significance.</li>
<li>Predict the response for new sets of explanatories <span class="math inline">\(\mathbf{X}\)</span>.</li>
</ol>
<div id="introduction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction<a href="linear-regression.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear regression is the most famous and the most widely used statistical model around. The name may appear reductive, but many tests statistics (<em>t</em>-tests, ANOVA, Wilcoxon, Kruskal–Wallis) <a href="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf">can be formulated using a linear regression</a>, while <a href="https://threadreaderapp.com/thread/1286420597505892352.html">models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise</a>. What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).</p>
<p>This chapter explores the basics of linear regression, parameter interpretation and testing for coefficients and sub-models. Analysis of variance will be presented as special case of linear regression.</p>
<p>To make concepts and theoretical notions more concrete, we will use data from a study performed in a college in the United States. The goal of the administration who collected these information was to investigate potential gender inequality in the salary of faculty members. The data contains the following variables:</p>
<ul>
<li><code>salary</code>: nine-month salary of professors during the 2008–2009 academic year (in thousands USD).</li>
<li><code>rank</code>: academic rank of the professor (<code>assistant</code>, <code>associate</code> or <code>full</code>).</li>
<li><code>field</code>: categorical variable for the field of expertise of the professor, one of <code>applied</code> or <code>theoretical</code>.</li>
<li><code>sex</code>: binary indicator for sex, either <code>man</code> or <code>woman</code>.</li>
<li><code>service</code>: number of years of service in the college.</li>
<li><code>years</code>: number of years since PhD.</li>
</ul>
<p>Before drafting a model, a quick look at the data is in due order. If salary increases with year, there is more heterogeneity in the salary of higher ranked professors: logically, assistant professors are either promoted or kicked out after at most 6 years according to the data. The limited number of years prevents large variability for their salaries.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:edacollege"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/edacollege-1.png" alt="Exploratory data analysis of $\texttt{college}$ data: salaries of professors as a function of the number of years of service and the academic ranking" width="70%" />
<p class="caption">
Figure 2.1: Exploratory data analysis of <span class="math inline">\(\texttt{college}\)</span> data: salaries of professors as a function of the number of years of service and the academic ranking
</p>
</div>
<p>Salary increases over years of service, but its variability also increases with rank. Note the much smaller number of women in the sample: this will impact our power to detect differences between sex. A contingency table of sex and academic rank can be useful to see if the proportion of women is the same in each rank: women represent 16% of assistant professors and 16% of associate profs, but only 7% of full professors and these are better paid on average.</p>
<table>
<caption>
<span id="tab:tableaucontingence">Table 2.1: </span>Contingency table of the number of prof in the college by sex and academic rank.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
assistant
</th>
<th style="text-align:right;">
associate
</th>
<th style="text-align:right;">
full
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
man
</td>
<td style="text-align:right;">
56
</td>
<td style="text-align:right;">
54
</td>
<td style="text-align:right;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
woman
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
18
</td>
</tr>
</tbody>
</table>
<p>The simple linear regression model only includes a single explanatory variable and defines a straight line linking two variables <span class="math inline">\(\mathrm{X}\)</span> and <span class="math inline">\(Y\)</span> by means of an equation of the form <span class="math inline">\(y=a+bx\)</span>; Figure <a href="linear-regression.html#fig:droitenuage">2.2</a> shows the line passing through the scatterplot for years of service.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:droitenuage"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/droitenuage-1.png" alt="Simple linear regression model for the salary of professors as a function of the number of years of service; the line is the solution of the least squares problem." width="70%" />
<p class="caption">
Figure 2.2: Simple linear regression model for the salary of professors as a function of the number of years of service; the line is the solution of the least squares problem.
</p>
</div>
</div>
<div id="ordinary-least-squares" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Ordinary least squares<a href="linear-regression.html#ordinary-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The ordinary least square estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}=(\widehat{\beta}_0, \ldots, \widehat{\beta}_p)\)</span> are the values that simultaneously minimize the Euclidean distance between the random observations <span class="math inline">\(Y_i\)</span> and the <strong>fitted values</strong>
<span class="math display">\[\begin{align*}
\widehat{Y}_i &amp;= \widehat{\beta}_0 + \widehat{\beta}_1 \mathrm{X}_{i1} + \cdots + \widehat{\beta}_p \mathrm{X}_{ip}, \qquad i =1, \ldots, n.
\end{align*}\]</span>
In other words, the least square estimators are the solution of the convex optimization problem
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} &amp;=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (Y_i-\widehat{Y}_i)^2= \min_{\boldsymbol{\beta}} \|\boldsymbol{Y}-\mathbf{X}\boldsymbol{\beta}\|^2
\end{align*}\]</span>
This system of equations has an explicit solution which is better expressed using matrix notation: this amounts to expressing equation <a href="linear-regression.html#eq:olsmean">(2.2)</a> with one observation per line.</p>
<p>Consider the matrices
<span class="math display">\[\begin{align*}
\boldsymbol{Y} =
\begin{pmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{pmatrix} ,
\;
\boldsymbol{\varepsilon} =
\begin{pmatrix}
  \varepsilon_1 \\
  \varepsilon_2 \\
  \vdots \\
  \varepsilon_n
\end{pmatrix} ,
\;
\mathbf{X} = \begin{pmatrix}
\mathrm{X}_{11} &amp; \mathrm{X}_{12} &amp; \cdots &amp; \mathrm{X}_{1p} \\
\mathrm{X}_{21} &amp; \mathrm{X}_{22} &amp; \cdots &amp; \mathrm{X}_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\mathrm{X}_{n1} &amp; \mathrm{X}_{n2} &amp; \cdots &amp; \mathrm{X}_{np}
\end{pmatrix} , \;
\boldsymbol{\beta} =
\begin{pmatrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_p
\end{pmatrix}
\end{align*}\]</span>
The model in compact form is
<span class="math display">\[\begin{align*}
\boldsymbol{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}.
\end{align*}\]</span>
The ordinary least squares estimator solves the unconstrained optimization problem
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span>
and a proof is provided <a href="math.html#ols">in the Appendix</a>. If the <span class="math inline">\(n \times (p+1)\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is full-rank, we obtain a unique solution to the optimization problem,
<span class="math display" id="eq:ols">\[\begin{align}
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{Y}. \tag{2.3}
\end{align}\]</span></p>
<p>What does the solution to the least squares problem represent in two dimensions? The estimator is the one minimizing the sum of squared residuals: the <span class="math inline">\(i\)</span>th <strong>ordinary residual</strong> <span class="math inline">\(e_i = y_i -\widehat{y}_i\)</span> is the <em>vertical</em> distance between a point <span class="math inline">\(y_i\)</span> and the fitted value <span class="math inline">\(\widehat{y}_i\)</span> on the line; the blue segments on Figure <a href="linear-regression.html#fig:distancevert">2.3</a> represent the individual vectors of residuals.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:distancevert"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/distancevert-1.png" alt="Illustration of ordinary residuals added to the regression line (blue vectors)." width="70%" />
<p class="caption">
Figure 2.3: Illustration of ordinary residuals added to the regression line (blue vectors).
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-1" class="remark"><em>Remark</em> (Geometry of least squares). </span>If we consider the <span class="math inline">\(n\)</span> observations as a (column) vector, the term <span class="math inline">\(\mathbf{X} \widehat{\boldsymbol{\beta}}\)</span> is the projection of the response vector <span class="math inline">\(\boldsymbol{y}\)</span> on the linear span generated by the columns of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathscr{S}_{\mathbf{X}}\)</span>. The ordinary residuals are thus orthogonal to <span class="math inline">\(\mathscr{S}_{\mathbf{X}}\)</span> and to the fitted values, meaning <span class="math inline">\(\boldsymbol{e}^\top\widehat{\boldsymbol{y}}=0\)</span>.
A direct consequence of this fact is that the linear correlation between <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> is zero; we will use this property to build graphical diagnostics.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-2" class="remark"><em>Remark</em> (Complexity of ordinary least squares). </span>This is an aside: in machine learning, you will often encounter linear models fitted using a (stochastic) gradient descent algorithm. Unless your sample size <span class="math inline">\(n\)</span> or the number of covariates <span class="math inline">\(p\)</span> is significant (think at the Google scale), an approximate should not be prefered to the exact solution! From a numerical perspective, obtaining the least square estimates requires inverting a <span class="math inline">\((p+1) \times (p+1)\)</span> matrix <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span>, which is the most costly operation. In general, direct inversion should be avoided because it is not the most numerically stable way of obtaining the solution. <strong>R</strong> uses the QR decomposition, which has a complexity of <span class="math inline">\(\mathrm{O}(np^2)\)</span>. Another more stable alternative, which has the same complexity but is a bit more costly is use of a singular value decomposition.</p>
</div>
<p>Any good software will calculate ordinary least square estimates for you. Keep in mind that there is an explicit and unique solution provided your model matrix <span class="math inline">\(\mathbf{X}\)</span> doesn’t contain collinear columns. If you have more than one explanatory variable, the fitted values lie on a hyperplan (which is hard to represent graphically). Mastering the language and technical term (fitted values, ordinary residuals, etc.) is necessary for the continuation.</p>
</div>
<div id="interpretation-of-the-model-parameters" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Interpretation of the model parameters<a href="linear-regression.html#interpretation-of-the-model-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What do the <span class="math inline">\(\boldsymbol{\beta}\)</span> parameters of the linear model represent? In the simple case presented in Figure <a href="linear-regression.html#fig:droitenuage">2.2</a>, the equation of the line is <span class="math inline">\(\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1\mathrm{X}_1\)</span>, <span class="math inline">\(\beta_0\)</span> is the intercept (the mean value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(\mathrm{X}_1=0\)</span>) and <span class="math inline">\(\beta_1\)</span> is the slope, i.e., the average increase of <span class="math inline">\(Y\)</span> when <span class="math inline">\(\mathrm{X}_1\)</span> increases by one unit.</p>
<p>In some cases, the intercept is <strong>meaningless</strong> because the value <span class="math inline">\(\mathrm{X}_1=0\)</span> is impossible (e.g., <span class="math inline">\(\mathrm{X}_1\)</span> represents the height of a human). In the same vein, there may be no observation in a neighborhood of <span class="math inline">\(\mathrm{X}_1=0\)</span>, even if this value is plausible, in which case the intercept is an <strong>extrapolation</strong>.</p>
<p>If the columns of <span class="math inline">\(\mathbf{X}\)</span> are arbitrary, it is customary to include an intercept: this amounts to including <span class="math inline">\(\mathbf{1}_n\)</span> as column of the model matrix <span class="math inline">\(\mathbf{X}\)</span>. Because the residuals are orthogonal to the columns of <span class="math inline">\(\mathbf{X}\)</span>, their mean is zero, since <span class="math inline">\(n^{-1}\mathbf{1}_n^\top\boldsymbol{e}=\bar{\boldsymbol{e}}=0\)</span>. In general, we could also obtain mean zero residuals by including a set of vectors in <span class="math inline">\(\mathbf{X}\)</span> that span <span class="math inline">\(\mathbf{1}_n\)</span>.</p>
<p>In our example, the equation of the fitted line of Figure <a href="linear-regression.html#fig:droitenuage">2.2</a> is <span class="math display">\[\widehat{\texttt{salary}} = 99.975 + 0.78\texttt{service}.\]</span>
The average salary of a new professor would then be 99974.653 dollars, whereas the average annual increase for each additional year of service is 779.569 dollars.</p>
<p>If the response variable <span class="math inline">\(Y\)</span> should be continuous (for the least square criterion to be meaningful), we place no such restriction on the explanatories. The case of dummies in particular is common: these variables are encoded using binary indicators (<span class="math inline">\(0/1\)</span>). Consider for example the sex of the professors in the study:
<span class="math display">\[\texttt{sex} = \begin{cases} 0 , &amp; \text{for men},\\
1, &amp; \text{for women.}
\end{cases}
\]</span>
The equation of the simple linear model that includes the binary variable <span class="math inline">\(\texttt{sex}\)</span> is <span class="math inline">\(\texttt{salary} = \beta_0 + \beta_1 \texttt{sex} + \varepsilon\)</span>. Let <span class="math inline">\(\mu_0\)</span> denote the average salary of mean and <span class="math inline">\(\mu_1\)</span> that of women. The intercept <span class="math inline">\(\beta_0\)</span> can be interpreted as usual: it is the average salary when <span class="math inline">\(\texttt{sex}=0\)</span>, meaning that <span class="math inline">\(\beta_0=\mu_0\)</span>. We can write the equation for the conditional expectation for each sex,
<span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{salary} \mid \texttt{sex})= \begin{cases}
\beta_0, &amp; \texttt{sex}=0 \text{ (men)}, \\
\beta_0 + \beta_1 &amp; \texttt{sex}=1 \text{ (women)}.
\end{cases}
\end{align*}\]</span>
A linear model that only contains a binary variable <span class="math inline">\(\mathrm{X}\)</span> as regressor amounts to specifying a different mean for each of two groups: the average of women is <span class="math inline">\(\mathsf{E}(\texttt{salary} \mid \texttt{sex}=1) = \beta_0 + \beta_1 = \mu_1\)</span> and <span class="math inline">\(\beta_1=\mu_1-\mu_0\)</span> represents the difference between the average salary of men and women. The least square estimator <span class="math inline">\(\widehat{\beta}_0\)</span> is the sample mean of men and <span class="math inline">\(\widehat{\beta}_1\)</span> is the difference of the sample mean of women and men. The parametrization of the linear model with <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is in terms of <strong>contrasts</strong> and is particularly useful if we want to test for mean difference between the groups, as this amounts to testing <span class="math inline">\(\mathscr{H}_0: \beta_1=0\)</span>. If we wanted our model to directly output the sample means, we would need to replace the model matrix <span class="math inline">\(\mathbf{X}=[\mathbf{1}_n, \texttt{sex}]\)</span> by <span class="math inline">\([\mathbf{1}_n- \texttt{sex}, \texttt{sex}]\)</span>. The fitted model would be the same because they span the same 2D subspace, but this is not recommended because software treat cases without intercept differently and it can lead to unexpected behaviour (more on this latter).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:graphcollegesex"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/graphcollegesex-1.png" alt="Simple linear model for the $\texttt{college}$ data using the binary variable sex as regressor: even if the equation defines a line, only its values in $0/1$ are realistic." width="70%" />
<p class="caption">
Figure 2.4: Simple linear model for the <span class="math inline">\(\texttt{college}\)</span> data using the binary variable sex as regressor: even if the equation defines a line, only its values in <span class="math inline">\(0/1\)</span> are realistic.
</p>
</div>
<p>If we fit the model with sex only to the <code>college</code> data, we find that the average salary of men is <span class="math inline">\(\widehat{\beta}_0=1.151\times 10^{5}\)</span> USD and the mean difference estimate of the salary between women and men is <span class="math inline">\(\widehat{\beta}_1=14088.009\)</span> dollars. Since the estimate is negative, this means women are paid less. Bear in mind that the model is not adequate for determining if there are gender inequalities in the salary distribution: <a href="linear-regression.html#fig:droitenuage">2.2</a> shows that the number of years of service and the academic rank strongly impact wages, yet the distribution of men and women is not the same within each rank.</p>
<p>Even if the linear model defines a line, the latter is only meaningful when evaluated at <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>; Figure <a href="linear-regression.html#fig:graphcollegesex">2.4</a> shows it in addition to sample observations (jittered) and a density estimate for each sex. The colored dot represents the mean, showing that the line does indeed pass through the mean of each group.
<!-- http://easysas.blogspot.com/2011/10/sas-how-to-draw-added-variable-plot.html
: certains logiciels, **R** en tête, modifient les sorties et ne présentent plus le coefficient de détermination car il n'est pas donné que les résidus soient centrés. --></p>
<p>A binary indicator is a categorical variable with two levels, so we could extend our reasoning and fit a model with a categorical explanatory variable with <span class="math inline">\(k\)</span> levels. To do this, we add <span class="math inline">\(k-1\)</span> indicator variables plus the intercept: if we want to model a different mean for each of the <span class="math inline">\(k\)</span> groups, it is logical to only include <span class="math inline">\(k\)</span> parameters in the mean model. We will choose, as we did with sex, a <strong>reference category</strong> or <strong>baseline</strong> whose average will be encoded by the intercept <span class="math inline">\(\beta_0\)</span>. The other parameters <span class="math inline">\(\beta_1, \ldots, \beta_{k-1}\)</span> are contrasts relative to the baseline. The college data includes the ordinal variable <code>rank</code>, which has three levels (assistant, associate and full). We thus need two binary variables, <span class="math inline">\(\mathrm{X}_1 = \mathsf{I}(\texttt{rank}=\texttt{associate})\)</span> and <span class="math inline">\(\mathrm{X}_2 = \mathsf{I}(\texttt{rank}=\texttt{full})\)</span>; the <span class="math inline">\(i\)</span>th element of the vector <span class="math inline">\(\mathrm{X}_1\)</span> is one for an associate professor and zero otherwise. The linear model is
<span class="math display">\[\begin{align*}
\texttt{salary} =\beta_0 + \beta_1 \mathrm{X}_1+\beta_2\mathrm{X}_2 + \varepsilon,
\end{align*}\]</span>
and the conditional expectation of salary
<span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{salary} \mid \texttt{rank})= \begin{cases}
\beta_0, &amp; \texttt{rank}=\texttt{assistant},\\
\beta_0 + \beta_1 &amp; \texttt{rank}=\texttt{associate},\\
\beta_0 + \beta_2 &amp; \texttt{rank}=\texttt{full},
\end{cases}
\end{align*}\]</span>
Thus <span class="math inline">\(\beta_1\)</span> (respectively <span class="math inline">\(\beta_2\)</span>) are the difference between the average salary of associate (respectively full) professors and assistant professors. The choice of the baseline category is arbitrary and all choices yield the same model: only the interpretation changes from one parametrization to the next. For an ordinal variable, it is recommended to choose the smallest or the largest category to ease comparisons.</p>
<p>The models we have fitted so far are not adequate because they ignore variables that are necessarily to correctly explain variations in salaries: Figure <a href="linear-regression.html#fig:edacollege">2.1</a> show for example that rank is critical for explaining the salary variations in the college. We should thus fit a model that include those simultaneously to investigate the gender gap (which consists of differences that are unexplained by other factors). Before doing this, we come back to the interpretation of the parameters in the multiple linear regression setting.</p>
<p>Consider the model <span class="math inline">\(Y= \beta_0 + \beta_1 \mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p + \varepsilon\)</span>. The intercept <span class="math inline">\(\beta_0\)</span> represents the mean value of <span class="math inline">\(Y\)</span> when <em>all</em> of the covariates are set to zero,
<span class="math display">\[\begin{align*}
\beta_0 &amp;= \mathsf{E}(Y \mid \mathrm{X}_1=0,\mathrm{X}_2=0,\ldots,\mathrm{X}_p=0).
\end{align*}\]</span>
For categorical variables, this yields the baseline, whereas we fix the continous variables to zero: again, this may be nonsensical depending on the study. The coefficient <span class="math inline">\(\beta_j\)</span> <span class="math inline">\((j \geq 1)\)</span> can be interpreted as the mean increase of the response <span class="math inline">\(Y\)</span> when <span class="math inline">\(\mathrm{X}_j\)</span> increases by one unit, all other things being equal (<em>ceteris paribus</em>); e.g.,
<span class="math display">\[\begin{align*}
\beta_1 &amp;= \mathsf{E}(Y \mid \mathrm{X}_1=x_1+1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
&amp; \qquad \qquad - \mathsf{E}(Y \mid \mathrm{X}_1=x_1,\mathrm{X}_2=x_2,\ldots,\mathrm{X}_p=x_p) \\
&amp;= \left\{\beta_0 + \beta_1 (x_1+1) + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\} \\
&amp; \qquad \qquad -\left\{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots +\beta_p \mathrm{X}_p \right\}
\end{align*}\]</span>
It is not always possible to fix the value of an explanatory if multiple columns of <span class="math inline">\(\mathbf{X}\)</span> contains functions/transformations of it. For example, if we included a polynomial of order <span class="math inline">\(k\)</span> for some variable <span class="math inline">\(\mathrm{X}\)</span>,
<span class="math display">\[\begin{align*}
Y=\beta_0+ \beta_1 \mathrm{X}+ \beta_2 \mathrm{X}^2 + \ldots +\beta_k \mathrm{X}^k + \varepsilon.
\end{align*}\]</span>
If we include a term of order <span class="math inline">\(k\)</span>, <span class="math inline">\(\mathrm{X}^k\)</span>, we must <em>always</em> include the lower order terms <span class="math inline">\(1, \mathrm{X}, \ldots, \mathrm{X}^{k-1}\)</span> to make sure the resulting model is interpretable (otherwise, it amounts to a particular class of polynomials with some zero coefficients). Interpreting nonlinear effects (even polynomials, for which <span class="math inline">\(k\leq 3\)</span> in practice), is complicated because the effect of an increase of one unit of <span class="math inline">\(\mathrm{X}\)</span> <em>depends of the value of the latter</em>.</p>
<div class="example">
<p><span id="exm:auto" class="example"><strong>Example 2.1  (Auto data) </strong></span>We consider a linear regression model for the fuel autonomy of cars as a function of the power of their motor (measured in horsepower) from the <code>auto</code> dataset. The postulated model,
<span class="math display">\[
\texttt{mpg}_i = \beta_0 + \beta_1 \texttt{horsepower}_i + \beta_2 \texttt{horsepower}_i^2 + \varepsilon_i,
\]</span>
includes a quadratic term. Figure <a href="linear-regression.html#fig:autoquad2d">2.5</a> shows the scatterplot with the fitted regression line, above which the line for the simple linear regression for horsepower is added.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:autoquad2d"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/autoquad2d-1.png" alt="Linear regression models for the fuel autonomy of cars as a function of motor power" width="70%" />
<p class="caption">
Figure 2.5: Linear regression models for the fuel autonomy of cars as a function of motor power
</p>
</div>
<p>It appears graphically that the quadratic model fits better than the simple linear alternative: we will assess this hypothesis formally later. For the degree two polynomial, Figure <a href="linear-regression.html#fig:autoquad2d">2.5</a> show that fuel autonomy decreases rapidly when power increases between 50 to 100, then more slow until 189.35 hp. After that, the model postulates that autonomy increases again as evidenced by the scatterplot, but beware of extrapolating (weird things can happen beyond the range of the data, as exemplified by <a href="https://livefreeordichotomize.com/2020/05/05/model-detective/">Hassett’s cubic model for the number of daily cases of Covid19 in the USA</a>).</p>
<p>The representation in Figure <a href="linear-regression.html#fig:autoquad2d">2.5</a> may seem counter-intuitive given that we fit a linear model, but it is a 2D projection of 3D coordinates for the equation <span class="math inline">\(\beta_0 + \beta_1x-y +\beta_2z =0\)</span>, where <span class="math inline">\(x=\texttt{horsepower}\)</span>, <span class="math inline">\(z=\texttt{horsepower}^2\)</span> and <span class="math inline">\(y=\texttt{mpg}\)</span>. Physics and common sense force <span class="math inline">\(z = x^2\)</span>, and so the fitted values lie on a curve in a 2D subspace of the fitted plan, as shown in grey in the 3D Figure <a href="linear-regression.html#fig:hyperplan">2.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplan"></span>
<div id="htmlwidget-a68d69b74e58baa58133" style="width:70%;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-a68d69b74e58baa58133">{"x":{"visdat":{"a3512357c1fe":["function () ","plotlyVisDat"],"a3513db3a6f4":["function () ","data"]},"cur_data":"a3513db3a6f4","attrs":{"a3512357c1fe":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","name":"data","opacity":0.8,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.8},"inherit":true},"a3512357c1fe.1":{"z":{},"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"inherit":false},"a3513db3a6f4":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","color":["#003C71"],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"horsepower"},"yaxis":{"title":"square of horsepower"},"zaxis":{"title":"fuel autonomy (mpg)"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[130,165,150,150,140,198,220,215,225,190,170,160,150,225,95,95,97,85,88,46,87,90,95,113,90,215,200,210,193,88,90,95,100,105,100,88,100,165,175,153,150,180,170,175,110,72,100,88,86,90,70,76,65,69,60,70,95,80,54,90,86,165,175,150,153,150,208,155,160,190,97,150,130,140,150,112,76,87,69,86,92,97,80,88,175,150,145,137,150,198,150,158,150,215,225,175,105,100,100,88,95,46,150,167,170,180,100,88,72,94,90,85,107,90,145,230,49,75,91,112,150,110,122,180,95,100,100,67,80,65,75,100,110,105,140,150,150,140,150,83,67,78,52,61,75,75,75,97,93,67,95,105,72,72,170,145,150,148,110,105,110,95,110,110,129,75,83,100,78,96,71,97,97,70,90,95,88,98,115,53,86,81,92,79,83,140,150,120,152,100,105,81,90,52,60,70,53,100,78,110,95,71,70,75,72,102,150,88,108,120,180,145,130,150,68,80,58,96,70,145,110,145,130,110,105,100,98,180,170,190,149,78,88,75,89,63,83,67,78,97,110,110,48,66,52,70,60,110,140,139,105,95,85,88,100,90,105,85,110,120,145,165,139,140,68,95,97,75,95,105,85,97,103,125,115,133,71,68,115,85,88,90,110,130,129,138,135,155,142,125,150,71,65,80,80,77,125,71,90,70,70,65,69,90,115,115,90,76,60,70,65,90,88,90,90,78,90,75,92,75,65,105,65,48,48,67,67,67,67,62,132,100,88,72,84,84,92,110,84,58,64,60,67,65,62,68,63,65,65,74,75,75,100,74,80,76,116,120,110,105,88,85,88,88,88,85,84,90,92,74,68,68,63,70,88,75,70,67,67,67,110,85,92,112,96,84,90,86,52,84,79,82],"y":[16900,27225,22500,22500,19600,39204,48400,46225,50625,36100,28900,25600,22500,50625,9025,9025,9409,7225,7744,2116,7569,8100,9025,12769,8100,46225,40000,44100,37249,7744,8100,9025,10000,11025,10000,7744,10000,27225,30625,23409,22500,32400,28900,30625,12100,5184,10000,7744,7396,8100,4900,5776,4225,4761,3600,4900,9025,6400,2916,8100,7396,27225,30625,22500,23409,22500,43264,24025,25600,36100,9409,22500,16900,19600,22500,12544,5776,7569,4761,7396,8464,9409,6400,7744,30625,22500,21025,18769,22500,39204,22500,24964,22500,46225,50625,30625,11025,10000,10000,7744,9025,2116,22500,27889,28900,32400,10000,7744,5184,8836,8100,7225,11449,8100,21025,52900,2401,5625,8281,12544,22500,12100,14884,32400,9025,10000,10000,4489,6400,4225,5625,10000,12100,11025,19600,22500,22500,19600,22500,6889,4489,6084,2704,3721,5625,5625,5625,9409,8649,4489,9025,11025,5184,5184,28900,21025,22500,21904,12100,11025,12100,9025,12100,12100,16641,5625,6889,10000,6084,9216,5041,9409,9409,4900,8100,9025,7744,9604,13225,2809,7396,6561,8464,6241,6889,19600,22500,14400,23104,10000,11025,6561,8100,2704,3600,4900,2809,10000,6084,12100,9025,5041,4900,5625,5184,10404,22500,7744,11664,14400,32400,21025,16900,22500,4624,6400,3364,9216,4900,21025,12100,21025,16900,12100,11025,10000,9604,32400,28900,36100,22201,6084,7744,5625,7921,3969,6889,4489,6084,9409,12100,12100,2304,4356,2704,4900,3600,12100,19600,19321,11025,9025,7225,7744,10000,8100,11025,7225,12100,14400,21025,27225,19321,19600,4624,9025,9409,5625,9025,11025,7225,9409,10609,15625,13225,17689,5041,4624,13225,7225,7744,8100,12100,16900,16641,19044,18225,24025,20164,15625,22500,5041,4225,6400,6400,5929,15625,5041,8100,4900,4900,4225,4761,8100,13225,13225,8100,5776,3600,4900,4225,8100,7744,8100,8100,6084,8100,5625,8464,5625,4225,11025,4225,2304,2304,4489,4489,4489,4489,3844,17424,10000,7744,5184,7056,7056,8464,12100,7056,3364,4096,3600,4489,4225,3844,4624,3969,4225,4225,5476,5625,5625,10000,5476,6400,5776,13456,14400,12100,11025,7744,7225,7744,7744,7744,7225,7056,8100,8464,5476,4624,4624,3969,4900,7744,5625,4900,4489,4489,4489,12100,7225,8464,12544,9216,7056,8100,7396,2704,7056,6241,6724],"z":[18,15,18,16,17,15,14,14,14,15,15,14,15,14,24,22,18,21,27,26,25,24,25,26,21,10,10,11,9,27,28,25,19,16,17,19,18,14,14,14,14,12,13,13,18,22,19,18,23,28,30,30,31,35,27,26,24,25,23,20,21,13,14,15,14,17,11,13,12,13,19,15,13,13,14,18,22,21,26,22,28,23,28,27,13,14,13,14,15,12,13,13,14,13,12,13,18,16,18,18,23,26,11,12,13,12,18,20,21,22,18,19,21,26,15,16,29,24,20,19,15,24,20,11,20,19,15,31,26,32,25,16,16,18,16,13,14,14,14,29,26,26,31,32,28,24,26,24,26,31,19,18,15,15,16,15,16,14,17,16,15,18,21,20,13,29,23,20,23,24,25,24,18,29,19,23,23,22,25,33,28,25,25,26,27,17.5,16,15.5,14.5,22,22,24,22.5,29,24.5,29,33,20,18,18.5,17.5,29.5,32,28,26.5,20,13,19,19,16.5,16.5,13,13,13,31.5,30,36,25.5,33.5,17.5,17,15.5,15,17.5,20.5,19,18.5,16,15.5,15.5,16,29,24.5,26,25.5,30.5,33.5,30,30.5,22,21.5,21.5,43.1,36.1,32.8,39.4,36.1,19.9,19.4,20.2,19.2,20.5,20.2,25.1,20.5,19.4,20.6,20.8,18.6,18.1,19.2,17.7,18.1,17.5,30,27.5,27.2,30.9,21.1,23.2,23.8,23.9,20.3,17,21.6,16.2,31.5,29.5,21.5,19.8,22.3,20.2,20.6,17,17.6,16.5,18.2,16.9,15.5,19.2,18.5,31.9,34.1,35.7,27.4,25.4,23,27.2,23.9,34.2,34.5,31.8,37.3,28.4,28.8,26.8,33.5,41.5,38.1,32.1,37.2,28,26.4,24.3,19.1,34.3,29.8,31.3,37,32.2,46.6,27.9,40.8,44.3,43.4,36.4,30,44.6,33.8,29.8,32.7,23.7,35,32.4,27.2,26.6,25.8,23.5,30,39.1,39,35.1,32.3,37,37.7,34.1,34.7,34.4,29.9,33,33.7,32.4,32.9,31.6,28.1,30.7,25.4,24.2,22.4,26.6,20.2,17.6,28,27,34,31,29,27,24,36,37,31,38,36,36,36,34,38,32,38,25,38,26,22,32,36,27,27,44,32,28,31],"type":"scatter3d","mode":"markers","name":"data","opacity":0.8,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.8,"line":{"color":"rgba(31,119,180,1)"},"showscale":false},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"mpg<br />surf","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(190,190,190,1)"],["1","rgba(190,190,190,1)"]],"showscale":false,"z":[[38.0591911137724,-47.7197007965408],[100.550736455475,14.7718445451618]],"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"frame":null},{"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250],"y":[0,1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900,961,1024,1089,1156,1225,1296,1369,1444,1521,1600,1681,1764,1849,1936,2025,2116,2209,2304,2401,2500,2601,2704,2809,2916,3025,3136,3249,3364,3481,3600,3721,3844,3969,4096,4225,4356,4489,4624,4761,4900,5041,5184,5329,5476,5625,5776,5929,6084,6241,6400,6561,6724,6889,7056,7225,7396,7569,7744,7921,8100,8281,8464,8649,8836,9025,9216,9409,9604,9801,10000,10201,10404,10609,10816,11025,11236,11449,11664,11881,12100,12321,12544,12769,12996,13225,13456,13689,13924,14161,14400,14641,14884,15129,15376,15625,15876,16129,16384,16641,16900,17161,17424,17689,17956,18225,18496,18769,19044,19321,19600,19881,20164,20449,20736,21025,21316,21609,21904,22201,22500,22801,23104,23409,23716,24025,24336,24649,24964,25281,25600,25921,26244,26569,26896,27225,27556,27889,28224,28561,28900,29241,29584,29929,30276,30625,30976,31329,31684,32041,32400,32761,33124,33489,33856,34225,34596,34969,35344,35721,36100,36481,36864,37249,37636,38025,38416,38809,39204,39601,40000,40401,40804,41209,41616,42025,42436,42849,43264,43681,44100,44521,44944,45369,45796,46225,46656,47089,47524,47961,48400,48841,49284,49729,50176,50625,51076,51529,51984,52441,52900,53361,53824,54289,54756,55225,55696,56169,56644,57121,57600,58081,58564,59049,59536,60025,60516,61009,61504,62001,62500],"z":[56.9000997021131,56.4351406082665,55.9726425866214,55.512605637178,55.055029759936,54.5999149548956,54.1472612220568,53.6970685614195,53.2493369729838,52.8040664567496,52.3612570127169,51.9209086408858,51.4830213412563,51.0475951138283,50.6146299586018,50.1841258755769,49.7560828647535,49.3305009261317,48.9073800597114,48.4867202654927,48.0685215434755,47.6527838936599,47.2395073160458,46.8286918106333,46.4203373774223,46.0144440164129,45.611011727605,45.2100405109987,44.8115303665939,44.4154812943907,44.021893294389,43.6307663665888,43.2421005109902,42.8558957275932,42.4721520163977,42.0908693774037,41.7120478106113,41.3356873160205,40.9617878936311,40.5903495434434,40.2213722654572,39.8548560596725,39.4908009260894,39.1292068647078,38.7700738755278,38.4134019585493,38.0591911137724,37.707441341197,37.3581526408232,37.0113250126509,36.6669584566801,36.325052972911,35.9856085613433,35.6486252219772,35.3141029548127,34.9820417598497,34.6524416370882,34.3253025865283,34.00062460817,33.6784077020132,33.3586518680579,33.0413571063042,32.726523416752,32.4141507994014,32.1042392542524,31.7967887813048,31.4917993805589,31.1892710520144,30.8892037956716,30.5915976115302,30.2964524995905,30.0037684598522,29.7135454923156,29.4257835969804,29.1404827738468,28.8576430229148,28.5772643441843,28.2993467376553,28.0238902033279,27.7508947412021,27.4803603512778,27.212287033555,26.9466747880338,26.6835236147142,26.4228335135961,26.1646044846795,25.9088365279645,25.655529643451,25.4046838311391,25.1562990910287,24.9103754231199,24.6669128274126,24.4259113039069,24.1873708526027,23.9512914735001,23.717673166599,23.4865159318995,23.2578197694015,23.0315846791051,22.8078106610102,22.5864977151168,22.367645841425,22.1512550399348,21.9373253106461,21.7258566535589,21.5168490686733,21.3103025559893,21.1062171155067,20.9045927472258,20.7054294511464,20.5087272272685,20.3144860755922,20.1227059961174,19.9333869888442,19.7465290537725,19.5621321909024,19.3801964002338,19.2007216817668,19.0237080355013,18.8491554614374,18.677063959575,18.5074335299141,18.3402641724548,18.1755558871971,18.0133086741409,17.8535225332862,17.6961974646331,17.5413334681816,17.3889305439316,17.2389886918831,17.0915079120362,16.9464882043909,16.803929568947,16.6638320057048,16.5261955146641,16.3910200958249,16.2583057491873,16.1280524747512,16.0002602725167,15.8749291424837,15.7520590846523,15.6316500990224,15.513702185594,15.3982153443673,15.285189575342,15.1746248785183,15.0665212538962,14.9608787014756,14.8576972212565,14.756976813239,14.6587174774231,14.5629192138087,14.4695820223958,14.3787059031845,14.2902908561747,14.2043368813665,14.1208439787599,14.0398121483547,13.9612413901512,13.8851317041491,13.8114830903487,13.7402955487497,13.6715690793524,13.6053036821565,13.5414993571623,13.4801561043695,13.4212739237783,13.3648528153887,13.3108927792006,13.2593938152141,13.2103559234291,13.1637791038456,13.1196633564637,13.0780086812834,13.0388150783046,13.0020825475273,12.9678110889516,12.9360007025774,12.9066513884048,12.8797631464337,12.8553359766642,12.8333698790963,12.8138648537298,12.796820900565,12.7822380196016,12.7701162108399,12.7604554742796,12.753255809921,12.7485172177638,12.7462396978082,12.7464232500542,12.7490678745017,12.7541735711508,12.7617403400013,12.7717681810535,12.7842570943072,12.7992070797625,12.8166181374192,12.8364902672776,12.8588234693375,12.8836177435989,12.9108730900619,12.9405895087264,12.9727669995925,13.0074055626602,13.0445051979293,13.0840659054001,13.1260876850723,13.1705705369462,13.2175144610215,13.2669194572984,13.3187855257769,13.3731126664569,13.4299008793385,13.4891501644216,13.5508605217062,13.6150319511924,13.6816644528802,13.7507580267695,13.8223126728603,13.8963283911527,13.9728051816467,14.0517430443422,14.1331419792392,14.2170019863378,14.3033230656379,14.3921052171396,14.4833484408428,14.5770527367476,14.673218104854,14.7718445451618,14.8729320576712,14.9764806423822,15.0824902992947,15.1909610284088,15.3018928297244,15.4152857032416,15.5311396489603,15.6494546668805,15.7702307570023,15.8934679193257,16.0191661538506,16.147325460577,16.277945839505,16.4110272906346,16.5465698139657,16.6845734094983,16.8250380772325,16.9679638171682,17.1133506293055,17.2611985136444],"type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,60,113,1)","line":{"color":"rgba(0,60,113,1)"},"showscale":false},"textfont":{"color":"rgba(0,60,113,1)"},"error_y":{"color":"rgba(0,60,113,1)"},"error_x":{"color":"rgba(0,60,113,1)"},"line":{"color":"rgba(0,60,113,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly",".hideLegend":true},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 2.6: 3D graphical representation of the linear regression model for the <span class="math inline">\(\texttt{auto}\)</span> data.
</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-3" class="remark"><em>Remark</em> (There are better alternatives to polynomials for modelling nonlinear effects). </span>Generally speaking, one uses flexible basis vectors (splines) rather than polynomials for smoothing when the relation between the response <span class="math inline">\(Y\)</span> and an explanatory variable <span class="math inline">\(\mathrm{X}\)</span> is nonlinear; one then adds a penalty term to prevent overfitting and control the wiggliness. A better (physical) understanding of the system, or a theoretical model can also guide the choice of functional form to use.</p>
</div>
<p>The coefficient <span class="math inline">\(\beta_j\)</span> in Eq. <a href="linear-regression.html#eq:linearreg">(2.1)</a> represents the <em>marginal</em> contribution of <span class="math inline">\(\mathrm{X}_j\)</span> when all the other covariates are included in the model and which is not explained by them. This can be represented graphically by projecting <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathrm{X}_j\)</span> in the orthogonal complement of <span class="math inline">\(\mathbf{X}_{-j}\)</span> (the matrix containing all but the <span class="math inline">\(j\)</span>th column <span class="math inline">\(\mathrm{X}_j\)</span>). The <strong>added-variable plot</strong> is a graphical tool showing this projection: the residuals from the linear regression of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(\mathscr{S}(\mathbf{X}_{-j})\)</span> are mapped to the <span class="math inline">\(y\)</span>-axis, whereas the residuals from the linear regression of <span class="math inline">\(\mathrm{X}_j\)</span> as a function of <span class="math inline">\(\mathbf{X}_{-j}\)</span> are shown on the <span class="math inline">\(x\)</span>-axis. The regression line passes through (<span class="math inline">\(0,0\)</span>) and its slope is <span class="math inline">\(\widehat{\beta}_j\)</span>. This graphical diagnostic is useful for detecting collinearity and the impact of outliers.</p>
<div class="example">
<p><span id="exm:inequite-salariale" class="example"><strong>Example 2.2  (Wage inequality in an American college) </strong></span>We consider a multiple regression model for the <code>college</code> data that includes sex, academic rank, field of study and the number of years of service as regressors.</p>
</div>
<p>If we multiply <span class="math inline">\(\texttt{salary}\)</span> by a thousand to get the resulting estimates in US dollars, the postulated model is
<span class="math display">\[\begin{align*}
\texttt{salary}\times 1000 &amp;= \beta_0 + \beta_1 \texttt{sex}_{\texttt{woman}} +\beta_2 \texttt{field}_{\texttt{theoretical}} \\&amp;\quad +\beta_3 \texttt{rank}_{\texttt{associate}}
+\beta_4 \texttt{rank}_{\texttt{full}}  +\beta_5 \texttt{service} + \varepsilon.
\end{align*}\]</span></p>
<table>
<caption>
<span id="tab:collegecoefs">Table 2.2: </span>Estimated coefficients of the linear model for the <span class="math inline">\(\texttt{college}\)</span> (in USD, rounded to the nearest dollar).
</caption>
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_0\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_1\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_2\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_3\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_4\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\widehat{\beta}_5\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
86596
</td>
<td style="text-align:right;">
-4771
</td>
<td style="text-align:right;">
-13473
</td>
<td style="text-align:right;">
14560
</td>
<td style="text-align:right;">
49160
</td>
<td style="text-align:right;">
-89
</td>
</tr>
</tbody>
</table>
<p>The interpretation of the coefficients is as follows:</p>
<ul>
<li>The estimated intercept is <span class="math inline">\(\widehat{\beta}_0=86596\)</span> dollars; it corresponds to the mean salary of men assistant professors who just started the job and works in an applied domain.</li>
<li>everything else being equal (same field, academic rank, and number of years of service), the estimated salary difference between a woman and is estimated at <span class="math inline">\(\widehat{\beta}_1=-4771\)</span> dollars.</li>
<li><em>ceteris paribus</em>, the salary difference between a professor working in a theoretical field and one working in an applied field is <span class="math inline">\(\beta_2\)</span> dollars: our estimate of this difference is <span class="math inline">\(-13473\)</span> dollars, meaning applied pays more than theoretical.</li>
<li><em>ceteris paribus</em>, the estimated mean salary difference between associate and assistant professors is <span class="math inline">\(\widehat{\beta}_3=14560\)</span> dollars.</li>
<li><em>ceteris paribus</em>, the estimated mean salary difference between full and assistant professors is <span class="math inline">\(\widehat{\beta}_4=49160\)</span> dollars.</li>
<li>au sein d’un même échelon, chaque année supplémentaire de service mène à une augmentation de salary annuelle moyenne de <span class="math inline">\(\widehat{\beta}_5=-89\)</span> dollars.</li>
</ul>
<p>All other factors taken into account, women get paid less than men. It remains to see if this difference is statistically significant. Perhaps more surprising, the model estimates that salary decreases with every additional year of service: this seems counterintuitive when looking at Figure <a href="linear-regression.html#fig:droitenuage">2.2</a>, which showed a steady increas of salary over the years. However, this graphical representation is misleading because Figure <a href="linear-regression.html#fig:edacollege">2.1</a> showed that academic ranking mattered the most. Once all the other factors are accounted for, years of service serves to explain the salary of full professors who have been working unusual amounts of time and who gain less than the average full professor, as shown by the added-variable plot of Figure <a href="linear-regression.html#fig:avplotcollege">2.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:avplotcollege"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/avplotcollege-1.png" alt="Added-variable plot for years of service in the linear regression model of the  $\texttt{college}$ data." width="70%" />
<p class="caption">
Figure 2.7: Added-variable plot for years of service in the linear regression model of the <span class="math inline">\(\texttt{college}\)</span> data.
</p>
</div>
<p><a href="r.html#rlmfunc">Details about implementation of linear models using <strong>R</strong> are provided in the Appendix</a>.</p>
</div>
<div id="test-lm" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Tests for parameters of the linear model<a href="linear-regression.html#test-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We need to make further assumptions in order to carry out inference and build testing procedures for the mean model parameters of the linear model. In order to get a tractable distribution for test statistics, it is customary to assume that the disturbances <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> are independent normal random variables with mean zero and common variance <span class="math inline">\(\sigma^2\)</span>. It follows that <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are conditionally <em>independent</em> random variables with
<span class="math display">\[\begin{align*}
\mathsf{E}(Y_i \mid \mathbf{X}_i)=\beta_0 + \sum_{j=1}^p \mathrm{X}_{ij}\beta_j, \quad  \mathsf{Va}(Y_i\mid \mathbf{X}_i)= \sigma^2, \quad i=1, \ldots, n.
\end{align*}\]</span>
Under this hypothesis, the least square estimators for the mean parameters coincide with the maximum likelihood estimators. The advantage of imposing this (more stringent than necessary) assumption is that we can use our toolbox for testing. The theory underlying likelihood tests is presented in the chapter on <a href="likelihood">likelihood-based inference</a>. Assuming normal errors leads to exact distributions for the test statistics (which also coincide with the asymptotic ones in large samples).</p>
<p>Of particular interest are tests of restrictions for components of <span class="math inline">\(\boldsymbol{\beta}\)</span>. The large sample properties of the maximum likelihood estimator imply that
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} \stackrel{\cdot}{\sim}\mathsf{No}_{p+1}\left\{\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right\}
\end{align*}\]</span>
for sufficiently large sample size. One can thus easily estimate the standard errors from the matrix upon replacing <span class="math inline">\(\sigma^2\)</span> by an estimator, typically the unbiased estimator of the variance.</p>
<p>In an inferential setting, it’s often important to test whether the effect of an explanatory variable is significant: if <span class="math inline">\(\mathrm{X}_j\)</span> is binary or continuous, the test for <span class="math inline">\(\mathscr{H}_0: \beta_j=0\)</span> corresponds to a null marginal effect for <span class="math inline">\(\mathrm{X}_j\)</span>. The null model is a linear regression in which we remove the <span class="math inline">\((j+1)\)</span>st column of <span class="math inline">\(\mathbf{X}\)</span>, so both models are nested. The Wald test statistic is reported by most software <span class="math inline">\(W=\widehat{\beta}_j/\mathsf{se}(\widehat{\beta}_j)\)</span> and the null distribution is Student-<em>t</em> with <span class="math inline">\(n-p-1\)</span> degrees of freedom, which explains the terminology (<span class="math inline">\(t\)</span> values). In addition to coefficient estimates, it is possible to obtain confidence intervals for <span class="math inline">\(\beta_j\)</span>, which are the usual <span class="math inline">\(\widehat{\beta}_j \pm t_{n-p-1,\alpha/2} \mathsf{se}(\widehat{\beta}_j)\)</span>, with <span class="math inline">\(t_{n-p-1,\alpha/2}\)</span> denoting the <span class="math inline">\(1-\alpha/2\)</span> quantile of the <span class="math inline">\(\mathsf{St}_{n-p-1}\)</span> distribution.</p>
<p>For categorical variables with more than two levels, testing if <span class="math inline">\(\beta_j=0\)</span> is typically not of interest because the contrast represent the different between the category <span class="math inline">\(\mathrm{X}_j\)</span> and the baseline: these two categories could have a small difference, but the categorical variable as a whole may still be a useful predictor given the other explanatories. The hypothesis of zero contrast is awkward because it implies a null model in which selected categories are merged.</p>
<div id="ftestslm" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> <em>F</em>-tests for comparison of nested linear models<a href="linear-regression.html#ftestslm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the <em>full</em> linear model which contains <span class="math inline">\(p\)</span> predictors,
<span class="math display">\[\begin{align*}
\mathbb{M}_1: Y=\beta_0+\beta_1 \mathrm{X}_1 + \cdots + \beta_g \mathrm{X}_g + \beta_{k+1}\mathrm{X}_{k+1} + \ldots + \beta_p \mathrm{X}_p + \varepsilon.
\end{align*}\]</span>
Suppose without loss of generality that we want to test <span class="math inline">\(\mathscr{H}_0: \beta_{k+1}=\beta_{k+2}=\ldots=\beta_p=0\)</span> (one could permute columns of the model matrix to achieve this configuration).
The global hypothesis specifies that <span class="math inline">\((p-k)\)</span> of the <span class="math inline">\(\beta\)</span> parameters are zero. The <em>restricted model</em> corresponding to the null hypothesis contains only the covariates for which <span class="math inline">\(\beta_j \neq 0\)</span>,
<span class="math display">\[\begin{align*}
\mathbb{M}_0: Y=\beta_0+\beta_1 \mathrm{X}_1 + \ldots + \beta_k \mathrm{X}_k + \varepsilon.
\end{align*}\]</span>
Let <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_1)\)</span> be the residuals sum of squares for model <span class="math inline">\(\mathbb{M}_1\)</span>,
<span class="math display">\[\begin{align*}
\mathsf{SS}_e(\mathbb{M}_1)=\sum_{i=1}^n (Y_i-\widehat{Y}_i^{\mathbb{M}_1})^2,
\end{align*}\]</span>
where <span class="math inline">\(\widehat{Y}_i^{\mathbb{M}_1}\)</span> is the <span class="math inline">\(i\)</span>th fitted value from <span class="math inline">\(\mathbb{M}_1\)</span>. Similarly define <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_0)\)</span> for the residuals sum of square of <span class="math inline">\(\mathbb{M}_0\)</span>. Clearly, <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_0) \geq \mathsf{SS}_e(\mathbb{M}_1)\)</span> (why?)</p>
<p>The <span class="math inline">\(F\)</span>-test statistic is
<span class="math display">\[\begin{align*}
F=\frac{\{\mathsf{SS}_e(\mathbb{M}_0)-\mathsf{SS}_e(\mathbb{M}_1)\}/(p-k)}{\mathsf{SS}_e(\mathbb{M}_1)/(n-p-1)}
\end{align*}\]</span>
Under <span class="math inline">\(\mathscr{H}_0\)</span>, the <span class="math inline">\(F\)</span> statistic follows a <a href="https://en.wikipedia.org/wiki/F-distribution">Fisher distribution</a> with <span class="math inline">\((p-k)\)</span> and <span class="math inline">\((n-p-1)\)</span> degrees of freedom, <span class="math inline">\(\mathsf{F}(p-k, n-p-1)\)</span> — <span class="math inline">\(p-k\)</span> is the number of restrictions, <span class="math inline">\(n-p-1\)</span> is sample size minus the number of <span class="math inline">\(\beta\)</span>’s in <span class="math inline">\(\mathbb{M}_1\)</span>.</p>
<p>It turns out that both <span class="math inline">\(F\)</span> and <span class="math inline">\(t\)</span>-statistics are equivalent for testing a single coefficient <span class="math inline">\(\beta_j\)</span>: the <span class="math inline">\(F\)</span>-statistic is the square of the Wald statistic and they lead to the same inference — the <span class="math inline">\(p\)</span>-value for the test are identical. While it may get reported in tables, the test for <span class="math inline">\(\beta_0=0\)</span> is not of interest; we keep the intercept merely to centre the residuals.</p>
<p>For normal linear regression, the likelihood ratio test for comparing models <span class="math inline">\(\mathbb{M}_1\)</span> and <span class="math inline">\(\mathbb{M}_0\)</span> is a function of the sum of squared residuals: the usual formula simplifies to <span class="math inline">\(R = n\ln\{\mathsf{SS}_e(\mathbb{M}_0)/\mathsf{SS}_e(\mathbb{M}_1)\}\)</span>. This is reminiscent of the <span class="math inline">\(F\)</span>-statistic formula and the two are in fact intimately related modulo null distribution and scaling. The <span class="math inline">\(t\)</span>-tests and <span class="math inline">\(F\)</span>-tests presented above could thus both be viewed as particular cases of <a href="liktests">likelihood-based tests</a>.</p>
<p>Consider the <code>college</code> data example and the associated linear model with <code>rank</code>, <code>sex</code>, years of <code>service</code> and <code>field</code> as covariates.</p>
<table>
<caption>
<span id="tab:summarytestslmcollege">Table 2.3: </span>Table of linear regression coefficients with associated standard errors, Wald tests and <span class="math inline">\(p\)</span>-values based on Student-<span class="math inline">\(t\)</span> distribution
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std. error
</th>
<th style="text-align:right;">
Wald stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
86.596
</td>
<td style="text-align:right;">
2.96
</td>
<td style="text-align:right;">
29.25
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
sex [woman]
</td>
<td style="text-align:right;">
-4.771
</td>
<td style="text-align:right;">
3.878
</td>
<td style="text-align:right;">
-1.23
</td>
<td style="text-align:right;">
0.22
</td>
</tr>
<tr>
<td style="text-align:left;">
field [theoretical]
</td>
<td style="text-align:right;">
-13.473
</td>
<td style="text-align:right;">
2.315
</td>
<td style="text-align:right;">
-5.82
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
rank [associate]
</td>
<td style="text-align:right;">
14.56
</td>
<td style="text-align:right;">
4.098
</td>
<td style="text-align:right;">
3.55
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
rank [full]
</td>
<td style="text-align:right;">
49.16
</td>
<td style="text-align:right;">
3.834
</td>
<td style="text-align:right;">
12.82
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
service
</td>
<td style="text-align:right;">
-0.089
</td>
<td style="text-align:right;">
0.112
</td>
<td style="text-align:right;">
-0.8
</td>
<td style="text-align:right;">
0.43
</td>
</tr>
</tbody>
</table>
<p>Table <a href="linear-regression.html#tab:summarytestslmcollege">2.3</a> shows the estimated coefficients (in thousands of dollars). The coefficients are the least squares estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, the standard errors are the square root of the diagonal elements of <span class="math inline">\(S^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span>. The Wald (or <em>t</em>-) statistic is simply <span class="math inline">\(W=\widehat{\beta}/\mathsf{se}(\widehat{\beta})\)</span> for <span class="math inline">\(\mathscr{H}_0: \beta_j=0\)</span>: given two of the three estimates, we could easily recover the third using the formula for the test. The <span class="math inline">\(p\)</span>-values are for the two-sided alternative test with <span class="math inline">\(\mathscr{H}_a: \beta_j \neq 0\)</span>.</p>
<p>The interpretation is usual: <span class="math inline">\(p\)</span>-values that are less than our prescribed level <span class="math inline">\(\alpha\)</span> do not contribute significantly given the other variables already in the model. Neither years of service nor sex are statistically different from zero given all the other variables. The test for <span class="math inline">\(\beta_{\texttt{sex}}\)</span> is comparing the model with all covariates (including service), and vice-versa. Note that the conclusion changes depending on the model: both coefficients would be statistically significant had we removed rank from the set of covariates, because they are correlated. The gender imbalance among ranks explains most of the gap between sex, whereas year of service is largely redundant once we account for the jumps due to change of academic rank.</p>
<table>
<caption>
<span id="tab:summaryanovalmcollege">Table 2.4: </span>Type 3 sum of square decomposition table: sum of square decomposition comparing nested models with and without covariates, <span class="math inline">\(F\)</span>-statistic and associated <span class="math inline">\(p\)</span>-value.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:right;">
sum of square
</th>
<th style="text-align:right;">
df
</th>
<th style="text-align:right;">
F stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
439059.2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
855.71
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
sex
</td>
<td style="text-align:right;">
776.7
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.51
</td>
<td style="text-align:right;">
0.22
</td>
</tr>
<tr>
<td style="text-align:left;">
field
</td>
<td style="text-align:right;">
17372.5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
33.86
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
rank
</td>
<td style="text-align:right;">
102883.1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
100.26
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
service
</td>
<td style="text-align:right;">
324.5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.63
</td>
<td style="text-align:right;">
0.43
</td>
</tr>
<tr>
<td style="text-align:left;">
Residuals
</td>
<td style="text-align:right;">
200620.4
</td>
<td style="text-align:right;">
391
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>Table <a href="linear-regression.html#tab:summaryanovalmcollege">2.4</a> gives the <span class="math inline">\(F\)</span>-statistics values and the associated <span class="math inline">\(p\)</span>-values. The sum of squares represent the difference <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_0)-\mathsf{SS}_e(\mathbb{M}_1)\)</span> for various null models <span class="math inline">\(\mathbb{M}_0\)</span>, except the last line for residuals which reports <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_1)\)</span>. You can verify that (up to rounding) these <span class="math inline">\(p\)</span>-pvalues are identical to those of the Wald test in the output when <code>df=1</code>. The only categorical variable here with more than one level is <code>rank</code>, and it is strongly significant: removing it from the model leads to a sharp decrease in fit.</p>
<p>We could have also easily computed the likelihood ratio test to compare the models: for example, the log likelihood for the full model is -1799.027 and that of the model without rank is -1881.202, so the likelihood ratio statistic would be 164.349 and this is strongly significant when compared to the <span class="math inline">\(\chi^2_2\)</span> distribution (both likelihood ratio test and <span class="math inline">\(F\)</span>-test give a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(2.2 \times 10^{-16}\)</span>).</p>
</div>
</div>
<div id="coefR2" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Coefficient of determination<a href="linear-regression.html#coefR2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we specify a model, the error term <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> accounts for the fact no perfect linear relationship characterizes the data (if it did, we wouldn’t need statistic to begin with). Once we have fitted a model, we estimate the variance <span class="math inline">\(\sigma^2\)</span>; one may then wonder which share of the total variance in the sample is explained by the model.</p>
<p>The total sum of squares, defined as the sum of squared residuals from the intercept-only model, serves as comparison — the simplest model we could come up with would involving every observation by the sample mean of the response and so this gives (up to scale) the variance of the response, <span class="math inline">\(\mathsf{SS}_c = \sum_{i=1}^n (y_i - \overline{y})^2\)</span>. We can then compare the variance of the original data with that of the residuals from the model with covariate matrix <span class="math inline">\(\mathbf{X}\)</span>, defined as <span class="math inline">\(\mathsf{SS}_e =\sum_{i=1}^n e_i^2\)</span> with <span class="math inline">\(e_i = y_i - \widehat{\beta}_0 - \sum_{j=1}^p \widehat{\beta}_j\mathrm{X}_j\)</span>.
We define the coefficient of determination, or squared multiple correlation coefficient of the model, <span class="math inline">\(R^2\)</span>, as
<span class="math display">\[\begin{align*}
R^2 = 1- \frac{\mathsf{SS}_e}{\mathsf{SS}_c} = \frac{\sum_{i=1}^n (y_i - \overline{y})^2- \sum_{i=1}^n e_i^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
\end{align*}\]</span>
The coefficient of determination can be interpreted as the square of <a href="moments">Pearson’s linear correlation</a> between the response <span class="math inline">\(\boldsymbol{y}\)</span> and the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span>; see <a href="math.html#derivationR2">the Appendix for a derivation of this fact.</a></p>
<p>Its important to note that <span class="math inline">\(R^2\)</span> is not a goodness-of-fit criterion: some phenomena are inherently noisy and even a good model will fail to account for much of the response’s variability. Moreover, one can inflate the value of <span class="math inline">\(R^2\)</span> by including more explanatory variables: the coefficient is non-decreasing in the dimension of <span class="math inline">\(\mathbf{X}\)</span>, so a model with <span class="math inline">\(p+1\)</span> covariate will necessarily have a higher <span class="math inline">\(R^2\)</span> values than only <span class="math inline">\(p\)</span> of the explanatories. For model comparisons, it is better to employ <a href="likelihood.html#information-criteria">information criteria</a>, or else rely on the predictive performance if this is the purpose of the regression. Lastly, a model with a high <span class="math inline">\(R^2\)</span> may imply high correlation, but <a href="http://www.tylervigen.com/spurious-correlations">the relation may be spurious</a>: linear regression does not yield causal models!</p>
</div>
<div id="predictions-lm" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Predictions<a href="linear-regression.html#predictions-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we compute least square estimates, we obtain fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> as <span class="math inline">\(\mathbf{X}\widehat{\boldsymbol{\beta}}\)</span>, where <span class="math inline">\(\mathbf{X}\)</span> denotes the <span class="math inline">\(n \times (p+1)\)</span> matrix of original observations. Recalling that <span class="math inline">\(\mathsf{E}(Y_i \mid \mathbf{X}_i) = \beta_0 + \sum_{j=1}^p \beta_j \mathrm{X}_{ij}\)</span>, we can obtain an estimate of the mean surface for any value of <span class="math inline">\(\mathbf{x}_{n+1} \in \mathbb{R}^p\)</span> by replacing the unknown coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> by our estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> – this actually yields the best linear unbiased predictor of the mean.</p>
<p>If we want to predict the value of a new observation, say <span class="math inline">\(Y_{n+1}\)</span>, with explanatories <span class="math inline">\(\mathbf{x}_{n+1} = (1, x_1, \ldots, x_p)\)</span>, the prediction of the value will also be <span class="math inline">\(\widehat{y}_{n+1} = \mathbf{x}_{n+1}\widehat{\boldsymbol{\beta}}\)</span> because
<span class="math display">\[\begin{align*}
\mathsf{E}(Y_{n+1} \mid \mathbf{x}_{n+1}) = \mathbf{x}_{n+1}\boldsymbol{\beta} + \mathsf{E}(\varepsilon_{n+1} \mid \mathbf{x}_{n+1}) = \mathbf{x}_{n+1}\boldsymbol{\beta}.
\end{align*}\]</span>
However, individual observations vary more than averages (which are themselves based on multiple observations). Intuitively, this is due to the added uncertainty of the error term <span class="math inline">\(\varepsilon_{n+1}\)</span> appearing in the model equation: the variability of new predictions is the sum of uncertainty due to the estimators (based on random data) and the intrinsic variance of the observations assuming the new observation is independent of those used to estimate the coefficients,
<span class="math display">\[\begin{align*}
\mathsf{Va}(Y_{n+1} \mid \mathbf{x}_n) &amp;= \mathsf{Va}(\mathrm{x}_{n+1}\widehat{\boldsymbol{\beta}} + \varepsilon_{n+1} \mid \mathbf{x}_n)
\\&amp;=\mathsf{Va}(\mathrm{x}_{n+1}\widehat{\boldsymbol{\beta}} \mid \mathbf{x}_n) +\mathsf{Va}(\varepsilon_{n+1} \mid \mathbf{x}_n)
\\&amp; = \sigma^2\mathrm{x}_{n+1}(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}_{n+1}^\top + \sigma^2,
\end{align*}\]</span>
where <span class="math inline">\(S^2\)</span> is the unbiased estimator of the variance <span class="math inline">\(\sigma^2\)</span>. The distribution of <span class="math inline">\(Y_{n+1}\)</span> is by assumption normal, but since we do not know the variance, we base the prediction interval on the Student distribution, viz.
<span class="math display">\[\begin{align*}
\frac{\mathrm{x}_{n+1}\widehat{\boldsymbol{\beta}}-Y_{n+1}}{\sqrt{S^2\{1+\mathrm{x}_{n+1}(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}_{n+1}^\top\}}}\sim \mathsf{St}_{n-p-1}.
\end{align*}\]</span>
and obtain <span class="math inline">\(1-\alpha\)</span> prediction interval for <span class="math inline">\(Y_{n+1}\)</span> by inverting the test statistic,
<span class="math display">\[\begin{align*}
\mathrm{x}_{n+1}\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\{1+\mathrm{x}_{n+1}(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}_{n+1}^\top\}}.
\end{align*}\]</span>
Similar calculations yield the formula for confidence intervals for the mean,
<span class="math display">\[\begin{align*}
\mathrm{x}_{n+1}\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\mathrm{x}_{n+1}(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}_{n+1}^\top}.
\end{align*}\]</span>
The two differ only because of the additional variance of individual observations.</p>
<p>Figure <a href="linear-regression.html#fig:predinterval">2.8</a> shows pointwise uncertainty bands for a simple linear regression of the <code>intention</code> data as a function of <code>fixation</code>, illustrating the limitations of the linear model in this example: the model is not accounting for the fact that our response arises from a bounded discrete distribution with integer values ranging from 2 to 14. The middle line gives the prediction of individuals as we vary their fixation time. Looking at the formula for the confidence, it is clear that the bands are not linear (we consider the square root of a function that involves the predictors), but it is not obvious that the uncertainty increases as you move away from the average of the predictors. This is more easily seen by replicating the potential curves that could have happened with different data: I generated new potential slopes from the asymptotic normal distribution of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> estimators to show the hyperbolic shape is not surprising: we are basically tilting curves from the average fixation/intention, and they have higher potential from deviating far from the range of observations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:predinterval"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/predinterval-1.gif" alt="Prediction for the simple linear regression of buying intention as a function of fixation time. The plot shows predictions along with pointwise 95\% confidence intervals of the mean and the individual predictions." width="70%" />
<p class="caption">
Figure 2.8: Prediction for the simple linear regression of buying intention as a function of fixation time. The plot shows predictions along with pointwise 95% confidence intervals of the mean and the individual predictions.
</p>
</div>
</div>
<div id="interactions" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Interactions<a href="linear-regression.html#interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the interpretation of the linear model, the effect of an explanatory variable is assumed to be the same, regardless of the other explanatory variables (<em>ceteris paribus</em>). To isolate the effect of <span class="math inline">\(\beta_j\)</span>, we indeed fix the value of the other explanatories and increase by one the variable <span class="math inline">\(\mathrm{X}_j\)</span> (whenever this makes sense) to obtain the slope coefficient. However, the effect of a covariate may sometimes depend on another explanatory.</p>
<p>A good example of interaction is provided by the <code>insurance</code> dataset. An exploratory data analysis suggested that premiums depended on age, smoker status and body mass index, but through obesity status. It can be best represented graphically by looking at body mass index.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:insuranceinter1"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/insuranceinter1-1.png" alt="Graph of insurance charges against body mass index. The figure clearly shows the interaction: the premium vary for smokers depending on whether or not they are obese, but we see no such behaviour for non-smokers. There is a clear linear increase of charges with age, but it is not clear whether the annual increase is the same for the three groups." width="70%" />
<p class="caption">
Figure 2.9: Graph of insurance charges against body mass index. The figure clearly shows the interaction: the premium vary for smokers depending on whether or not they are obese, but we see no such behaviour for non-smokers. There is a clear linear increase of charges with age, but it is not clear whether the annual increase is the same for the three groups.
</p>
</div>
<p>From there, we could create an indicator variable <span class="math inline">\(\texttt{obese}=\mathsf{I}(\texttt{bmi} \geq 30)\)</span> and add an interaction term between smoker/obese (categorical) and age (continuous) in our mean model. We take non-smoker as baseline category. To make interpretation more meaningful, we rescale age so that <span class="math inline">\(\texttt{age}=0\)</span> corresponds to 18 years old.</p>
<table>
<caption>
<span id="tab:coefintercharges">Table 2.5: </span>Table of regression coefficients for the insurance data with interaction terms between age, obesity and smoker status.
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std. error
</th>
<th style="text-align:right;">
Wald stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
2668.7
</td>
<td style="text-align:right;">
362.7
</td>
<td style="text-align:right;">
7.36
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
age
</td>
<td style="text-align:right;">
265.9
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
17.7
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
obesity [obese]
</td>
<td style="text-align:right;">
115.3
</td>
<td style="text-align:right;">
510.3
</td>
<td style="text-align:right;">
0.23
</td>
<td style="text-align:right;">
0.82
</td>
</tr>
<tr>
<td style="text-align:left;">
smoker [yes]
</td>
<td style="text-align:right;">
13526.2
</td>
<td style="text-align:right;">
803.1
</td>
<td style="text-align:right;">
16.84
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
age * obesity [obese]
</td>
<td style="text-align:right;">
1.4
</td>
<td style="text-align:right;">
20.1
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
0.94
</td>
</tr>
<tr>
<td style="text-align:left;">
age * smoker [yes]
</td>
<td style="text-align:right;">
-5.3
</td>
<td style="text-align:right;">
33.5
</td>
<td style="text-align:right;">
-0.16
</td>
<td style="text-align:right;">
0.87
</td>
</tr>
<tr>
<td style="text-align:left;">
obesity [obese] * smoker [yes]
</td>
<td style="text-align:right;">
19308.7
</td>
<td style="text-align:right;">
1110.5
</td>
<td style="text-align:right;">
17.39
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
age * obesity [obese] * smoker [yes]
</td>
<td style="text-align:right;">
19.1
</td>
<td style="text-align:right;">
44.8
</td>
<td style="text-align:right;">
0.43
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
</tbody>
</table>
<p>The linear regression model has eight parameters, which could be mapped to four intercepts and four different slopes for <code>age</code>; however, the model is parametrized in terms of contrasts, which facilitates testing restrictions.</p>
<p><span class="math display">\[\begin{align*}
\texttt{charges} &amp;= \beta_{0} + \beta_{1}\texttt{age} + \beta_{2}\texttt{obese} + \beta_{3}\texttt{smoker} +
\beta_{4}\texttt{age} \cdot \texttt{obese} \\&amp;\quad + \beta_{5}\texttt{age} \cdot \texttt{smoker} + \beta_{6}\texttt{obese} \cdot \texttt{smoker} + \beta_{7}\texttt{age} \cdot \texttt{obese} \cdot \texttt{smoker} + \varepsilon
\end{align*}\]</span></p>
<p>Because of the three-way interaction, it is not possible to recover individual parameters by changing the value of the corresponding covariate and keeping everything else constant: changing the smoker status likely impacts multiple regressors simultaneously. To retrieve the interpretation of the different coefficients, we will need to change one parameter at the time, write the mean equation and then isolate the coefficients. Throughout, <span class="math inline">\(\texttt{obese}\)</span> is a dummy variable equal to one if the person has a body mass index greater than 30 and likewise <span class="math inline">\(\texttt{smoker}\)</span> if the person is a smoker.</p>
<p><span class="math display">\[\begin{align*}
\texttt{charges}  =
\begin{cases}
\beta_{0} + \beta_{1}\texttt{age}  &amp; (\mathrm{g}_1)\, \texttt{non-obese}, \texttt{non-smoker} \\
(\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{4})\texttt{age},  &amp; (\mathrm{g}_2)\,  \texttt{obese}, \texttt{non-smoker} \\
(\beta_{0} + \beta_{3}) + (\beta_{1} + \beta_{5})\texttt{age},  &amp; (\mathrm{g}_3)\,  \texttt{non-obese}, \texttt{smoker} \\
(\beta_{0} + \beta_{2} + \beta_{3}+ \beta_{6}) + (\beta_{1} + \beta_{4} +\beta_{5} + \beta_7)\texttt{age},  &amp; (\mathrm{g}_4)\,  \texttt{obese}, \texttt{smoker}
\end{cases}
\end{align*}\]</span></p>
<ul>
<li>The intercept <span class="math inline">\(\beta_0\)</span> is the average at 18 years old of non-smokers who are not obese.</li>
<li>The slope <span class="math inline">\(\beta_1\)</span> is the average annual increase in charges for non-smokers who are not obese.</li>
<li>The parameter <span class="math inline">\(\beta_2\)</span> is a contrast, the difference between the average charges of 18 years old non-smokers who are obese and those who are not.</li>
<li>The parameter <span class="math inline">\(\beta_3\)</span> is a contrast, the difference between the average premium for non-obese 18 years old who smoke and those who don’t.</li>
<li>The parameter <span class="math inline">\(\beta_4\)</span> is a contrast, the difference in average annual increase for non-smokers between obese and non-obese adults.</li>
<li>The parameter <span class="math inline">\(\beta_5\)</span> is a contrast, the difference in average annual increase for non-obese between smoker and non-smoker adults.</li>
</ul>
<p>The other two coefficients, <span class="math inline">\(\beta_6\)</span> and <span class="math inline">\(\beta_7\)</span> represent differences of average between groups <span class="math inline">\(\mathrm{g}_1 + \mathrm{g}_4 - \mathrm{g}_2 - \mathrm{g}_3\)</span> for both intercepts and slopes.</p>
<p>The only <span class="math inline">\(F\)</span>-test that we should consider in the analysis of variance table (containing the Type III sum of squares decomposition) is the test for the three-way interaction, which corresponds to <span class="math inline">\(\mathscr{H}_0: \beta_7=0\)</span>. The <span class="math inline">\(p\)</span>-value against the two-sided alternative is <span class="math inline">\(0.6704\)</span>, suggesting no difference in slope. The reason why we do not consider the other tests is that they correspond to irrelevant hypotheses. For example, the test for the two-way interaction term <span class="math inline">\(\mathscr{H}_0: \beta_4=0\)</span> associated to <span class="math inline">\(\texttt{age} \cdot \texttt{obese}\)</span> would correspond to merging the intercepts of group <span class="math inline">\(\mathrm{g}_1\)</span> and <span class="math inline">\(\mathrm{g}_2\)</span>. Changing the baseline category would imply that a different difference in intercept is forced to be zero.</p>
<p>Sometimes, however, specific hypothesis could be of interest because of the problem setting. We could perform bespoke test to check here that <span class="math inline">\(\mathscr{H}_0: \beta_2=\beta_4=0\)</span>, which consists in merging the two non-smoker categories, or even <span class="math inline">\(\mathscr{H}_0: \beta_2=\beta_4= \beta_5=\beta_7=0\)</span>, which amounts to merging non-smokers and the imposing a common slope for <span class="math inline">\(\texttt{age}\)</span>. Such tests are not directly available in the output, but we can implement them manually by fitting two models and then plug-in in the values of residual sum of squares of both model in the formula of the <span class="math inline">\(F\)</span> statistic. Using the <span class="math inline">\(F\)</span> null distribution, we get a <span class="math inline">\(p\)</span>-value of 0.965. This suggests both non-smoker groups are indistinguishable and likewise that there is no evidence that slopes for <span class="math inline">\(\texttt{age}\)</span> are not equal.</p>
<p>Most interactions include functions of categorical variables together with either other categorical variables (different intercepts / conditional means per subgroups) or else other. Keep in mind that the validity of our tests above depend on the model being correctly specified: there is however evidence of difference in heterogeneity between groups, with unexplained non-smoker records. Plotting the residuals from the model that includes four different intercepts and slopes for <span class="math inline">\(\texttt{age}\)</span> for each combination of smoker/obesity status misses other features that one would capture in diagnostic plots. in particular, except for some notable outliers, there is evidence that the premiums of smokers also increase with body mass index; as evidenced by Figure <a href="linear-regression.html#fig:resod">2.10</a>. If we include <code>bmi</code> as additional covariate in the model in addition of <code>obese</code>, the interpretation of changes in obesity will depend on the value of <code>bmi</code> and vice-versa.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resod"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/resod-1.png" alt="Residuals from the interaction model for $\texttt{charges}$ with $\texttt{age}$, $\texttt{smoker}$ and $\texttt{obesity}$. There is a notably outlier for a male smoker whose $\texttt{bmi}$ is exactly 30 and other points above. There is indication of a linear trend for both smoker sub-groups as their body mass index increase, which was not apparent previously because of the effect of age." width="70%" />
<p class="caption">
Figure 2.10: Residuals from the interaction model for <span class="math inline">\(\texttt{charges}\)</span> with <span class="math inline">\(\texttt{age}\)</span>, <span class="math inline">\(\texttt{smoker}\)</span> and <span class="math inline">\(\texttt{obesity}\)</span>. There is a notably outlier for a male smoker whose <span class="math inline">\(\texttt{bmi}\)</span> is exactly 30 and other points above. There is indication of a linear trend for both smoker sub-groups as their body mass index increase, which was not apparent previously because of the effect of age.
</p>
</div>
<div id="two-way-anova" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Two-way ANOVA<a href="linear-regression.html#two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>“We’re the same as regression, but we’ve established a separate persona.” — Chelsea Parlett-Pelleriti</p>
</blockquote>
<p>Two-way analysis of variance is a linear regression model with two categorical variables, and possibly an interaction term between the two. We consider a study on the effect of the type of delay and source of delay on the evaluation of service and on time lost waiting.</p>
<blockquote>
<p>Hui, M. K., Thakor, M. V. et Gill, R. (1998). <em>The Effects of Delay Type and Service Stage on Consumers’ Reaction to Waiting</em>. Journal of Consumer Research <strong>24</strong>, 469-479.</p>
</blockquote>
<p>In a university, 120 participants tried a new course registration system. They were actually using a fake registration system where factors of interest could be manipulated, the advancement stage in the registration process and the type of delay. The two levels for stage of advancement were <code>close to end</code> and <code>far from end</code>. In stage <code>far from end</code>, a message indicating a delay appeared immediately at the beginning of the registration process. For the stage <code>close to end</code>, the delay message appeared after the participant has entered personal information and course choices.</p>
<p>There were three types of delays: <code>procedural</code>, <code>correctional</code>, and <code>unknown</code>. The delay message indicated that the server was currently busy (<code>procedural</code> delay), that there were problems with the terminal and that the system needed to reestablish the connection (<code>correctional</code> delay) or else simply stated “please wait” (<code>unknown</code> delay).</p>
<p>At the end of the registration process, the participants were asked to estimate the delay time (in minutes) incurred during the registration process. They were also asked to provide an evaluation of the service using two measurement scales. The simulated data corresponding to this study are found in the <span class="math inline">\(\texttt{delay}\)</span> database, which contains the following variables</p>
<ul>
<li><span class="math inline">\(\texttt{time}\)</span>: delay time (in minutes) according to the participant.</li>
<li><span class="math inline">\(\texttt{eval}\)</span>: evaluation of service (standardized score).</li>
<li><span class="math inline">\(\texttt{stage}\)</span>: stage of advancement, a factor with levels <code>close to end</code> and <code>far from end</code></li>
<li><span class="math inline">\(\texttt{delay}\)</span>: type of delay, a factor with levels <code>procedural</code>, <code>correctional</code> and <code>unknown</code>.</li>
</ul>
<p>In the experimental design, the 120 participants were randomly assigned to one of these six conditions, but nine of the 120 participants were removed because they were not able to specify the type of delay that occurred. The dataset is <strong>unbalanced</strong>, meaning that the number of observations in each cell is unequal. If there had been the same number of observations in each subgroup for factors <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, the test for the effect of factor <span class="math inline">\(A\)</span> given that <span class="math inline">\(B\)</span> is already in the model would be the same as the marginal effect of <span class="math inline">\(A\)</span> only.</p>
<p>We will evaluate the effect of the factors <span class="math inline">\(\texttt{delay}\)</span> and <span class="math inline">\(\texttt{stage}\)</span> on the estimated waiting time.</p>
<p>If there is an interaction between explanatory variables, but the latter is not included in the models, the tests are misleading: one assumption of the linear model being that all of the relevant covariates have been included and their effect properly accounted for. In this example, omitting the interaction leads to effects being averaged and cancelling each other. Fitting an ANOVA model with only additive (main effects, without interaction) for the evaluation of service suggests that the factors <span class="math inline">\(\texttt{stage}\)</span> and <span class="math inline">\(\texttt{delay}\)</span> are not significant: the <span class="math inline">\(p\)</span>-values for the main effects, reported in Table <a href="linear-regression.html#tab:summaryanovadelaynointer">2.6</a>, are <span class="math inline">\(0.409\)</span> and <span class="math inline">\(0.137\)</span>. Therefore, there seemingly is no significant difference in delay time between the two stages and the three delay types.</p>
<table>
<caption>
<span id="tab:summaryanovadelaynointer">Table 2.6: </span>Analysis of variance table (Type 3 sum of square decomposition table) for the model for evaluation of service without interaction. While both factor appear not to be significant given the other, interaction plots show this is due to cancellation effects. The conclusions of the test are invalid because model assumptions are violated.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:right;">
sum of square
</th>
<th style="text-align:right;">
df
</th>
<th style="text-align:right;">
F stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
0.74
</td>
</tr>
<tr>
<td style="text-align:left;">
stage
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.69
</td>
<td style="text-align:right;">
0.41
</td>
</tr>
<tr>
<td style="text-align:left;">
delay
</td>
<td style="text-align:right;">
1.6
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2.02
</td>
<td style="text-align:right;">
0.14
</td>
</tr>
<tr>
<td style="text-align:left;">
Residuals
</td>
<td style="text-align:right;">
41.9
</td>
<td style="text-align:right;">
105
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>However, looking at the data and the mean of each sub-class paints a different portrait, as evidenced by the interaction plot of Figure <a href="linear-regression.html#fig:twowayanovadelay">2.11</a>. The score for service increases for <code>correctional</code> delays when the error occurs close to the end, but the effect is opposite for other delay types. The fact that our <span class="math inline">\(p\)</span>-values for the effects were not significative merely indicate that the effects cancelled out each other.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twowayanovadelay"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/twowayanovadelay-1.png" alt="Interactions plots for the $\texttt{delay}$ data, with jitterd observations: if there was no interaction, the curves should be parallel. These plots seem to indicate an interaction." width="70%" />
<p class="caption">
Figure 2.11: Interactions plots for the <span class="math inline">\(\texttt{delay}\)</span> data, with jitterd observations: if there was no interaction, the curves should be parallel. These plots seem to indicate an interaction.
</p>
</div>
<p>There is a significant interaction between the variables <span class="math inline">\(\texttt{delay}\)</span> and <span class="math inline">\(\texttt{stage}\)</span> on evaluation. Therefore, the effect of <span class="math inline">\(\texttt{delay}\)</span> on the variable <span class="math inline">\(\texttt{eval}\)</span> depends on the level of the variable <span class="math inline">\(\texttt{stage}\)</span> ance vice-versa. Since there is an interaction term, tests for the main effects of <span class="math inline">\(\texttt{stage}\)</span> or <span class="math inline">\(\texttt{delay}\)</span> are not of interest.</p>
<table>
<caption>
<span id="tab:summarydelaywinteraction">Table 2.7: </span>Analysis of variance table (Type 3 sum of square decomposition table). The only test of interest is that for the interaction, which is highly significant.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:right;">
sum of square
</th>
<th style="text-align:right;">
df
</th>
<th style="text-align:right;">
F stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
3.8
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
18.29
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
stage
</td>
<td style="text-align:right;">
5.3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
25.67
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
delay
</td>
<td style="text-align:right;">
8.1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
19.64
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
stage:delay
</td>
<td style="text-align:right;">
20.5
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
49.63
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
Residuals
</td>
<td style="text-align:right;">
21.3
</td>
<td style="text-align:right;">
103
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
</table>
<p>If a difference is detected at this stage, one could then proceed to compute all pairwise differences within a given level of the other categorical variable. This is equivalent to running multiple <span class="math inline">\(t\)</span>-tests, but the linear model assumes equal variance and would thus pool all observations to estimate the latter. For example, we could test if the difference between procedural and correctional delays when the error occurs far from the end of the procedure is significant. The mean differences of a factor for a given level of another factor are called <strong>simple effects</strong>. If the interaction is significant, there must be at least one pairwise difference which is also significant. However, repeated testing can inflate the Type I error, <a href="https://xkcd.com/882/">as illustrated in this comic strip</a>: if we conduct 20 independent tests and the null is true in all cases (so there are no difference between any of the sub-groups), we would still expect to reject (by mistake) 1/20 on average for tests performed at level <span class="math inline">\(\alpha\)</span>. There exists method to adjust for multiple testing.</p>
<p>While the normality assumption is less crucial for the reliability of the tests, it requires sufficient sample size in each subgroup for these (say 20), so that the central limit theorem kicks in. The tests and models typically use the pooled variance estimators: in small samples, it makes better use of the data and has higher precision than estimators of the individual variance in each subgroups. If the variance were unequal, comparisons would require extended models covered later in the term and pairwise comparisons could be based on Welch test statistic.</p>
</div>
</div>
<div id="collinearity" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Collinearity<a href="linear-regression.html#collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The linearity assumption can be interpreted broadly to mean that all relevant covariates have been included and that their effect is correctly specified in the equation of the mean. Adding superfluous covariates to a model has limited impact: if the (partial) correlation between a column vector <span class="math inline">\(\mathbf{X}_k\)</span> and the response variable <span class="math inline">\(\boldsymbol{Y}\)</span> is zero, then <span class="math inline">\(\beta_k=0\)</span> and the estimated coefficient <span class="math inline">\(\widehat{\beta}_k \approx 0\)</span> because the least square estimators are unbiased. If we include many useless variables, say <span class="math inline">\(k\)</span>, the lack of parsimony can however make interpretation more difficult. The price to pay for including the <span class="math inline">\(k\)</span> additional covariates is an increase in the variance of the estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>
<p>It is nevertheless preferable to include more variables than to forget key predictors: if we omit an important predictor, their effect may be picked up by other regressors (termed <strong>confounders</strong>) in the model with are correlated with the omitted variable. The interpretation of the other effects can be severely affected by confounders. For example, the simple linear model (or two-sample <span class="math inline">\(t\)</span>-test) for salary as a function of sex for the <code>college</code> data is invalid because sex is a confounder for rank. Since there are more men than women full professor, the mean salary difference between men and women is higher than it truly is. One way to account for this is to include control variables (such as rank), whose effect we need not be interested in, but that are necessary for the model to be adequate. We could also have used stratification, i.e., tested for wage discrimination within each academic rank. This is the reason why sociodemographic variables (sex, age, education level, etc.) are collected as part of studies.</p>
<p>A linear model is not a <a href="https://xkcd.com/552/">causal model</a>: all it does is capture the linear correlation between an explanatory variable and the response. When there are more than one explanatory, the effect of <span class="math inline">\(\mathrm{X}_j\)</span> given what has not already been explained by <span class="math inline">\(\mathbf{X}_{-j}\)</span>. Thus, if we fail to reject <span class="math inline">\(\mathscr{H}_0:\beta_j=0\)</span> in favor of the alternative <span class="math inline">\(\mathscr{H}_1: \beta_j \neq 0\)</span>, we can only say that there is no significant <em>linear</em> association between <span class="math inline">\(\mathrm{X}_j\)</span> and <span class="math inline">\(Y\)</span> once the effect of other variables included in the model has been accounted for. There are thus two scenarios: either the response is uncorrelated with <span class="math inline">\(\mathrm{X}_j\)</span> (uninteresting case, but easy to pick up by plotting both or computing linear correlation), or else there is a strong correlation between <span class="math inline">\(\mathrm{X}_j\)</span> and both the response <span class="math inline">\(Y\)</span> as well as (some) of the other explanatory variables <span class="math inline">\(\mathrm{X}_1, \ldots, \mathrm{X}_p\)</span>. This problem is termed (multi)collinearity.</p>
<p>One potential harm of collinearity is a decrease in the precision of parameter estimators. With collinear explanatories, many linear combinations of the covariates represent the response nearly as well. Due to the (near) lack of identifiability, the estimated coefficients become numerically unstable and this causes an increase of the standard errors of the parameters. The predicted or fitted values are unaffected. Generally, collinearity leads to high estimated standard errors and the regression coefficients can change drastically when new observations are included in the model, or when we include or remove explanatories. The individual <span class="math inline">\(\beta\)</span> coefficients may not be statistically significant, but the global <span class="math inline">\(F\)</span>-test will indicate that some covariates are relevant for explaining the response. This however would also be the case if there are predictors with strong signal, so neither is likely to be useful to detect issues.</p>
<p>The added-variable plot shows the relation between the response <span class="math inline">\(Y\)</span> and an explanatory <span class="math inline">\(\mathrm{X}_j\)</span> after accounting for other variables: the slope <span class="math inline">\(\widehat{\beta}_j\)</span> of the simple linear regression is the same of the full model. A similar idea can be used to see how much of <span class="math inline">\(\mathrm{X}_j\)</span> is already explained by the other variables. For a given explanatory variable <span class="math inline">\(\mathrm{X}_j\)</span>, we define its <strong>variance inflation factor</strong> as <span class="math inline">\(\mathsf{VIF}(j)=(1-R^2(j))^{-1}\)</span>, where <span class="math inline">\(R^2(j)\)</span> is the coefficient of determination of the model obtained by regressing <span class="math inline">\(\mathrm{X}_j\)</span> on all the other explanatory variables, i.e.,
<span class="math display">\[\begin{align*}
\mathrm{X}_j = \beta^{\star}_0 + \beta^{\star}_1 \mathrm{X}_1 + \cdots + \beta^{\star}_{j-1} \mathrm{X}_{j-1} + \beta^{\star}_{j+1} \mathrm{X}_{j+1} + \cdots + \beta^{\star}_p\mathrm{X}_p + \varepsilon^{\star}
\end{align*}\]</span>
By definition, <span class="math inline">\(R^2(j)\)</span> represents the proportion of the variance of <span class="math inline">\(\mathrm{X}_j\)</span> that is explained by all the other predictor variables. Large variance inflation factors are indicative of problems (typically covariates with <span class="math inline">\(\mathsf{VIF}&gt;10\)</span> require scrutinity, and values in the hundreds or more indicate serious problems).</p>
<p>Added-variable plots can also serve as diagnostics, by means of comparison of the partial residuals with a scatterplot of the pair <span class="math inline">\((Y, \mathrm{X}_j)\)</span>; if the latter shows very strong linear relation, but the slope is nearly zero in the added-variable plot, this hints that collinearity is an issue.</p>
<p>What can one do about collinearity? If the goal of the study is to develop a predictive model and we’re not interested in the parameters themselves, then we don’t need to do anything. Collinearity is not a problem for the overall model: it’s only a problem for the individual effects of the variables. Their joint effect is still present in the model, regardless of how the individual effects are combined.</p>
<p>If we are interested in individual parameter estimates, for example,
to see how (and to what extent) the predictor variables explain the behaviour of <span class="math inline">\(Y\)</span>, then things get more complicated. Collinearity only affects the variables that are strongly correlated with one another, so we only care if it affects one or more of the variables of interest. There sadly is no good solution to the problem. One could</p>
<ul>
<li>try to obtain more data, so as to reduce the effects of collinearity appearing in specific samples or that are due to small sample size.</li>
<li>create a composite score by somehow combining the variables showing collinearity.</li>
<li>remove one or more of the collinear variables. You need to be careful when doing this not to end up with a misspecified model.</li>
<li>use penalized regression. If <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span> is (nearly) not invertible, this may restore the uniqueness of the solution. Penalties introduce bias, but can reduce the variance of the estimators <span class="math inline">\(\boldsymbol{\beta}\)</span>. Popular choices include ridge regression (with an <span class="math inline">\(l_2\)</span> penalty), lasso (<span class="math inline">\(l_1\)</span> penalty), but these require adjustment in order to get valid inference.</li>
</ul>
<p>Whatever the method, it’s important to understand that it can be very difficult (and sometimes impossible) to isolate the individual effect of a predictor variable strongly correlated with other predictors.</p>
<div class="example">
<p><span id="exm:collegedatcollinear" class="example"><strong>Example 2.3  (Collinearity in the $\texttt{college}$ data) </strong></span>We consider the <code>college</code> data analysis and include all the covariates in the database, including <code>years</code>, the number of years since PhD. One can suspect that, unless a professor started his or her career elsewhere before moving to the college, they will have nearly the same years of service. In fact, the correlation between the two variables, <code>service</code> and <code>years</code> is <code>r cor(college$service, college$years)</code>. The variance inflation factor for the five covariates</p>
<p>For categorical variables, the variance inflation factor definition would normally yield for each level a different value; an alternative is the generalized variance inflation factor <span class="citation">(<a href="references.html#ref-Fox:1992" role="doc-biblioref">Fox and Monette 1992</a>)</span>. Here, we are interested in gender disparities, so the fact that both service and field are
strongly correlated is not problematic, since the <span class="math inline">\(\mathsf{VIF}\)</span> for <span class="math inline">\(\texttt{sex}\)</span> is not high and the other variables are there to act as control and avoid confounders.</p>
</div>
<table>
<caption>
<span id="tab:unnamed-chunk-1">Table 2.8: </span>(Generalized) variance inflation factor for the <span class="math inline">\(\texttt{college}\)</span> data.
</caption>
<thead>
<tr>
<th style="text-align:right;">
service
</th>
<th style="text-align:right;">
years
</th>
<th style="text-align:right;">
rank
</th>
<th style="text-align:right;">
sex
</th>
<th style="text-align:right;">
field
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5.92
</td>
<td style="text-align:right;">
7.52
</td>
<td style="text-align:right;">
2.01
</td>
<td style="text-align:right;">
1.03
</td>
<td style="text-align:right;">
1.06
</td>
</tr>
</tbody>
</table>
</div>
<div id="graphical-analysis-of-residuals" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Graphical analysis of residuals<a href="linear-regression.html#graphical-analysis-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have fit models and tested significance of the parameters without checking the model assumptions. The correctness of statements about the <span class="math inline">\(p\)</span>-values and confidence intervals depend on the (approximate) validity of the model assumptions, which all stem from the distributional assumption for the error, assumed to be independent and identically distributed with <span class="math inline">\(\varepsilon_i \stackrel{\cdot}{\sim} \mathsf{No}(0, \sigma^2)\)</span>. This compact mathematical description can be broken down into four assumptions.</p>
<ul>
<li>normality of the errors</li>
<li>linearity: the mean of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\beta_0 + \beta_1\mathrm{X}_1 + \cdots + \beta_p \mathrm{X}_p\)</span>.</li>
<li>homoscedasticity: the error variance is constant</li>
<li>independence of the errors.</li>
</ul>
<p>This section reviews the assumptions made in order to allow statistical inference using the linear model and different residuals that serve as building blocks for graphical diagnostics. We investigate the consequences of violation of these assumptions and outline potential mitigation strategies, many of which are undertaken in other chapters.</p>
<p>When we perform an hypothesis test, we merely fail to reject the null hypothesis, either because the latter is true or else due to lack of evidence. The same goes for checking the validity of model assumptions: scientific reasoning dictates that we cannot know for certain whether these hold true. Our strategy is therefore to use implications of the linear model assumptions to create graphical diagnostic tools, so as to ensure that there is no gross violation of these hypothesis. However, it is important to beware of over-interpreting diagnostic plots: the human eye is very good at finding spurious patterns.</p>
<p>Other good references for the material in this section is:</p>
<ul>
<li><a href="https://otexts.com/fpp2/regression-evaluation.html">Forecasting: Principles and Practice, section 5.3</a></li>
</ul>
<div id="residuals" class="section level3 hasAnchor" number="2.9.1">
<h3><span class="header-section-number">2.9.1</span> Residuals<a href="linear-regression.html#residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Residuals are predictions of the errors <span class="math inline">\(\varepsilon\)</span> and represent the difference between the observed value <span class="math inline">\(Y_i\)</span> and the estimated value on the line.
The ordinary residuals are
<span class="math display">\[\begin{align*}
e_i=Y_i-\widehat{Y}_i, \qquad i =1, \ldots, n.
\end{align*}\]</span>
The sum of the ordinary residuals is always zero by construction if the model includes an intercept, meaning <span class="math inline">\(\overline{e} = 0\)</span>.</p>
<p>Not all observations contribute equally to the adjustment of the fitted hyperplane. The geometry of least squares shows that the residuals are orthogonal to the fitted values, and <span class="math inline">\(\boldsymbol{e} = (\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\boldsymbol{Y}\)</span>, where <span class="math inline">\(\mathbf{H}_{\mathbf{X}}=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span> is an <span class="math inline">\(n \times n\)</span> projection matrix that spans the <span class="math inline">\(p\)</span>-dimensional linear combination of the columns of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathcal{S}(\mathbf{X})\)</span>. If <span class="math inline">\(\mathsf{Va}(\boldsymbol{Y}) = \sigma^2\mathbf{I}_n\)</span>, it follows that <span class="math inline">\(\mathsf{Va}(\boldsymbol{e})=\sigma^2(\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\)</span> because <span class="math inline">\((\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\)</span> is a projection matrix, therefore idempotent and symmetric. Because the matrix has rank <span class="math inline">\(n-p\)</span>, the ordinary residuals cannot be independent from one another.</p>
<p>If the errors are independent and homoscedastic, the ordinary residual <span class="math inline">\(e_i\)</span> has variance <span class="math inline">\(\sigma^2(1-h_{i})\)</span>, where the leverage term <span class="math inline">\(h_i =(\mathbf{H}_{\mathbf{X}})_{ii} = \mathbf{x}_i (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal entry of the projection matrix <span class="math inline">\((\mathbf{H}_{\mathbf{X}})\)</span> and <span class="math inline">\(\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th row of the model matrix corresponding to observation <span class="math inline">\(i\)</span>.</p>
<p>We thus conclude that ordinary residuals do not all have the same standard deviation and they are not independent. This is problematic, as we cannot make meaningful comparisons: points with low leverage are bound to deviate more from the fitted model than others. To palliate to this, we can standardize the residuals so each has the same variance under the null of independent homoscedastic errors — the leverage terms <span class="math inline">\(h_i\)</span> are readily calculated from the model matrix <span class="math inline">\(\mathbf{X}\)</span>.
The only remaining question is how to estimate the variance. If we use the <span class="math inline">\(i\)</span>th observation to estimate both the residual and the variance, we introduce additional dependence. A better way is remove the <span class="math inline">\(i\)</span>th observation and refit the model with the <span class="math inline">\(n-1\)</span> remaining observations to get of <span class="math inline">\(s^2_{(-i)}\)</span> (there are tricks and closed-form expressions for these, so one doesn’t need to fit <span class="math inline">\(n\)</span> different linear models). The jacknife studentized residual <span class="math inline">\(r_i = e_i/\{s_{(-i)}(1-h_i)\}\)</span>, also termed externally studentized residuals, are not independent, but they are identically distributed and follow a Student distribution with <span class="math inline">\(n-p-2\)</span> degrees of freedom.
These can be obtained in <strong>R</strong> with the command <code>rstudent</code>, also in <strong>SAS</strong>.</p>
<p>When to use which residuals? By construction, the vector of ordinary residuals <span class="math inline">\(\boldsymbol{e}\)</span> is orthogonal to the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and also to each column of the model matrix <span class="math inline">\(\mathbf{X}\)</span>: this means a simple linear regression of <span class="math inline">\(\boldsymbol{e}\)</span> with any of these as covariate gives zero intercept and zero slope. However, residual patterns due to forgotten interactions, nonlinear terms, etc. could be picked up from pair plots of ordinary residuals against the explanatories.</p>
<p>While the jackknife studentized residuals <span class="math inline">\(r_i\)</span> are not orthogonal, they are not very different. Use jackknife residuals <span class="math inline">\(\boldsymbol{r}\)</span> to check for equality of variance and distributional assumptions (e.g., using quantile-quantile plots).</p>
<p>One thus typically uses ordinary residuals <span class="math inline">\(\boldsymbol{e}\)</span> for plots of fitted values/explanatories against residuals and otherwise jackknife studentized residuals for any other graphical diagnostic plot.</p>
</div>
<div id="leverage-and-outliers" class="section level3 hasAnchor" number="2.9.2">
<h3><span class="header-section-number">2.9.2</span> Leverage and outliers<a href="linear-regression.html#leverage-and-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The leverage <span class="math inline">\(h_i\)</span> of observation <span class="math inline">\(i\)</span> measures its impact on the least square fit, since we can write <span class="math inline">\(h_i = \partial \widehat{y}_i/\partial y_i\)</span>. Leverage values tell us how much each point impacts the fit: they are strictly positive, are bounded below by <span class="math inline">\(1/n\)</span> and above by <span class="math inline">\(1\)</span>. The sum of the leverage values is <span class="math inline">\(\sum_{i=1}^n h_i=p+1\)</span>: in a good design, each point has approximately the same contribution, with average weight <span class="math inline">\((p+1)/n\)</span>.</p>
<p>Points with high leverage are those that have unusual combinations of explanatories. An influential observation (<span class="math inline">\(h_i\approx 1\)</span>) pulls the fitted hyperplane towards itself so that <span class="math inline">\(\hat{y}_i \approx y_i\)</span>. As a rule of thumb, points with <span class="math inline">\(h_i&gt; 2(p+1)/n\)</span> should be scrutinized.</p>
<p>It is important to distinguish betwen <strong>influential</strong> observations (which have unusual <span class="math inline">\(\mathbf{x}\)</span> value, i.e., far from the overall mean) and <strong>outliers</strong> (unusual value of the response <span class="math inline">\(y\)</span>).
If an observation is both an outlier and has a high leverage, it is problematic.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:outliers"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/outliers-1.png" alt="Outlier and influential observation. The left panel shows an outlier, whereas the right panel shows an influential variable (rightmost $x$ value)." width="70%" />
<p class="caption">
Figure 2.12: Outlier and influential observation. The left panel shows an outlier, whereas the right panel shows an influential variable (rightmost <span class="math inline">\(x\)</span> value).
</p>
</div>
<p>If influential observations can be detected by inspecting the leverage of each observation, outliers are more difficult to diagnose.</p>
<p>An outlier stands out from the rest of the observations, either because it has an usual response value, or because it falls far from the regression surface.
Loosely speaking, an outlier is an unusual values of <span class="math inline">\(Y\)</span> for a given combination of <span class="math inline">\(\mathbf{X}\)</span> that ``stand out’’ from the rest.
Outliers can be detected during the exploratory data analysis or picked-up in residual plots (large values of <span class="math inline">\(|e_i|\)</span> in plots of fitted versus residuals) or added-variable plots. One could potentially test whether an jackknife studentized residual is an outlier (adjusting for the fact we would consider only largest values). One can also consider Cook’s distance, <span class="math inline">\(C_j\)</span>, a statistic giving the scaled distance between the fitted values <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> and the fitted values for the model with all but the <span class="math inline">\(j\)</span>th observation, <span class="math inline">\(\hat{\boldsymbol{y}}^{(-j)}\)</span>,
<span class="math display">\[\begin{align*}
C_j = \frac{1}{(p+1)S^2} \sum_{i=1}^n \left\{\hat{y}_i - \hat{y}_{i}^{(-j)}\right\}^2
\end{align*}\]</span>
Large values of <span class="math inline">\(C_j\)</span> indicate that its residual <span class="math inline">\(e_j\)</span> is large relative to other observations or else its leverage <span class="math inline">\(h_j\)</span> is high. A rule of thumb is to consider points for which <span class="math inline">\(C_j &gt; 4/(n-p-1)\)</span>. In practice, if two observations are outlying and lie in the same region, their Cook distance will be halved.</p>
<p>Outliers and influential observations should not be disregarded because they don’t comply with the model, but require further investigation. They may motivate further modelling for features not accounted for. It is also useful to check for registration errors in the data (which can be safely discarded).</p>
<p>Except in obvious scenarios, unusual observations should not be discarded. In very large samples, the impact of a single outlier is hopefully limited. Transformations of the response may help reduce outlyingness. Otherwise, alternative objective functions (as those employed in robust regression) can be used; these downweight extreme observations, at the cost of efficiency.</p>
</div>
<div id="diagnostic-plots" class="section level3 hasAnchor" number="2.9.3">
<h3><span class="header-section-number">2.9.3</span> Diagnostic plots<a href="linear-regression.html#diagnostic-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We review the assumptions in turn and discuss what happens when the assumptions fail to hold.</p>
<div id="independence-assumption" class="section level4 hasAnchor" number="2.9.3.1">
<h4><span class="header-section-number">2.9.3.1</span> Independence assumption<a href="linear-regression.html#independence-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Usually, the independence of the observations follows directly from the type of sampling used — this assumption is implicitly true if the observations were taken from a <em>random sample</em> from the population. This is generally not the case for longitudinal data, which contains repeated measures from the same individuals across time. Likewise, time series are bound not to have independent observations. If we want to include all the time points in the analysis, we must take into account the possible dependence (correlation) between observations. If we ignore correlation, the estimated standard errors are too small relative to the truth, so the effective sample size is smaller than number of observations.</p>
<p>The first source of dependence is clustered data, meaning measurements taken from subjects that are not independent from one another (family, groups, etc.)
More generally, correlation between observations arises from time dependence, roughly categorized into</p>
<ul>
<li>longitudinal data: repeated measurements are taken from the same subjects (few time points)</li>
<li>time series: observations observed at multiple time periods (many time points). Time series require dedicated models not covered in this course.</li>
</ul>
<p>Because of autocorrelation, positive errors tend to be followed by positive errors, etc. We can plot the residuals as a function of time, and a scatterplot of lagged residuals <span class="math inline">\(e_i\)</span> versus <span class="math inline">\(e_{i-1}\)</span> (<span class="math inline">\(i=2, \ldots, n\)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:timeresidplot"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/timeresidplot-1.png" alt="Lagged residual plots: there is no evidence against independence in the left panel, whereas the right panel shows positively correlated residuals." width="70%" />
<p class="caption">
Figure 2.13: Lagged residual plots: there is no evidence against independence in the left panel, whereas the right panel shows positively correlated residuals.
</p>
</div>
<p>However, lagged residuals plots only show dependence at lag one between observations. For time series, we can look instead at a correlogram, i.e., a bar plot of the correlation between two observations <span class="math inline">\(h\)</span> units apart as a function of the lag <span class="math inline">\(h\)</span> <span class="citation">(<a href="references.html#ref-Brockwell/Davis:2016" role="doc-biblioref">Brockwell and Davis 2016</a>, Definition 1.4.4)</span>.</p>
<p>For <span class="math inline">\(y_1, \ldots, y_n\)</span> and constant time lags <span class="math inline">\(h=0, 1, \ldots\)</span> units, the autocorrelation at lag <span class="math inline">\(h\)</span> is
<span class="math display">\[\begin{align*}
r(h) = \frac{\gamma(h)}{\gamma(0)}, \qquad \gamma(h) = \frac{1}{n}\sum_{i=1}^{n-|h|} (y_i-\overline{y})(y_{i+h}) - \overline{y})
\end{align*}\]</span></p>
<p>If the series is correlated, the sample autocorrelation will likely fall outside of the pointwise confidence intervals, as shown in Figure <a href="linear-regression.html#fig:correlogram">2.14</a>. Presence of autocorrelation requires modelling the correlation between observations explicitly using dedicated tools from the time series literature that are covered in MATH 60638. We will however examine <span class="math inline">\(\mathsf{AR}(1)\)</span> models as part of the chapter on longitudinal data.</p>
<p>When observations are positively correlated, the estimated standard errors reported by the software are too small. This means we are overconfident and will reject the null hypothesis more often then we should if the null is true (inflated Type I error, or false positive).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correlogram"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/correlogram-1.png" alt="Correlogram of independent observations (left) and the ordinary residuals of the log-linear model fitted to the air passengers data (right). While the mean model of the latter is seemingly correctly specified, there is residual dependence between monthly observations and yearly (at lag 12). The blue lines give approximate pointwise 95\% confidence intervals for white noise (uncorrelated observations)." width="70%" />
<p class="caption">
Figure 2.14: Correlogram of independent observations (left) and the ordinary residuals of the log-linear model fitted to the air passengers data (right). While the mean model of the latter is seemingly correctly specified, there is residual dependence between monthly observations and yearly (at lag 12). The blue lines give approximate pointwise 95% confidence intervals for white noise (uncorrelated observations).
</p>
</div>
</div>
<div id="linearity-assumption" class="section level4 hasAnchor" number="2.9.3.2">
<h4><span class="header-section-number">2.9.3.2</span> Linearity assumption<a href="linear-regression.html#linearity-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The second assumption of the linear model is that of linearity, which means that the mean model is correctly specified, all relevant covariates have been included and their effect is correctly specified.
To check that the response surface of the linear model is adequate, we plot <span class="math inline">\(e_i\)</span> against <span class="math inline">\(\widehat{y}_i\)</span> or <span class="math inline">\(\mathrm{X}_{ij}\)</span> (for <span class="math inline">\(j=1, \ldots, p\)</span>). Since the linear correlation between <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> (or <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\mathbf{X}_j\)</span>) is zero by construction, patterns (e.g., quadratic trend, cycles, changepoints) are indicative of misspecification of the mean model. One can add a smoother to detect patterns. Figure <a href="linear-regression.html#fig:regdiaglin">2.15</a> shows three diagnostics plots, the second of which shows no pattern in the residuals, but skewed fitted values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regdiaglin"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/regdiaglin-1.png" alt="Scatterplots of residuals against fitted values. The first two plots show no departure from linearity (mean zero). The third plot shows a clear quadratic pattern, suggesting the mean model is misspecified. Note that the distribution of the fitted value need not be uniform, as in the second panel which shows more high fitted values." width="70%" />
<p class="caption">
Figure 2.15: Scatterplots of residuals against fitted values. The first two plots show no departure from linearity (mean zero). The third plot shows a clear quadratic pattern, suggesting the mean model is misspecified. Note that the distribution of the fitted value need not be uniform, as in the second panel which shows more high fitted values.
</p>
</div>
<p>If there is residual structure in plots of ordinary residuals against either (a) the fitted values or (b) the explanatory variables, a more complex model can be adjusted including interactions, nonlinear functions, If the effect of an explanatory variable is clearly nonlinear and complicated, smooth terms could be added (we won’t cover generalized additive models in this course).</p>
<p>Plotting residuals against left-out explanatory variables can also serve to check that all of the explanatory power of the omitted covariate is already explained by the columns of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>If an important variable has been omitted and is not available in the dataset, then the effect of that variable is captured by both the errors (the portion orthogonal to the model matrix <span class="math inline">\(\mathbf{X}\)</span>, i.e., unexplained by the covariates included in the model) and the remaining part is captured by other explanatories of the model that are correlated with the omitted variable. These variables can act as confounders. There is little that can be done in either case unless the data for the omitted variable are available, but subject-specific knowledge may help make sense of the results.</p>
</div>
<div id="homoscedasticity-assumption" class="section level4 hasAnchor" number="2.9.3.3">
<h4><span class="header-section-number">2.9.3.3</span> Homoscedasticity assumption<a href="linear-regression.html#homoscedasticity-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If the variance of the errors is the same for all observations, that of the observations <span class="math inline">\(Y\)</span> is also constant. The most common scenarios for heteroscedasticity are increases in variance with the response, or else variance that depends on explanatory variables <span class="math inline">\(\mathbf{X}\)</span>, most notably categorical variables. For the former, a log-transform (or Box–Cox transformation) can help stabilize the variance, but we need the response to be positive. For the latter, we can explicitly model that variance and we will see how to include different variance per group later on. A popular strategy in the econometrics literature, is to use robust (inflated) estimators of the standard errors such as <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">White’s sandwich estimator of the variance</a>.</p>
<p>If the residuals (or observations) are heteroscedastic (non constant variance), the estimated effects of the variables (the <span class="math inline">\(\beta\)</span> parameters) are still valid in the sense that the ordinary least squares estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is unbiased. However, the estimated standard errors of the <span class="math inline">\(\widehat{\beta}\)</span> are no longer reliable and, consequently, the confidence intervals and the hypothesis tests for the model parameters will be incorrect. Indeed, if the variance of the errors differs from one observation to the next, we will estimate an average of the different variance terms. The standard errors of each term are incorrect (too small or too large) and the conclusions of the tests (<span class="math inline">\(p\)</span>-values) will be off because the formulas of both <span class="math inline">\(t\)</span>-test and <span class="math inline">\(F\)</span>-test statistics include estimates of <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<p>Looking at the plot of jackknife studentized residuals against regressors (or fitted values) is instructive — for example, we often see a funnel pattern when there is an increase in variance in the plot of the jackknife studentized residuals against fitted value, or else in boxplots with a categorical variable as in Figure <a href="linear-regression.html#fig:diagfitvalhomosce">2.17</a>.
However, if we want to fit a local smoother to observe trends, it is better to plot the absolute value of the jackknife studentized residuals against regressors or observation number.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:residhomoscedastic"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/residhomoscedastic-1.png" alt="Plot of the absolute value of jackknife studentized residuals against observation number. The left panel is typical of homoscedastic data, whereas the right panel indicates an increase in the variance." width="70%" />
<p class="caption">
Figure 2.16: Plot of the absolute value of jackknife studentized residuals against observation number. The left panel is typical of homoscedastic data, whereas the right panel indicates an increase in the variance.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diagfitvalhomosce"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagfitvalhomosce-1.png" alt="Plot of jackknife studentized residuals against fitted value (left) and categorical explanatory (right). Both clearly display heteroscedasticity." width="70%" />
<p class="caption">
Figure 2.17: Plot of jackknife studentized residuals against fitted value (left) and categorical explanatory (right). Both clearly display heteroscedasticity.
</p>
</div>
</div>
<div id="normality-assumption" class="section level4 hasAnchor" number="2.9.3.4">
<h4><span class="header-section-number">2.9.3.4</span> Normality assumption<a href="linear-regression.html#normality-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The normality assumption is mostly for convenience: if the errors are assumed normally distributed, then the least square and the maximum likelihood estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> coincide.
The maximum likelihood estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> are asymptotically normal under mild conditions on the model matrix and <span class="math inline">\(t\)</span>-tests are robust to departure of the normality assumption. This means that inference is valid in large samples, regardless of the distribution of the errors/residuals (even if the null distribution are not exact). It is important to keep in mind that, for categorical explanatory variables, the sample size in each group must be sufficiently large for the central limit theorem to kick in.</p>
<p>Sometimes, transformations can improve normality: if the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate. This can be assessed by looking at the quantile-quantile plot of the externally studentized residuals. If the response <span class="math inline">\(Y\)</span> is not continuous (including binary, proportion or count data), linear models give misleading answers and generalized linear models are more suitable.</p>
<p>The inference will be valid for large samples even if the errors are not normally distributed by virtue of the central limit theorem. If the errors <span class="math inline">\(\varepsilon_i \sim \mathsf{No}(0, \sigma^2)\)</span>, then the jacknnife studentized residuals should follow a Student distribution, with <span class="math inline">\(r_i \sim \mathsf{St}(n-p-2)\)</span>, (identically distributed, but not independent). A Student quantile-quantile plot can thus be used to check the assumption (and for <span class="math inline">\(n\)</span> large, the normal plotting positions could be used as approximation if <span class="math inline">\(n-p&gt; 50\)</span>). One can also plot a histogram of the residuals. Keep in mind that if the mean model is not correctly specified, some residuals may incorporate effect of leftover covariates.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qqplotresid"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/qqplotresid-1.png" alt="Histogram (left) and Student quantile-quantile plot (right) of the jackknife studentized residuals. The left panel includes a kernel density estimate (black), with the density of Student distribution (blue) superimposed. The right panel includes pointwise 95\% confidence bands calculated using a bootstrap." width="70%" />
<p class="caption">
Figure 2.18: Histogram (left) and Student quantile-quantile plot (right) of the jackknife studentized residuals. The left panel includes a kernel density estimate (black), with the density of Student distribution (blue) superimposed. The right panel includes pointwise 95% confidence bands calculated using a bootstrap.
</p>
</div>
<p>Quantile-quantile plots are discussed in Section <a href="complement.html#diagramme-qq">A.2.5</a>, but their interpretation requires training. For example, Figure <a href="linear-regression.html#fig:qqplotsbad">2.19</a> shows many common scenarios that can be diagnosed using quantile-quantile plots: discrete data is responsible for staircase patterns, positively skewed data has too high low quantiles and too low high quantiles relative to the plotting positions, heavy tailed data have high observations in either tails and bimodal data leads to jumps in the plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qqplotsbad"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/qqplotsbad-1.png" alt="Quantile-quantile plots of non-normal data, showing typical look of behaviour of discrete (top left), heavy tailed  (top right), skewed (bottom left) and bimodal data (bottom right)." width="70%" />
<p class="caption">
Figure 2.19: Quantile-quantile plots of non-normal data, showing typical look of behaviour of discrete (top left), heavy tailed (top right), skewed (bottom left) and bimodal data (bottom right).
</p>
</div>
<div class="example">
<p><span id="exm:diagplotcollege" class="example"><strong>Example 2.4  (Diagnostic plots for the $\texttt{college}$ data.) </strong></span>We can look at the <code>college</code> data to see if the linear model assumptions hold.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diagplotscollege"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagplotscollege-1.png" alt="Diagnostic plots for the college data example: ordinary residuals against fitted values (top left), absolute value of the jacknnife studentized residuals against fitted values (top right), box and whiskers plot of jacknnife studentized residuals (bottom left) and detrended Student quantile-quantile plot (bottom right). There is clear group heteroscedasticity." width="70%" />
<p class="caption">
Figure 2.20: Diagnostic plots for the college data example: ordinary residuals against fitted values (top left), absolute value of the jacknnife studentized residuals against fitted values (top right), box and whiskers plot of jacknnife studentized residuals (bottom left) and detrended Student quantile-quantile plot (bottom right). There is clear group heteroscedasticity.
</p>
</div>
<p>Based on the plots of Figure <a href="linear-regression.html#fig:diagplotscollege">2.20</a>, we find that there is residual heteroscedasticity, due to rank. Since the number of years in the first rank is limited and all assistant professors were hired in the last six years, there is less disparity in their income. It is important not to mistake the pattern on the <span class="math inline">\(x\)</span>-axis for the fitted value (due to the large effect of rank and field, both categorical variable) with patterns in the residuals (none apparent). Fixing the heteroscedasticity would correct the residuals and improve the appearance of the quantile-quantile plot.</p>
</div>
</div>
</div>
<div id="transformation-response" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Transformation of the response<a href="linear-regression.html#transformation-response" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This ultimate section deals with strategies for fixing the linear model if we detected non-normality. If the response is strictly positive, an option is to use the Box–Cox transformation presented in Section <a href="likelihood.html#exm:boxcox">3.3</a> and fit a linear model to a transformation of <span class="math inline">\(Y\)</span>,
<span class="math display">\[\begin{align*}
y_i(\lambda)= \begin{cases}
(y^{\lambda}-1)/\lambda, &amp; \lambda \neq 0\\
\ln(y), &amp; \lambda=0.
\end{cases}
\end{align*}\]</span>
We then fit a linear model over a grid of <span class="math inline">\(\lambda\)</span>, using the profile likelihood to select the optimal value for the transformation. Note that the latter will depend on the covariates present in the model; the example with the <span class="math inline">\(\texttt{college}\)</span> data already shows that the diagnostics of normality is impacted by departures from the other hypothesis. The cases <span class="math inline">\(\lambda=1\)</span> (identity) and <span class="math inline">\(\lambda=0\)</span> (log-linear model) are perhaps the most important because they yield interpretable coefficients.</p>
<p>If the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate and the parameters can be interpreted.
Theory sometimes dictates a multiplicative model: for example, the Cobb–Douglas production function in economics is
<span class="math inline">\(P=\alpha L^{\beta_1}C^{\beta_2}\)</span>, where <span class="math inline">\(P\)</span> stands for production, <span class="math inline">\(L\)</span> for labor and <span class="math inline">\(C\)</span> for capital; all inputs are positive, so taking a log-transform yields a model that is linear in <span class="math inline">\(\beta\)</span>, with <span class="math inline">\(\beta_0=\ln(\alpha)\)</span>.</p>
<p>We can rewrite the model in the original response scale as
<span class="math display">\[\begin{align*}
Y = \exp\left(\beta_0+\sum_{j=1}^p\beta_j\mathrm{X}_j +  \varepsilon \right) = \exp\left(\beta_0+ \sum_{j=1}^p\beta_j\mathrm{X}_j\right)\cdot \exp(\varepsilon),
\end{align*}\]</span>
and thus
<span class="math display">\[\begin{align*}
\mathsf{E}(Y \mid \mathbf{X}) = \exp(\beta_0 +\beta_1 \mathrm{X}_1 +\cdots + \beta_p\mathrm{X}_p ) \times \mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}.
\end{align*}\]</span>
If <span class="math inline">\(\varepsilon \mid \mathbf{X} \sim \mathsf{No}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}= \exp(\mu+\sigma^2/2)\)</span> and <span class="math inline">\(\exp(\varepsilon)\)</span> follows a log-normal distribution.</p>
<p>In order to interpret the parameters, we can compare the ratio of <span class="math inline">\(\mathsf{E}(Y \mid \mathrm{X}_1=x+1)\)</span> to <span class="math inline">\(\mathsf{E}(Y \mid \mathrm{X}_1=x)\)</span>,
<span class="math display">\[\begin{align*}
\frac{\mathsf{E}(Y \mid \mathrm{X}_1=x+1, \mathrm{X}_2, \ldots, \mathrm{X}_p)}{\mathsf{E}(Y \mid \mathrm{X}_1=x,  \mathrm{X}_2, \ldots, \mathrm{X}_p)} = \frac{\exp\{\beta_1(x+1)\}}{\exp(\beta_1 x)} = \exp(\beta_1).
\end{align*}\]</span>
Thus, <span class="math inline">\(\exp(\beta_1)\)</span> represents the ratio of the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(\mathrm{X}_1=x+1\)</span> in comparison to that when <span class="math inline">\(\mathrm{X}_1=x\)</span>, <em>ceteris paribus</em> (and provided this statement is meaningful). We can interpret <span class="math inline">\(\exp(\beta_1)\)</span> as the multiplicative effect of <span class="math inline">\(\mathrm{X}_1\)</span> on the mean of <span class="math inline">\(Y\)</span>: increasing <span class="math inline">\(\mathrm{X}_1\)</span> by one unit causes <span class="math inline">\(Y\)</span> to increase by a factor of <span class="math inline">\(\exp(\beta_1)\)</span>, on average.</p>
<p>Sometimes, we may wish to consider a log transformation of both the response and some of the continuous positive explanatories, when this make sense.</p>
<p>Consider the case where both <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathrm{X}_1\)</span> is log-transformed, so the equation for the mean on the original data scale reads
<span class="math display">\[\begin{align*}
Y= \mathrm{X}_1^{\beta_1}\exp(\beta_0 + \beta_2\mathrm{X}_2 + \cdots + \beta_p\mathrm{X}_p + \varepsilon)
\end{align*}\]</span>
Taking the derivative of the left hand side with respect to <span class="math inline">\(\mathrm{X}_1&gt;0\)</span>, we get
<span class="math display">\[\begin{align*}
\frac{\partial Y}{\partial \mathrm{X}_1}&amp;= \beta_1 \mathrm{X}_1^{\beta_1-1}\exp(\beta_0 + \beta_2\mathrm{X}_2 + \cdots + \beta_p\mathrm{X}_p + \varepsilon)
\\&amp;= \frac{\beta_1 Y}{\mathrm{X}_1}
\end{align*}\]</span>
and thus we can rearrange the expression so that
<span class="math display">\[\begin{align*}
\frac{\partial \mathrm{X}_1}{\mathrm{X}_1}\beta_1 = \frac{\partial Y}{Y};
\end{align*}\]</span>
this is a partial <strong>elasticity</strong>, so <span class="math inline">\(\beta_1\)</span> is interpreted as a <span class="math inline">\(\beta_1\)</span> percentage change in <span class="math inline">\(Y\)</span> for each percentage increase of <span class="math inline">\(\mathrm{X}_1\)</span>, <em>ceteris paribus</em>.</p>
<div class="example">
<p><span id="exm:poisonboxcox" class="example"><strong>Example 2.5  </strong></span>The paper of Box and Cox consider survival time for 48 animals based on a randomized trial; these data are analyzed in Example 8.25 of <span class="citation">Davison (<a href="references.html#ref-SM:2008" role="doc-biblioref">2008</a>)</span>. Three poisons were administered with four treatments; each factor combination contained four animals, chosen at random. There is strong evidence that both the choice of poison and treatment affect survival time.</p>
<p>We could consider a two-way analysis of variance model for these data without interaction, given the few observations for each combination. The model would be of the form
<span class="math display">\[\begin{align*}
Y &amp;= \beta_0 + \beta_1 \texttt{poison}_2 + \beta_2\texttt{poison}_3  +\beta_3\texttt{treatment}_2 \\ &amp;\qquad+ \beta_4\texttt{treatment}_3
+\beta_5\texttt{treatment}_4 + \varepsilon
\end{align*}\]</span></p>
<p>The plot of fitted values against residuals shows that the model is not additive; there is also indications that the variance increases with the mean response. The model is inadequate: lowest survival times are underpredicted, meaning the residuals are positive and likewise the middle responses is positive. A formal test of non-additivity based on constructed variables further point towards non-additivity <span class="citation">(<a href="references.html#ref-SM:2008" role="doc-biblioref">Davison 2008</a>, Example 8.24)</span>. Overall, the model fit is poor and any conclusion drawn from it dubious.</p>
<p>One could consider using a Box–Cox to find a suitable transformation of the residuals so as to improve normality. The profile log likelihood at the bottom left of Figure <a href="linear-regression.html#fig:poisonplots">2.21</a> suggests that <span class="math inline">\(\lambda\approx -1\)</span> would be a good choice. This has the benefit of being interpretable, as the reciprocal response <span class="math inline">\(Y^{-1}\)</span> corresponds to the speed of action of the poison depending on both poison type and treatment. The diagnostics plots also indicate that the model for the reciprocal has no residual structure and the variance appears constant.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poisonplots"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/poisonplots-1.png" alt="Diagnostic plots for the poison data. The top panel shows the ordinary residuals for the linear model for survival time as a function of poison and treatment, with jittered observations. The middle left plot shows the fitted values against residuals, which display evidence of trend and increase in variance with the survival time. The quantile-quantile plot in the middle right plot shows some evidence of departure from the normality, but the non-linearity and heteroscedasticity obscure this. The bottom panel shows the profile log likelihood for the Box--Cox transform, suggesting a value of $-1$ would be within the 95\% confidence interval. After fitting the same additive model with main effect only to the reciprocal survival time, there is no more evidence of residual structure and unequal variance." width="70%" />
<p class="caption">
Figure 2.21: Diagnostic plots for the poison data. The top panel shows the ordinary residuals for the linear model for survival time as a function of poison and treatment, with jittered observations. The middle left plot shows the fitted values against residuals, which display evidence of trend and increase in variance with the survival time. The quantile-quantile plot in the middle right plot shows some evidence of departure from the normality, but the non-linearity and heteroscedasticity obscure this. The bottom panel shows the profile log likelihood for the Box–Cox transform, suggesting a value of <span class="math inline">\(-1\)</span> would be within the 95% confidence interval. After fitting the same additive model with main effect only to the reciprocal survival time, there is no more evidence of residual structure and unequal variance.
</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="likelihood.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60604A_Statistical_modelling.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"split_by": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
