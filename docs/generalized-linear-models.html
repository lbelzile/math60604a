<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Generalized linear models | Statistical Modelling</title>
  <meta name="description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Generalized linear models | Statistical Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="github-repo" content="lbelzile/math60604" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Generalized linear models | Statistical Modelling" />
  
  <meta name="twitter:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="likelihood.html"/>
<link rel="next" href="correlated-longitudinal-data.html"/>
<script src="libs/header-attrs-2.4/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-2/rglClass.src.js"></script>
<script src="libs/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="libs/rglWebGL-binding-0.100.54/rglWebGL.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary remarks</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to statistical inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#tests"><i class="fa fa-check"></i><b>1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#eda"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>2.2</b> Ordinary least squares</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#interpretation-of-the-model-parameters"><i class="fa fa-check"></i><b>2.3</b> Interpretation of the model parameters</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#test-lm"><i class="fa fa-check"></i><b>2.4</b> Tests for parameters of the linear model</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#coefR2"><i class="fa fa-check"></i><b>2.5</b> Coefficient of determination</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#predictions-lm"><i class="fa fa-check"></i><b>2.6</b> Predictions</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i><b>2.7</b> Interactions</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>2.8</b> Collinearity</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#graphical-analysis-of-residuals"><i class="fa fa-check"></i><b>2.9</b> Graphical analysis of residuals</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#transformation-response"><i class="fa fa-check"></i><b>2.10</b> Transformation of the response</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>3</b> Likelihood-based inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="likelihood.html"><a href="likelihood.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.1</b> Maximum likelihood</a></li>
<li class="chapter" data-level="3.2" data-path="likelihood.html"><a href="likelihood.html#liktests"><i class="fa fa-check"></i><b>3.2</b> Likelihood-based tests</a></li>
<li class="chapter" data-level="3.3" data-path="likelihood.html"><a href="likelihood.html#profile-likelihood"><i class="fa fa-check"></i><b>3.3</b> Profile likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="likelihood.html"><a href="likelihood.html#information-criteria"><i class="fa fa-check"></i><b>3.4</b> Information criteria</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basic-principles"><i class="fa fa-check"></i><b>4.1</b> Basic principles</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#theory-of-generalized-linear-models"><i class="fa fa-check"></i><b>4.2</b> Theory of generalized linear models</a></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>4.3</b> Binary responses</a></li>
<li class="chapter" data-level="4.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-data"><i class="fa fa-check"></i><b>4.4</b> Count data</a></li>
<li class="chapter" data-level="4.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#modelling-proportions"><i class="fa fa-check"></i><b>4.5</b> Modelling proportions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="correlated-longitudinal-data.html"><a href="correlated-longitudinal-data.html"><i class="fa fa-check"></i><b>5</b> Correlated and longitudinal data</a></li>
<li class="chapter" data-level="6" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Linear mixed models</a></li>
<li class="chapter" data-level="7" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>7</b> Survival analysis</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="complement.html"><a href="complement.html"><i class="fa fa-check"></i><b>A</b> Complements</a>
<ul>
<li class="chapter" data-level="A.1" data-path="complement.html"><a href="complement.html#population-sample"><i class="fa fa-check"></i><b>A.1</b> Population and samples</a></li>
<li class="chapter" data-level="A.2" data-path="complement.html"><a href="complement.html#random-variable"><i class="fa fa-check"></i><b>A.2</b> Random variable</a></li>
<li class="chapter" data-level="A.3" data-path="complement.html"><a href="complement.html#law-large-numbers"><i class="fa fa-check"></i><b>A.3</b> Laws of large numbers</a></li>
<li class="chapter" data-level="A.4" data-path="complement.html"><a href="complement.html#CLT"><i class="fa fa-check"></i><b>A.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>B</b> Mathematical derivations</a>
<ul>
<li class="chapter" data-level="B.1" data-path="math.html"><a href="math.html#ols"><i class="fa fa-check"></i><b>B.1</b> Derivation of the ordinary least squares estimator</a></li>
<li class="chapter" data-level="B.2" data-path="math.html"><a href="math.html#derivationR2"><i class="fa fa-check"></i><b>B.2</b> Derivation of the coefficient of determination</a></li>
<li class="chapter" data-level="B.3" data-path="math.html"><a href="math.html#restricted-estimation-maximum-likelihood"><i class="fa fa-check"></i><b>B.3</b> Restricted estimation maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="r.html"><a href="r.html"><i class="fa fa-check"></i><b>C</b> <strong>R</strong></a>
<ul>
<li class="chapter" data-level="C.1" data-path="r.html"><a href="r.html#basics-of-r"><i class="fa fa-check"></i><b>C.1</b> Basics of <strong>R</strong></a></li>
<li class="chapter" data-level="C.2" data-path="r.html"><a href="r.html#rlmfunc"><i class="fa fa-check"></i><b>C.2</b> Linear models in <strong>R</strong> using the <code>lm</code> function</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Generalized linear models</h1>
<p>Linear models are only suitable for data that are (approximately)
normally distributed. However, there are many settings where we may wish
to analyze a response variable which is not necessarily continuous,
including when <span class="math inline">\(Y\)</span> is binary, a count variable or is continuous, but
non-negative. We will consider in particular likelihood-based inference
for binary/proportion and counts data.</p>
<p>Generalized linear models (GLM) combine a model for the conditional mean
with a distribution for the response variable and a link function tying
predictors and parameters.</p>
<p>This chapter gives an introduction to generalized linear models and
focuses in particular on logistic regression and Poisson regression, but
only for the case of independent observations. Extensions of generalized
linear models for correlated and longitudinal data, the so-called
<em>generalized linear mixed models</em> (GLMM), are covered in MATH80621.</p>
<div id="basic-principles" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Basic principles</h2>
<p>The starting point is the same as for linear regression: we have a
random sample of independent observations
<span class="math inline">\((\boldsymbol{Y}, \mathbf{X})\)</span>, where <span class="math inline">\(Y\)</span> is the response variable and
<span class="math inline">\(\mathrm{X}_1, \ldots, \mathrm{X}_p\)</span> are <span class="math inline">\(p\)</span> explanatory variables or
covariates which are assumed fixed (non-random). The goal is to model
the mean of the response variable as a function of the explanatory
variables.</p>
<p>Let <span class="math inline">\(\mu_i=\mathsf{E}(Y_i \mid \mathbf{X}_i)\)</span> denote the conditional
expectation of <span class="math inline">\(Y_i\)</span> given covariates and let <span class="math inline">\(\eta_i\)</span> denote the linear
combination of the covariates that will be used to model the response
variable,
<span class="math display">\[\begin{align*}
\eta_i=\beta_0 + \beta_1 \mathrm{X}_{i1} + \cdots + \beta_p \mathrm{X}_{ip}.
\end{align*}\]</span>
The building blocks of the generalized linear model are</p>
<ul>
<li>A random component, consisting of the probability distribution for the outcome <span class="math inline">\(Y\)</span> that is a member of the exponential dispersion family (normal, binomial, Poisson, gamma, ).</li>
<li>A deterministic component, the <strong>linear predictor</strong> <span class="math inline">\(\boldsymbol{\eta}=\mathbf{X} \boldsymbol{\beta}\)</span>, where <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n\times (p+1)\)</span> matrix with columns <span class="math inline">\(\mathbf{1}_n, \mathbf{X}_1, \ldots, \mathbf{X}_p\)</span> and <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^{p+1}\)</span> are coefficients.</li>
<li>A monotone function <span class="math inline">\(g\)</span>, called <strong>link function</strong>, that maps the mean of <span class="math inline">\(Y_i\)</span> to the predictor variables, <span class="math inline">\(g(\mu_i)=\eta_i\)</span>.</li>
</ul>
</div>
<div id="theory-of-generalized-linear-models" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Theory of generalized linear models</h2>
<p>This section borrows from Chapter 4 of</p>
<blockquote>
<p>Agresti (2015). <em>Foundations of Linear and Generalized Linear Models</em>, Wiley.</p>
</blockquote>
<div id="exponential-dispersion-family-of-distributions" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Exponential dispersion family of distributions</h3>
<p>In a generalized linear model, the random component arises from an
exponential dispersion family. Consider a probability density or mass
function for <span class="math inline">\(Y\)</span> with parameters <span class="math inline">\((\theta, \phi)\)</span>, <span class="math display">\[\begin{align}
f(y; \theta, \phi)&amp;= \exp \left\{  \frac{y \theta -b(\theta)}{a(\phi)} + c(y, \phi)\right\}, 
(\#eq:expofam)
\end{align}\]</span>
where the support, i.e., the set of values taken by <span class="math inline">\(Y\)</span>,
doesn’t depend on the parameters. Throughout, we will assume the
<strong>natural parameter</strong> <span class="math inline">\(\theta\)</span> is unknown, but the <strong>dispersion
parameter</strong> <span class="math inline">\(\phi\)</span> may be known (exponential family) or unknown
(exponential dispersion family).</p>
<p>One particularity of exponential dispersion models is that there exists
an explicit relationship between mean and variance of <span class="math inline">\(Y\)</span>, which the
following derivation shows. the first and second derivative of the log
likelihood contribution of <span class="math inline">\(Y_i\)</span>, say <span class="math inline">\(\ell_i\)</span>, with respect to the
natural parameter <span class="math inline">\(\theta\)</span> are
<span class="math display">\[\begin{align*}
\frac{\partial \ell_i(y; \theta, \phi)}{\partial \theta} &amp;= \frac{\partial}{\partial \theta} \left\{\frac{y\theta-b(\theta)}{a(\phi)} + c(y, \phi) \right\} = \frac{y - b&#39;(\theta)}{a(\phi)}\\
\frac{\partial^2 \ell_i(y; \theta, \phi)}{\partial \theta^2} &amp;= - \frac{b&#39;&#39;(\theta)}{a(\phi)},
\end{align*}\]</span>
where <span class="math inline">\(b&#39;(\cdot)\)</span> and <span class="math inline">\(b&#39;&#39;(\cdot)\)</span> are the first two
derivatives of <span class="math inline">\(b(\cdot)\)</span> with respect to <span class="math inline">\(\theta\)</span>. Under regularity
condition, the <a href="https://math.stackexchange.com/q/2027660">Bartlett
identities</a> hold and
<span class="math display">\[\begin{align*}
\mathsf{E}\left\{\frac{\partial \ell(y; \theta, \phi)}{\partial \theta}\right\}=0, \qquad - \mathsf{E}\left\{\frac{\partial^2 \ell(y; \theta, \phi)}{\partial \theta^2}\right\} = \left[\mathsf{E}\left\{\frac{\partial \ell(y; \theta, \phi)}{\partial \theta}\right\}\right]^2.
\end{align*}\]</span>
These two equality give
<span class="math display">\[\begin{align*}
\mathsf{E}(Y_i) &amp;= b&#39;(\theta_i) \\
\mathsf{Va}(Y_i) &amp;= b&#39;&#39;(\theta_i)a(\phi_i)
\end{align*}\]</span>
Often, the term <span class="math inline">\(a_i(\phi)=a_i\phi\)</span>, where <span class="math inline">\(\phi\)</span> is
constant over all observations and <span class="math inline">\(a_i\)</span> is an observation-specific
weight. The mean of <span class="math inline">\(Y_i\)</span>, say <span class="math inline">\(\mu_i\)</span>, and the natural parameter
<span class="math inline">\(\theta_i\)</span> are related through the equation <span class="math inline">\(\mu_i=b&#39;(\theta_i)\)</span>. There
is also an explicit relationship with the variance through
<span class="math inline">\(V(\mu)=b&#39;&#39;(\theta)\)</span>, unless <span class="math inline">\(V(\mu)=1\)</span>.</p>

<div class="example">
<span id="exm:poissonglmexpf" class="example"><strong>Example 4.1  (Poisson distribution as exponential family member)  </strong></span>The mass function of the Poisson distribution is
<span class="math display">\[\begin{align*}
f(y; \lambda) = \frac{\lambda^y \exp(-\lambda)}{y!} =\exp \left\{ y \ln
(\lambda) -\lambda-\ln(y!)\right\}, \qquad y=0,1, \ldots
\end{align*}\]</span>
The natural parameter is <span class="math inline">\(\theta =\ln(\lambda)\)</span>, the dispersion
parameter <span class="math inline">\(\phi=1\)</span>, and <span class="math inline">\(b(\theta)=\exp(\theta)\)</span>. Replacing these expressions in the mean-variance formulas, we get <span class="math inline">\(\mathsf{E}(Y)=\exp(\theta)=\mu\)</span> and <span class="math inline">\(\mathsf{Va}(Y)=\exp(\theta)=\mu\)</span>, meaning <span class="math inline">\(V(\mu)=\mu\)</span>.
</div>

<div class="example">
<span id="exm:binomialglmexpf" class="example"><strong>Example 4.2  (Binomial distribution as member of the exponential family)  </strong></span>We consider the mass function of a binomial distribution, <span class="math inline">\(\mathsf{Bin}(m, \pi)\)</span>, with the number of trials <span class="math inline">\(m\)</span> known. The parametrization presented in Example <a href="complement.html#exm:binomialdist">A.3</a> is not convenient because the mean and the variance both depend on <span class="math inline">\(m\)</span>. We consider thus a different parametrization in which <span class="math inline">\(Y\)</span> represents the proportion of successes, so the mass function takes values in <span class="math inline">\(\{0, 1/m, \ldots, 1\}\)</span> and <span class="math inline">\(mY\)</span> denotes the number of successes. The mass function for <span class="math inline">\(Y\)</span> is then
<span class="math display">\[\begin{align*}
f(y, \pi)&amp;=\exp \left\{ my \ln \left( \frac{\pi}{1-\pi} \right) + m \ln
(1-\pi) + \ln \left[
\binom{m}{my}\right]\right\}\\&amp; =\exp \left\{
\frac{y \ln \left( \frac{\pi}{1-\pi} \right) +  \ln
(1-\pi)}{1/m}+ \ln \left[
\binom{m}{my}\right]\right\}
\end{align*}\]</span>
Set
<span class="math display">\[\begin{align*}
\theta= \ln \left( \frac{\pi}{1-\pi}\right)
\end{align*}\]</span>
with <span class="math inline">\(b(\theta) = \ln\{1+\exp(\theta)\}\)</span> and <span class="math inline">\(\phi=m^{-1}\)</span>. The expectation and variance are easily derived and
<span class="math display">\[\begin{align*}
\mathsf{E}(Y)&amp;=\pi = \mathrm{expit}(\theta)=\frac{\exp(\theta)}{1+\exp(\theta)}=\mu
\\\mathsf{Va}(Y) &amp;= \frac{\pi(1-\pi)}{m} = \frac{\mu(1-\mu)}{m} = \phi V(\mu)
\end{align*}\]</span>
where <span class="math inline">\(V(\mu)=\mu(1-\mu)\)</span>.
</div>

<div class="example">
<span id="exm:normalglmexpf" class="example"><strong>Example 4.3  (Normal distribution as member of the exponential family)  </strong></span>We consider a sample consisting of independent normal observations, <span class="math inline">\(Y_i \sim \mathsf{No}(\mu_i, \sigma^2)\)</span>, with
<span class="math display">\[\begin{align*}
f(y_i, \mu_i, \sigma^2)&amp;= (2\pi\sigma^2)^{-1/2} \exp \left\{ -\frac{(y_i-\mu_i)}{2\sigma^2}\right\}
\\&amp;= \exp \left[ \frac{y\mu - \mu^2/2}{\sigma^2} - \frac{1}{2} \left\{ \frac{y^2}{\sigma^2} + \ln(2\pi\sigma^2)\right\} \right],
\end{align*}\]</span>
meaning <span class="math inline">\(\theta=\mu\)</span>, <span class="math inline">\(\phi=\sigma^2\)</span> and <span class="math inline">\(a(\phi)=\sigma^2\)</span>, <span class="math inline">\(b(\theta)=\theta^2/2\)</span>.
</div>
</div>
<div id="link-functions" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Link functions</h3>
<p>The link between the mean of <span class="math inline">\(Y\)</span> and the <strong>linear predictor</strong> <span class="math inline">\(\eta\)</span> is
<span class="math display">\[\begin{align*}
g\left\{\mathsf{E}(Y \mid \mathrm{X}_1, \ldots, \mathrm{X}_p)\right\}=\eta = \beta_0 + \beta_1 \mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}.
\end{align*}\]</span>
The link function thus connects the mean of the random
variable <span class="math inline">\(Y\)</span> to the explanatory variables,
<span class="math inline">\(g(\mu_i) = \eta_i = \beta_0 + \sum_{j=1}^p\beta_j\mathrm{X}_{ij}\)</span> and
likewise <span class="math inline">\(\mu_i = g^{-1}(\eta_i)\)</span>. If the link function is chosen such
that <span class="math inline">\(\theta=\eta\)</span>, the link function is termed the canonical link
function.</p>
<p>The need for a link function often arises from parameter constraints:
for example, the mean <span class="math inline">\(\mu=\pi\)</span> of a Bernoulli distribution is the
proportion of successes and must lie in the interval <span class="math inline">\((0, 1)\)</span>.
Similarly, the mean <span class="math inline">\(\mu\)</span> of the Poisson distribution must be positive.
For the normal distribution (ordinary linear regression model), the mean
is unconstrained and the canonical link function is the identity,
<span class="math inline">\(\mu_i = \eta_i\)</span>.</p>
<p>An appropriate choice of link function <span class="math inline">\(g\)</span> sets <span class="math inline">\(\mu_i\)</span> equal to a
transformation of the linear combination <span class="math inline">\(\eta_i\)</span> so as to avoid
imposing parameter constraints on <span class="math inline">\(\boldsymbol{\beta}\)</span>. Certain choices
of link functions facilitate interpretation or make the likelihood
function convenient for optimization.<br />
For the Bernoulli and binomial distributions, the most common link
function <span class="math inline">\(g\)</span> is the logit function,
<span class="math display">\[\begin{align*}
\mathrm{logit}(\mu)&amp;=
\ln \left( \frac{\mu}{1-\mu} \right) =\ln(\mu) - \ln(1-\mu)= \eta,
\\\mathrm{expit}(\eta)&amp;= \frac{\exp(\eta)}{1+\exp(\eta)}=\mu.
\end{align*}\]</span>
The inverse link function is the distribution function of
the logistic distribution, hence the name <strong>logistic regression</strong>. This
choice of link function is far from unique: any quantile function of a
continuous random variable supported on <span class="math inline">\(\mathbb{R}\)</span> could be
considered. For the Poisson distribution, the canonical link function
<span class="math inline">\(g\)</span> is the natural logarithm, <span class="math inline">\(\ln\)</span>, with associated inverse link
function <span class="math inline">\(\exp\)</span>.</p>
<p>Canonical link functions are natural choices because of their nice
statistical properties: choosing the canonical link ensures that
<span class="math inline">\(\mathbf{X}^\top\boldsymbol{y}\)</span> is a minimal sufficient statistic. Other
considerations, such as parameter constraints, can be more important in
deciding on the choice of <span class="math inline">\(g\)</span>.</p>

<div class="example">
<p><span id="exm:inverslinklm" class="example"><strong>Example 4.4  (Transforming a nonlinear regression model into a linear model through a link function)  </strong></span>
Consider a linear model for which theory dictates that the mean should be <span class="math inline">\(Y \sim \mathsf{No}\{\alpha_0/(1+\alpha_1x), \sigma^2\}\)</span>.</p>
This model is nonlinear in <span class="math inline">\(\alpha_0\)</span>, <span class="math inline">\(\alpha_1\)</span>. If we opt for the reciprocal link function <span class="math inline">\(g(x)=1/x\)</span>, we get <span class="math inline">\(\beta_0+\beta_1 x\)</span>, where <span class="math inline">\(\beta_0 = 1/\alpha_0\)</span> and <span class="math inline">\(\beta_1 = \alpha_1/\alpha_0\)</span> provided <span class="math inline">\(\alpha_0 \neq 0\)</span>. Since the generalized linear model is estimated using maximum likelihood and the latter are invariant to reparametrization, we get easily the estimated coefficients of interest.
</div>
</div>
<div id="model-adjustment" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Model adjustment</h3>
<p>There is generally no closed-form expression for the maximum likelihood
estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> in generalized linear models
and the score equation is typically nonlinear in <span class="math inline">\(\boldsymbol{\beta}\)</span>
and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> must be obtained through iterative
numerical routines. The key reason for restricting attention to
exponential family is because the likelihood equation depends on the
response <span class="math inline">\(Y_i\)</span> only through <span class="math inline">\(\mu_i\)</span>, <span class="math inline">\(\phi\)</span> and <span class="math inline">\(V(\mu)\)</span>. The mean-variance thus characterizes the distribution.</p>
<p>Starting from Equation <a href="#eq:expofam">(<strong>??</strong>)</a>, we differentiate the log likelihood function <span class="math inline">\(\ell = \sum_{i=1}^n \ell_i\)</span> with respect to
<span class="math inline">\(\boldsymbol{\beta}\)</span>. For simplicity, we consider each likelihood
contribution and coefficient in turn. By the chain rule, <span class="math display">\[\begin{align*}
\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial
\eta_i}{\partial \beta_j} \frac{\partial \mu_i}{\partial \eta_i}
\frac{\partial \theta_i}{\partial \mu_i}\frac{\partial
\ell_i}{\partial \theta_i}
\end{align*}\]</span>
and the earlier derivations show <span class="math inline">\(\partial \ell_i/\partial \theta_i = (y_j-\mu_i)/a_i(\phi)\)</span> and <span class="math inline">\(\partial \mu_i / \partial \theta_i = b&#39;&#39;(\theta_i) = \mathsf{Va}(Y_i)/a_i(\phi)\)</span>.
The derivative of the linear predictor,<span class="math inline">\(\partial \eta_i / \partial \beta_j = \mathrm{X}_{ij}\)</span>. The only missing term, <span class="math inline">\(\partial \mu_i/\partial \eta_i\)</span>, depends on the choice of link function through <span class="math inline">\(\eta_i = g(\mu_i)\)</span>, but it is unity for the canonical link function.</p>
<p>Summing all the likelihood contribution, the score vector
<span class="math inline">\(\boldsymbol{U}\)</span> has element
<span class="math display">\[\begin{align*}
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{(y_i-\mu_i)\mathrm{X}_{ij}}{g&#39;(\mu_i)V(\mu_i)a_i(\phi)}, \qquad j=0, \ldots, p.
\end{align*}\]</span></p>
<p>Let
<span class="math display">\[\begin{align*}
U(\boldsymbol{\beta}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}}, \qquad j(\boldsymbol{\beta}) = - \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}
\end{align*}\]</span></p>
<p>In general, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> solves the score equation <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})=\boldsymbol{0}_{p+1}\)</span>, so we can use a Newton–Raphson algorithm to derive the maximum likelihood
estimator. This amounts to a first order Taylor series approximation of
the score <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})\)</span> around <span class="math inline">\(\boldsymbol{\beta}\)</span>,
<span class="math display">\[\begin{align*}
\boldsymbol{0}_{p+1} = U(\widehat{\boldsymbol{\beta}}) \stackrel{\cdot}{=} U(\beta) - j(\boldsymbol{\beta}) (\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})
\end{align*}\]</span>
If the <span class="math inline">\((p+1)\)</span> matrix <span class="math inline">\(j(\boldsymbol{\beta}^{(t)})\)</span> is
invertible, we can thus device an iterative algorithm: starting from
some initial value <span class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span>, we compute at step <span class="math inline">\(t+1\)</span>
<span class="math display">\[\begin{align*}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + j^{-1}(\boldsymbol{\beta}^{(t)})U(\boldsymbol{\beta}^{(t)}).
\end{align*}\]</span>
and iterate this formula until convergence. Most softwares
implement a variant of this algorithm, in which the negative hessian
<span class="math inline">\(j(\boldsymbol{\beta})\)</span> is replaced by its absolute value
<span class="math inline">\(i(\boldsymbol{\beta})\)</span>: the resulting algorithm is known as Fisher
scoring. For generalized linear models, these recursions can be done by
repeatedly computing a variance of least squares known as iteratively
reweighted least squares.</p>
</div>
<div id="likelihood-inference" class="section level3" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Likelihood inference</h3>
<p>Likelihood inference is straightforward, although some care is needed
because the asymptotic distribution of test statistics sometimes depend
on the model parameters.</p>
<p>Under regularity conditions, the maximum likelihood estimators of
<span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\phi\)</span> are jointly normally distributed under
mild regularity conditions,
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} \stackrel{\cdot}{\sim}\mathsf{No}_{p+1}\{\boldsymbol{\beta}, (\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1}\}
\end{align*}\]</span>
where
<span class="math inline">\(\mathbf{W} = \mathrm{diag}\{(\partial \mu_i/\partial \eta_i)^2/\mathsf{Va}(Y_i)\}\)</span>
is a diagonal matrix with elements
<span class="math inline">\(w_i=\{g&#39;(\mu_i)^2a_i(\phi)V(\mu_i)\}^{-1}\)</span>.</p>
<p>Since <span class="math inline">\(\mathbf{W}\)</span> depends on the unknown <span class="math inline">\(\boldsymbol{\beta}\)</span> through
the expected values <span class="math inline">\(\mu_i\)</span>, we can estimate <span class="math inline">\(\widehat{\mathbf{W}}\)</span> by
replacing the unknown coefficients by their maximum likelihood estimates
at <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>

<div class="example">
<span id="exm:weightpois" class="example"><strong>Example 4.5  (Elements of the weight matrix for Poisson data)  </strong></span>If we consider <span class="math inline">\(Y_i \sim \mathsf{Po}(\mu_i)\)</span> with canonical link function <span class="math inline">\(\mu_i = \exp(\eta_i)\)</span>, then <span class="math inline">\(\partial \eta_i / \partial \mu_i = \mu_i^{-1}\)</span> and <span class="math inline">\(w_i=\mu_i\)</span>.
</div>
<p>Since we use maximum likelihood estimation, the theory presented in
<a href="likelihood">Chapter 3</a> readily applies. For example, if we want to
compare two nested models and test if <span class="math inline">\(q\)</span> of the <span class="math inline">\(\boldsymbol{\beta}\)</span>
are jointly zero (corresponding to no effect for the associated
covariates), we can fit both null and restricted models and perform a
likelihood ratio test. Under regularity conditions, the likelihood ratio
statistic to compare two nested models will follow the usual <span class="math inline">\(\chi^2_q\)</span>
distribution.</p>
</div>
<div id="goodness-of-fit-criteria-and-residuals" class="section level3" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Goodness-of-fit criteria and residuals</h3>
<p>Goodness-of-fit diagnostics often rely on test statistics comparing the
fitted model with parameters <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> with a
<strong>saturated</strong> model in which there are <span class="math inline">\(n\)</span> parameters for the mean, as
many as there are observations — this amounts to maximizing the
log-likelihood contribution of each term <span class="math inline">\(\ell_i\)</span> individually, for
which the best value of the linear predictor is denoted
<span class="math inline">\(\widetilde{\eta}_i\)</span> (oftentimes, this is when <span class="math inline">\(\widetilde{\mu}_i=y_i\)</span>).
We can then build a likelihood ratio statistic to compare the saturated
model with the fitted model, taking
<span class="math display">\[\begin{align*}
\frac{D}{\phi}= \sum_{i=1}^n 2\{\ell(\widetilde{\eta}_i; y_i) - \ell(\widehat{\eta}_i; y_i)\}.
\end{align*}\]</span>
The <strong>scaled deviance</strong> <span class="math inline">\(D\)</span> will be small when the quality
of the adjustment is roughly the same for both models, whereas large
values of <span class="math inline">\(D\)</span> are indicative of poor fit.</p>

<div class="example">
<span id="exm:devglm" class="example"><strong>Example 4.6  (Deviance for common generalized linear models)  </strong></span>Suppose that the model matrix of the model <span class="math inline">\(\mathbf{X}\)</span> includes an intercept and let <span class="math inline">\(\widehat{\mu}_i\)</span> denote the fitted mean. Then, the deviance for the normal generalized linear model with homoscedastic errors is <span class="math inline">\(D = \sum_{i=1}^n (y_i - \widehat{\mu}_i)^2\)</span>, which is the sum of squared residuals. For Poisson data, the saturated model has <span class="math inline">\(\widetilde{\mu}_i=y_i\)</span> and <span class="math inline">\(D= 2\sum_{i=1}^n y_i \ln(y_i/\widehat{\mu}_i)\)</span>.
</div>
<p>Another alternative is found by looking at score test statistic
comparing the saturated model with the postulated model with <span class="math inline">\(p+1\)</span>
parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>. The Pearson <span class="math inline">\(X^2\)</span> statistic is
<span class="math display">\[\begin{align*}
 X^2= \sum_{i=1}^n \frac{(y_i-\widehat{\mu}_i)}{ V(\widehat{\mu}_i)},
\end{align*}\]</span>
where both expectation and variance are estimated using the
model; for Poisson data, <span class="math inline">\(\phi=1\)</span> and the estimated mean and variance
are equal, meaning <span class="math inline">\(\widehat{\mu}_i=V(\widehat{\mu}_i)\)</span>. Pearson’s <span class="math inline">\(X^2\)</span>
statistic measures standardized departures between observations and
fitted values.</p>
<p>In large samples, both Pearson <span class="math inline">\(X^2/\phi\)</span> and the deviance <span class="math inline">\(D/\phi\)</span>
follow approximately a <span class="math inline">\(\chi^2_{n-p-1}\)</span> distribution if the model is
correct and <span class="math inline">\(\phi\)</span> is known (but this result doesn’t hold for binary
data or binomial data unless <span class="math inline">\(m\)</span> is very large and <span class="math inline">\(\boldsymbol{\beta}\)</span>
are small). If <span class="math inline">\(\phi\)</span> is unknown, we would replace it throughout by an
estimate and the same distributional results hold approximatively.</p>
<p>We can use the individual contributions to the deviance and Pearson
<span class="math inline">\(X^2\)</span> statistic to build residuals. By considering
<span class="math inline">\(D = \sum_{i=1}^n d_i^2\)</span>, where
<span class="math display">\[\begin{align*}
d_i &amp;= \mathrm{sign}(\widetilde{\eta}_i - \widehat{\eta}_i) \{2\ell(\widetilde{\eta}_i; y_i) - 2\ell(\widehat{\eta}_i; y_i)\}^{1/2}
\end{align*}\]</span>
and the calculations simplify upon replacing the formula of
the log likelihood for the generic exponential family member,
<span class="math display">\[\begin{align*}
d_i^2=2 \left\{y_i (\widetilde{\theta}_i - \widehat{\theta}_i) - b(\widetilde{\theta}_i) + b(\widehat{\theta}_i)\right\}
\end{align*}\]</span>
The terms <span class="math inline">\(d_i\)</span> are called deviance residuals, whereas
Pearson residuals are based on the score contributions
<span class="math inline">\(u_i(\widehat{\beta}) w_i(\widehat{\beta})^{-1/2}\)</span>, where the score statistic <span class="math inline">\(u(\boldsymbol{\beta})\)</span> and the weights <span class="math inline">\(w_i\)</span> are
<span class="math display">\[\begin{align*}
u_i &amp;= \frac{\partial \theta_i}{\partial \eta_i} \frac{\partial \ell_i(\theta_i)}{\partial \theta_i} = \frac{y_i - \mu_i}{g&#39;(\mu_i)a_i(\phi)V(\mu_i)}\\
w_i &amp;= \left(\frac{\partial \theta_i}{\partial \eta_i}\right)^2 \frac{\partial^2 \ell_i(\theta_i)}{\partial \theta_i^2} = \frac{1}{g&#39;(\mu_i)^2 a_i(\phi)V(\mu_i)}
\end{align*}\]</span></p>
<p>In practice, these residuals are heteroscedastic and it is better to
standardize them by considering instead
<span class="math display">\[\begin{align*}
r_{D_i} = \frac{d_i}{(1-h_{ii})^2}, \qquad r_{P_i} = \frac{u_i(\widehat{\beta})}{\{w_i(\widehat{\beta})(1-h_{ii})\}^{1/2}},
\end{align*}\]</span>
both are scaled by <span class="math inline">\((1-h_{ii})^{1/2}\)</span>, a formula remniscent
of the linear model framework. In the above formulas, the leverage
<span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of the matrix <span class="math display">\[\begin{align*}
\mathbf{H}_{\mathbf{X}} = \mathbf{W}^{1/2}\mathbf{X}(\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{W}^{1/2};
\end{align*}\]</span>
since the terms of
<span class="math inline">\(\mathbf{W}=\mathrm{diag}\{w_1, \ldots, w_n\}\)</span> depend on the unknown
coefficient, the latter is estimated by replacing <span class="math inline">\(\boldsymbol{\beta}\)</span>
by <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>
<p>The standardized deviance residuals <span class="math inline">\(\{r_{D_i}\}\)</span> and standardized
Pearson residuals <span class="math inline">\(\{r_{P_i}\}\)</span> should have an approximate standard
normal distribution in large samples, but their distribution can be
skewed. Like in the linear regression, we will work with the
<strong>jackknifed deviance residuals</strong> for residual plots <span class="math display">\[\begin{align*}
r_{J_i} &amp;= \mathrm{sign}(y_i - \widehat{\mu}_i) \left\{ (1-{h_ii})r^2_{D_i} + h_{ii}r^2_{P_i}\right\}^{1/2}
\end{align*}\]</span></p>
<p>For ordinary linear regression, both <span class="math inline">\(r_{D_i}\)</span> and <span class="math inline">\(r_{P_i}\)</span> reduce to
the standardized residuals <span class="math inline">\(t_i=e_i\{S^2(1-h_{ii})\}^{-1/2}\)</span>.</p>
<p>There are clear parallels between generalized linear models and linear
models: we have so far derived an analog of residuals and leverage.
Collinearity is also an issue for generalized linear model; for the
latter, we define the Cook statistic as the change in the deviance,
<span class="math display">\[\begin{align*}
C = \frac{1}{p} 2\{\ell(\widehat{\boldsymbol{\beta}}) - \ell(\widehat{\boldsymbol{\beta}}_{-j})\},
\end{align*}\]</span>
where <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{-j}\)</span> is the estimate
obtained by dropping the <span class="math inline">\(j\)</span>th observation from the sample. This
requires fitting <span class="math inline">\(n\)</span> different models, which is computationally
prohibitive. In the linear regression, we can calculate the Cook
distance from the formula <span class="math inline">\(C_j = (p+1)^{-1}t_i^2h_{ii}/(1-h_{ii})\)</span>,
where <span class="math inline">\(t_i\)</span> are the standardized residuals defined in the previous
section. For generalized linear models, no such expression exists,
although a good approximation is
<span class="math inline">\(C_j \approx (p+1)^{-1}r_{P_i}^2h_{ii}/(1-h_{ii})\)</span>.</p>
<p>Diagnostic plots for generalized linear models are harder to interpret
because of the lack of orthogonality. It is customary to plot the
jackknife deviance residuals against the linear predictor
<span class="math inline">\(\widehat{\eta}_i\)</span>, produce normal quantile-quantile plots of
standardized deviance residuals and the approximate Cook statistics
against the <span class="math inline">\(h_{ii}/(1-h_{ii})\)</span>. We will show examples of such plots for
particular models.</p>
</div>
</div>
<div id="binary-responses" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Binary responses</h2>
<p>Binary response data are commonplace, notably due to their role in
classification. We begin this section by discussing interpretation of
the model in terms of censored observations, than tackle the
interpretation of the parameters for the canonical link function on
different scales (probability, odds and log-odds). We pursue with issues
due to the nature of the data, including non-regular asymptotics and
separation of variable.</p>
<p>The logistic model specifies
<span class="math display">\[\begin{align*}
\mathsf{P}(Y_i=1 \mid \mathbf{X}_i) = \pi_i = \frac{\exp(\beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip})}{1 + \exp(\beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip})},
\end{align*}\]</span>
whereas, on the linear predictor scale <span class="math inline">\(\eta_i\)</span>,
<span class="math display">\[\begin{align*}
\eta_i =\log \left(\frac{\pi_i}{1-\pi_i}\right) =  \beta_0 + \beta_1 \mathrm{X}_{i1}+ \cdots + \beta_p\mathrm{X}_{ip}.
\end{align*}\]</span></p>
<p>One way to view binary variables is as censored observations: suppose
<span class="math inline">\(Z_i = \eta_i + \varepsilon_i\)</span> where <span class="math inline">\(\varepsilon_i\)</span> is an error term
with distribution function <span class="math inline">\(F\)</span> and <span class="math inline">\(\eta_i\)</span> is a fixed linear predictor.
Suppose that the variables <span class="math inline">\(Z_i\)</span> are censored:we only observe the binary
indicators <span class="math inline">\(Y_i = 1\)</span> if <span class="math inline">\(Z_i &gt; 0\)</span> and <span class="math inline">\(Y_i=0\)</span> otherwise, not the <span class="math inline">\(Z_i\)</span>
terms themselves. It follows that <span class="math display">\[
\mathsf{P}(Y_i = 1 \mid \eta_i) = 1-F(-\eta_i)
\]</span> If <span class="math inline">\(F\)</span> is symmetric around zero, then <span class="math inline">\(F(\eta_i) = 1-F(-\eta_i)\)</span>. The
choice of the logistic distribution for <span class="math inline">\(F\)</span> gives
<span class="math inline">\(\pi_i = \mathrm{expit}(\eta_i)\)</span>. Another popular alternative is when
<span class="math inline">\(F\)</span> is the distribution function of a standard normal distribution,
<span class="math inline">\(F(x) = \Phi(x)\)</span>, which is the probit regression model. Both link
functions only differ in the tail, but the parameters of the the
logistic model are more readily interpreted since the quantile function
<span class="math inline">\(F^{-1}\)</span> of the logistic distribution, <span class="math inline">\(\mathrm{logit}(x)\)</span>, is available
in closed-form.</p>
<p>Dichotomization leads to a loss of information relative to the
underlying continuous data, which translates into more variability in
the parameter estimates.</p>
<div class="figure" style="text-align: center"><span id="fig:probitvslogit"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/probitvslogit-1.png" alt="Comparison of logistic (black) and probit (dashed blue) inverse link functions (left) for binary data: both are monotone increasing functions of the linear predictor. The logistic distribution has thicker tails and thus smaller peak at zero. The right panel shows the derivative of both inverse link functions." width="70%" />
<p class="caption">
Figure 4.1: Comparison of logistic (black) and probit (dashed blue) inverse link functions (left) for binary data: both are monotone increasing functions of the linear predictor. The logistic distribution has thicker tails and thus smaller peak at zero. The right panel shows the derivative of both inverse link functions.
</p>
</div>
<p>The logistic regression model is nonlinear in
<span class="math inline">\(\boldsymbol{\eta}=\mathbf{X}\boldsymbol{\beta}\)</span> and the left panel of
Figure <a href="generalized-linear-models.html#fig:probitvslogit">4.1</a> shows how changes in the linear
predictor impact the probability of success. As <span class="math inline">\(\eta_i\)</span> increases (for
example, if <span class="math inline">\(\mathrm{X}_j\)</span> increases by one unit and <span class="math inline">\(\beta_j\)</span> is
positive), so does the estimated probability of success because the
function <span class="math inline">\(\mathrm{expit}(x)\)</span> is monotone increasing. This
characterization is however not helpful. If we exponentiate the linear
predictor <span class="math inline">\(\eta\)</span>, we get
<span class="math display">\[\begin{align*}
\mathrm{odds}(Y\mid \mathbf{X}) =
\frac{\pi(\mathbf{X})}{1-\pi(\mathbf{X})}=\exp(\beta_0+ \beta_1
\mathrm{X}\_1 + \cdots + \beta_p\mathrm{X}\_p),
\end{align*}\]</span>
where <span class="math inline">\(\pi(\mathbf{X})/\{1-\pi(\mathbf{X})\}\)</span> are the odds of <span class="math inline">\(\mathsf{P}(Y=1 \mid \mathbf{X})\)</span> (success) relative to <span class="math inline">\(\mathsf{P}(Y=0 \mid\mathbf{X})\)</span> (failure). If <span class="math inline">\(\pi=0.8\)</span>, the odds are <span class="math inline">\(\pi/(1-\pi)=0.8/0.2=4\)</span>: on average, we expect four successes for one failure.</p>
<p>On the odds scale, the logistic regression model is multiplicative: for an increase of <span class="math inline">\(\mathrm{X}_j\)</span> of one unit, <em>ceteris paribus</em>, the odds are multiplied by <span class="math inline">\(\exp(\beta_j)\)</span>.</p>

<div class="example">
<span id="exm:gpaexample" class="example"><strong>Example 4.7  (GPA score and graduate admission)  </strong></span>We consider a simple logistic model for graduate admission (yes/no) in universities across the USA as a function of their grade point average (GPA).
</div>
<div class="figure" style="text-align: center"><span id="fig:figgpa"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/figgpa-1.png" alt="Graduate admission probability as a function of grade point averages (GPA) on the log odds (left), odds (middle) and probability of success scale (right). The line indicates fitted value with pointwise 95\% profile-based confidence intervals." width="70%" />
<p class="caption">
Figure 4.2: Graduate admission probability as a function of grade point averages (GPA) on the log odds (left), odds (middle) and probability of success scale (right). The line indicates fitted value with pointwise 95% profile-based confidence intervals.
</p>
</div>
<table>
<caption>
<span id="tab:gpatable">Table 4.1: </span>Logistic regression: table of coefficients for the graduate admission data with gpa as only covariate.
</caption>
<thead>
<tr>
<th style="text-align:left;">
term
</th>
<th style="text-align:right;">
estimate
</th>
<th style="text-align:right;">
std. error
</th>
<th style="text-align:right;">
Wald stat.
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
-4.36
</td>
<td style="text-align:right;">
1.04
</td>
<td style="text-align:right;">
-4.21
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
gpa
</td>
<td style="text-align:right;">
1.05
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:right;">
3.52
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
</tbody>
</table>
<p>The coefficients of the simple logistic model are reported in Table <a href="generalized-linear-models.html#tab:gpatable">4.1</a>, showing that the log odds of admission increase by 0.11 for each increase in GPA of <span class="math inline">\(0.1\)</span>. This translates into <span class="math inline">\(\exp(0.1\widehat{\beta}_{\mathrm{gpa}})=1.11\)</span> on the odds ratio scale, meaning a 11% increase in odds of admission for every <span class="math inline">\(0.1\)</span> increase in GPA. Of course, this model is simplistic and omits other important factors… Figure <a href="generalized-linear-models.html#fig:figgpa">4.2</a> shows the variation on the log odds, odds and probability scale as a function of grade point average. While the probability scale is the most intuitive, the effect of an increase of <span class="math inline">\(0.1\)</span> unit of GPA isn’t the same, depending on what is your starting value: according to the model, the absolute increase in the probability of admission for individuals with GPA scores <span class="math inline">\(2.3\)</span> and <span class="math inline">\(2.4\)</span> is 1.2%, whereas that corresponding to an increase in GPA from 3.9 to 4 is 2.6%; this illustrates how the impact of an increase of <span class="math inline">\(0.1\)</span> points is not the same across the board.</p>

<div class="example">
<span id="exm:birthweightex" class="example"><strong>Example 4.8  (Risk factors related to underweight babies at birth)  </strong></span>We consider a medical example related to risk factors explaining the birth weight: the birth weight is dichotomized and classified as low if the infant weight is inferior to 2.5 kg (1) and 0 otherwise. The data were collected in a medical center in Massachusetts in 1986.
</div>
<p>The data contains the following explanatory variables</p>
<ul>
<li><code>age</code>: mother’s age in years.</li>
<li><code>lwt</code>: mother’s weight in pounds at last menstrual period.</li>
<li><code>race</code>: mother’s race (<code>white</code>, <code>black</code> or <code>other</code>).</li>
<li><code>smoke</code>: smoking status during pregnancy (<code>yes</code>/<code>no</code>).</li>
<li><code>ptl</code>: previous premature labours (<code>none</code>/<code>at least one</code>).</li>
<li><code>ht</code>: history of hypertension.</li>
<li><code>ui</code>: presence of uterine irritability.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:birthweighteda"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/birthweighteda-1.png" alt="Exploratory data analysis for the birthweight data" width="70%" />
<p class="caption">
Figure 4.3: Exploratory data analysis for the birthweight data
</p>
</div>
<p>The exploratory data analysis suggests that smoking has a large impact, as the relative proportion of women who smoke having underweight babies is significant. Likewise, hypertension seems to bean important predictor but there are only a handful of women in the sample with this condition. Women who previously gave birth to premature babies are more likely to give birth to underweight (and potentially premature) babies, whereas women giving birth to babies under 2.5kg also typically have lower weight. We can use a logistic regression to assess the effect of these explanatories.</p>
<table>
<caption><span id="tab:logistibirthwgt">Table 4.2: </span>Logistic regression: table of coefficients for the birth weight data.</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std. error</th>
<th align="right">Wald stat.</th>
<th align="right">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-0.13</td>
<td align="right">0.97</td>
<td align="right">-0.13</td>
<td align="right">0.9</td>
</tr>
<tr class="even">
<td align="left">mother’s weight (lbs)</td>
<td align="right">-0.02</td>
<td align="right">0.01</td>
<td align="right">-2.29</td>
<td align="right">0.022</td>
</tr>
<tr class="odd">
<td align="left">race [black]</td>
<td align="right">1.3</td>
<td align="right">0.53</td>
<td align="right">2.46</td>
<td align="right">0.014</td>
</tr>
<tr class="even">
<td align="left">race [other]</td>
<td align="right">0.85</td>
<td align="right">0.44</td>
<td align="right">1.94</td>
<td align="right">0.053</td>
</tr>
<tr class="odd">
<td align="left">smoker [yes]</td>
<td align="right">0.87</td>
<td align="right">0.4</td>
<td align="right">2.14</td>
<td align="right">0.032</td>
</tr>
<tr class="even">
<td align="left">previous premature labours</td>
<td align="right">1.13</td>
<td align="right">0.45</td>
<td align="right">2.51</td>
<td align="right">0.012</td>
</tr>
<tr class="odd">
<td align="left">hypertension [yes]</td>
<td align="right">1.87</td>
<td align="right">0.71</td>
<td align="right">2.64</td>
<td align="right">0.008</td>
</tr>
<tr class="even">
<td align="left">uterine irritability</td>
<td align="right">0.75</td>
<td align="right">0.46</td>
<td align="right">1.64</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<p>Table <a href="generalized-linear-models.html#tab:logistibirthwgt">4.2</a> gives the estimated coefficients <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> on the logs odds scale along with standard errors, Wald statistics and the <span class="math inline">\(p\)</span>-values from the normal approximation.</p>
<p>We can interpret the effect of these variables: given the other variables and <em>ceteris paribus</em>:</p>
<ul>
<li>the odds of giving birth to an underweight babies decrease by 1.6% for each additional pound pre-pregnancy; this paradoxical interpretation, at odds with Figure <a href="generalized-linear-models.html#fig:birthweighteda">4.3</a> is (notably) due to the correlation between smoking and mother’s weight.</li>
<li>the odds of black women giving birth to an underweight baby are 2.67 times those of white women.</li>
<li>the odds of women who smoke of having a baby weighting less than 2.5kg at birth are 137.9% higher than those of women who do not smoke.</li>
</ul>
<p>The other parameters are interpreted accordingly. We can also check the parameter significance, by comparing the model with all the covariates and withholding one of the explanatories at the time. The results of the likelihood ratio test are presented in Table <a href="generalized-linear-models.html#tab:type3birthwgt">4.3</a>.
At level 5% and given the other factors, having previous premature labours history and uterine irritability are not statistically significant. There is no significant correlation between the explanatories, since all the variance inflation factors are inferior to <span class="math inline">\(1.5\)</span>.</p>
<table>
<caption><span id="tab:type3birthwgt">Table 4.3: </span>Analysis of deviance table (Type 3 decomposition) for the birthweight logistic regression model: the table gives the <span class="math inline">\(p\)</span>-value for likelihood ratio tests comparing the full model including all covariates with models in which a single explanatory is removed.</caption>
<thead>
<tr class="header">
<th align="left">variable</th>
<th align="right">lik. ratio</th>
<th align="right">df</th>
<th align="right">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">mother’s weight</td>
<td align="right">5.96</td>
<td align="right">1</td>
<td align="right">0.015</td>
</tr>
<tr class="even">
<td align="left">race</td>
<td align="right">7.61</td>
<td align="right">2</td>
<td align="right">0.022</td>
</tr>
<tr class="odd">
<td align="left">smoker</td>
<td align="right">4.72</td>
<td align="right">1</td>
<td align="right">0.03</td>
</tr>
<tr class="even">
<td align="left">previous premature labours</td>
<td align="right">6.37</td>
<td align="right">1</td>
<td align="right">0.012</td>
</tr>
<tr class="odd">
<td align="left">hypertension</td>
<td align="right">7.31</td>
<td align="right">1</td>
<td align="right">0.007</td>
</tr>
<tr class="even">
<td align="left">uterine irritability</td>
<td align="right">2.63</td>
<td align="right">1</td>
<td align="right">0.1</td>
</tr>
</tbody>
</table>
<div id="issues-with-binary-data" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Issues with binary data</h3>
<p>One oddity of regression models for binary data is that most of the
information in the sample is provided by response whose variance is
largest (points that have <span class="math inline">\(\pi\)</span> far from 0 or 1). This can be seen by
looking at how the probability <span class="math inline">\(\pi\)</span> changes as a function of changes in
the <span class="math inline">\(j\)</span>th explanatory <span class="math inline">\(\mathrm{X}_j\)</span>: if
<span class="math inline">\(\pi = \mathrm{expit}(\beta_0 + \beta_1\mathrm{X}_1 + \cdots + \beta_p\mathrm{X}_p)\)</span>,
then <span class="math inline">\(\partial \pi/\partial \mathrm{X}_j = \beta_j \pi(1-\pi)\)</span>, so the
effect of <span class="math inline">\(\beta_j\)</span> is large when <span class="math inline">\(\pi\)</span> is near <span class="math inline">\(0.5\)</span> and near zero if
the probabilities are close to the endpoints <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Thus, if one
wants to estimate a coefficient <span class="math inline">\(\beta_j\)</span> of large magnitude, the ratio
<span class="math inline">\(|\beta_j|/\mathsf{sd}(\beta_j) \to 0\)</span> as <span class="math inline">\(\beta_j \to \infty\)</span>: the
power for testing that <span class="math inline">\(\beta_j=0\)</span> using a Wald statistic tends to zero.
Wald statistics are not recommended for Bernoulli data: the following
simple example illustrates why.</p>

<div class="example">
<span id="exm:waldgarbage" class="example"><strong>Example 4.9  (Incoherent results for Wald tests)  </strong></span>Suppose we consider a simple binary model and <span class="math inline">\(n=100\)</span> observations and the null hypothesis <span class="math inline">\(\mathscr{H}_0:\pi = 0.5\)</span> and equivalently for the logistic model <span class="math inline">\(Y_i \sim \mathsf{Bin}\{m, \mathrm{expit}(\beta_0)\}\)</span>, where this same hypothesis amounts to <span class="math inline">\(\mathscr{H}_0:\beta_0=0\)</span>. Figure <a href="generalized-linear-models.html#fig:WaldgarbageR">4.4</a> clearly shows that lack of invariance of the Wald statistic to reparametrization. When <span class="math inline">\(\widehat{\beta}_0\)</span> increases and <span class="math inline">\(m\)</span> is large, the Wald statistic for <span class="math inline">\(\beta_0\)</span> decreases as we approach <span class="math inline">\(m\)</span> successes, because the standard error of <span class="math inline">\(\widehat{\beta}_0\)</span> increases faster than the difference <span class="math inline">\(\widehat{\beta}-0\)</span>. Both statistics are asymptotically normal if the null hypothesis holds true, so the two-sided test would reject if the value of the Wald statistic is greater than 1.96. Depending on the parametrization, however, they give different results: the statistic increases monotonically for <span class="math inline">\(\pi\)</span>, but would converge to zero for <span class="math inline">\(\beta_0\)</span> when <span class="math inline">\(m\)</span> is large (allowing thereby larger values of <span class="math inline">\(\widehat{\beta}_0\)</span>). No such problem arises when considering score and likelihood ratio tests.
</div>
<div class="figure" style="text-align: center"><span id="fig:WaldgarbageR"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/WaldgarbageR-1.png" alt="Wald test statistic for $Y_i \sim \mathsf{Bin}(m=500, \pi)$ (left)  for $\pi=0.5$ and the same hypothesis test on the logistic scale with $Y_i \sim \mathsf{Bin}(m=500, \mathrm{expit}(\beta_0))$ and $\mathscr{H}_0:\beta_0=0$ (right) as a function of the number of success out of 500 trials." width="70%" />
<p class="caption">
Figure 4.4: Wald test statistic for <span class="math inline">\(Y_i \sim \mathsf{Bin}(m=500, \pi)\)</span> (left) for <span class="math inline">\(\pi=0.5\)</span> and the same hypothesis test on the logistic scale with <span class="math inline">\(Y_i \sim \mathsf{Bin}(m=500, \mathrm{expit}(\beta_0))\)</span> and <span class="math inline">\(\mathscr{H}_0:\beta_0=0\)</span> (right) as a function of the number of success out of 500 trials.
</p>
</div>
<p>Thus, information about the coefficients only accumulates if the true
regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> are small. The deviance and
Pearson provide little to no information about the quality of the model
adjustment and their null distribution depends on
<span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>: with large coefficients, the asymptotic
<span class="math inline">\(\chi^2\)</span> distribution is not a good approximation and relying on these
to determine goodness-of-fit is not recommended.</p>
</div>
<div id="quasi-separation-of-variables" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Quasi-separation of variables</h3>
<p>One common problem with binary data is the problem of complete
separation of variables: in this scenario, a linear combination of the
explanatories allows to perfectly categorize <span class="math inline">\(Y_i\)</span> into <span class="math inline">\(0/1\)</span> depending
on the value of the covariates and the predicted probabilities will be
exactly zero or one. While this is not a problem for prediction, the
coefficients that yield such a sharp transition are near infinite, as
displayed in Figure <a href="generalized-linear-models.html#fig:sepvarplot">4.5</a>; software may or not pick up
such error, but in any case Wald tests for individual coefficients are
rubbish. This is problematic for inference because we cannot
meaningfully interpret the resulting parameters.</p>
<table class="kable_wrapper">
<caption>
<span id="tab:quasisepvar">Table 4.4: </span>Logistic regression: simulated dataset exhibiting separation of variables with standard logistic model (left) and Firth’s regression (right). The large coefficients and standard errors, combined with near zero residuals, are all indicative of separation of variable.
</caption>
<tbody>
<tr>
<td>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std. error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-829</td>
<td align="right">311291</td>
</tr>
<tr class="even">
<td align="left">x</td>
<td align="right">92.5</td>
<td align="right">34781</td>
</tr>
</tbody>
</table>
</td>
<td>
<table>
<thead>
<tr class="header">
<th align="left">estimate</th>
<th align="right">std. error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">-5.6</td>
<td align="right">2.8</td>
</tr>
<tr class="even">
<td align="left">0.6</td>
<td align="right">0.3</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>Inspection of the coefficients and their standard errors is a good way to detect such problems: parameters that are large (with standardized explanatories with mean zero variance one inputs) with <span class="math inline">\(|\widehat{\beta}_j| &gt; 36\)</span> are particularly suspicious; the number of iterations for the optimization routine is also often abnormally large. While the Wald statistic for <span class="math inline">\(\beta_1=0\)</span> is nearly zero and the <span class="math inline">\(p\)</span>-value 1, the likelihood ratio test yields a statistic of 20.016 with a negligible associated <span class="math inline">\(p\)</span>-value.</p>
<p>One way to restore finiteness of the <span class="math inline">\(\boldsymbol{\beta}\)</span> is to impose a penalty term that prevents near infinite parameter values. Firth’s penalty is the most popular solution (option <code>firth</code> in the <strong>SAS</strong> procedure <code>logistic</code> and <code>logistf::logistf</code> function in <strong>R</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:sepvarplot"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/sepvarplot-1.png" alt="Illustration of complete separation of variable on simulated observations. Parameters are near infinite to yield a sharp transition at $x=9$. Quasi-complete separation of variable occurs if both outcomes are observed at the cutoff point." width="70%" />
<p class="caption">
Figure 4.5: Illustration of complete separation of variable on simulated observations. Parameters are near infinite to yield a sharp transition at <span class="math inline">\(x=9\)</span>. Quasi-complete separation of variable occurs if both outcomes are observed at the cutoff point.
</p>
</div>
</div>
<div id="diagnostic-plots-for-binary-data-star" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Diagnostic plots for binary data (<span class="math inline">\(\star\)</span>)</h3>
<p>There are also other issues arising from the discreteness of the
observations. Since observations are 0/1, residuals are often separated.
Figure <a href="generalized-linear-models.html#fig:diagplotsbirthwt1">4.6</a> shows residual plots for Example
<a href="generalized-linear-models.html#exm:birthweightex">4.8</a>: both plots are of limited use to assess
goodness-of-fit and model assumptions.</p>
<div class="figure" style="text-align: center"><span id="fig:diagplotsbirthwt1"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagplotsbirthwt1-1.png" alt="Diagnostic plots for the binary regression for the birth weight data: jackknife deviance residuals against linear predictor (left) and quantile-quantile plot of ordered deviance residuals (right)." width="70%" />
<p class="caption">
Figure 4.6: Diagnostic plots for the binary regression for the birth weight data: jackknife deviance residuals against linear predictor (left) and quantile-quantile plot of ordered deviance residuals (right).
</p>
</div>
<p>Another strategy is use of quantile residuals
<span class="citation">(Brillinger and Preisler <a href="references.html#ref-Brillinger/Preisler:1983" role="doc-biblioref">1983</a>)</span>. If <span class="math inline">\(Y \sim F\)</span>, the quantile transform gives <span class="math inline">\(U=F(Y) \sim \mathsf{U}(0,1)\)</span>, meaning uniformly distributed on <span class="math inline">\((0,1)\)</span>. Replacing the unknown distribution <span class="math inline">\(F\)</span> by <span class="math inline">\(\widehat{F}\)</span> should yield approximately uniform observations. With <span class="math inline">\(\widehat{\pi}_i\)</span> denoting the probability of success, we take
<span class="math display">\[\begin{align*}
U_i = U_{i1} Y_i + U_{i2}(1-Y_i), \qquad U_{i1} \sim \mathsf{U}(0, \widehat{\pi}_i), \quad U_{i2} \sim \mathsf{U}(\widehat{\pi}_i, 1)
\end{align*}\]</span>
and the resulting <strong>uniform residuals</strong> will be, as their name hints, approximately uniform on <span class="math inline">\((0,1)\)</span>. The drawback of this approach lies in the randomness of the residuals. Figure <a href="generalized-linear-models.html#fig:diagplotbirthwgt">4.7</a> shows the diagnostic plots based on the uniform residuals (top two rows): there is no overall indication of poor fit, except for seemingly too few low/high residuals. The two observations with high leverage correspond to non-smoker mothers with no premature labour record: one with hypertension and a weight of 95lbs (unusual combination), the other weighting 200 lbs, with no hypertension and presence of uterine
irritability.</p>
<div class="figure" style="text-align: center"><span id="fig:diagplotbirthwgt"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagplotbirthwgt-1.png" alt="Diagnostic plots based on uniform residuals for the birth weight data. From left to right, top to bottom: residuals against explanatory and linear predictors, histogram of uniform residuals and Tukey's quantile-quantile plot, Cook statistic against weighted leverage and case number. " width="70%" />
<p class="caption">
Figure 4.7: Diagnostic plots based on uniform residuals for the birth weight data. From left to right, top to bottom: residuals against explanatory and linear predictors, histogram of uniform residuals and Tukey’s quantile-quantile plot, Cook statistic against weighted leverage and case number.
</p>
</div>
</div>
</div>
<div id="count-data" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Count data</h2>
<p>If a response is integer value, the linear model is seldom appropriate because the variable is skewed, which even the log-linear model may fail to capture. It may thus be appropriate to look at generalized linear models for discrete data.</p>
<p>The most widespread model for count data is the Poisson regression. The Poisson distribution arises as the limit of binomial data with a small probability of success as the number of attempts becomes large: examples include rare medical conditions, number of accidents, or goals in a soccer (football) match. The mean and variance of the Poisson distribution are both <span class="math inline">\(\lambda\)</span>, so the model imposes stringent constraints on the observations. Extensions that deal with zero-inflation, underdispersed and overdispersed data are widespread. Most count data are found in contingency tables, in which the cells give the counts, while the dimensions of the table represent categorical variables. Such data format are convenient for storage and display, but must be transformed to long format for model fitting.</p>
<p>The canonical link of the Poisson model is the natural logarithm <span class="math inline">\(\ln(x)\)</span> and the interpretation will be similar to that of the <a href="transformation-response">log-linear model</a>. Specifically, suppose the
mean model is
<span class="math display">\[\begin{align*}
\mu = \exp(\beta_0 + \beta_1 \mathrm{X}_{1} + \cdots + \beta_p \mathrm{X}_{p}),
\end{align*}\]</span>
so the mean is multiplied by <span class="math inline">\(\exp(\beta_j)\)</span> for an increase of one unit of <span class="math inline">\(\mathrm{X}_{j}\)</span>, <em>ceteris paribus</em>. If <span class="math inline">\(\beta_j &lt; 0\)</span>, <span class="math inline">\(\exp(\beta_j) &lt; 1\)</span> and so we have a decrease of <span class="math inline">\(100\cdot(1-\exp(\beta_j))\)</span>% of the mean response. Likewise, if <span class="math inline">\(\beta_j&gt;0\)</span>, the mean number increases by <span class="math inline">\(100\cdot(\exp(\beta_j)-1)\)</span>%. The interpretation of interactions between categorical and continuous/categorical variables is similar to that of multiplicative (log-linear) models.</p>

<div class="example">
<span id="exm:roadsweden" class="example"><strong>Example 4.10  (Road accidents and speed limits on the motorway in Sweden)  </strong></span>Sweden is a worlwide leader in road safety and has a long history of countermeasures to increase road traffic safety, including the Vision Zero program. Back in the 1960s, a study was conducted by the authorities to investigate the potential of speed limits on motorways to reduce the number of accidents. The <code>sweden</code> data contains the number of accidents on 92 matching days in both 1961 and 1962 <span class="citation">(Svensson <a href="references.html#ref-Svensson:1981" role="doc-biblioref">1981</a>)</span>; speed limits were in place on selected days in either year.
</div>
<p>We consider an example from Section 4.5 of <span class="citation">Brazzale, Davison, and Reid (<a href="references.html#ref-Brazzale/Davison/Reid:2007" role="doc-biblioref">2007</a>)</span>.
To study the impact of the restrictions we can fit a Poisson model. Let
<span class="math inline">\(Y_{i1}\)</span> (respectively <span class="math inline">\(Y_{i2}\)</span>) denote the number of accidents in 1961
(1962) on day <span class="math inline">\(i\)</span> and let <span class="math inline">\(\texttt{limit}_{ij}\)</span> denote a binary
indicator equal to one if speed limits were enforced on day <span class="math inline">\(i\)</span> of year
<span class="math inline">\(j\)</span>. We set
<span class="math display">\[\begin{align*}
Y_{i1} \sim \mathsf{Po}(\delta_i + \alpha \texttt{limit}_{i1}), \quad Y_{i2} \sim\mathsf{Po}(\delta_i + \gamma +  \alpha \texttt{limit}_{i2}), \qquad i=1, \ldots, 92. 
\end{align*}\]</span>
The nuisance parameters <span class="math inline">\(\delta_1, \ldots, \delta_{92}\)</span> control for changes in background number of accidents and are of no practical interest, while <span class="math inline">\(\gamma\)</span> denotes the change from 1961 to 1962. We are interested here in assessing changes in the number of accidents due to the policy, <span class="math inline">\(\alpha\)</span>; of secondary interest is to determine whether there has been a decrease in the number of accident relative to 1961.</p>
<table>
<caption>
<span id="tab:swedenglm">Table 4.5: </span>Analysis of deviance table (Type 3 decomposition) for the Poisson regression model fitted to the Sweden traffic restrictions data: the table gives the <span class="math inline">\(p\)</span>-value for likelihood ratio tests comparing the full model including all covariates with models in which a single explanatory is removed.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:right;">
lik. ratio
</th>
<th style="text-align:right;">
df
</th>
<th style="text-align:right;">
p-value
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
day
</td>
<td style="text-align:right;">
9395.22
</td>
<td style="text-align:right;">
92
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
limit
</td>
<td style="text-align:right;">
46.29
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
&lt; 0.001
</td>
</tr>
<tr>
<td style="text-align:left;">
year
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.4
</td>
</tr>
</tbody>
</table>
<p>The residual deviance is 107.95 for 90 degrees of freedom, suggests the overall fit is good, despite the large number of nuisance parameters <span class="math inline">\(\delta_1, \ldots, \delta_{92}\)</span>. The coefficient associated to limit is strongly significant: the estimated coefficient is <span class="math inline">\(\widehat{\alpha}=-0.292\)</span>, indicates that speed limits reduce the mean number of accidents by <span class="math inline">\(25.3\)</span>% on average. In contrast, the likelihood ratio test reported in Table <a href="generalized-linear-models.html#tab:swedenglm">4.5</a> shows that the change in the yearly number of accident from 1961 to 1962, <span class="math inline">\(\gamma\)</span>, is not significantly different from zero.</p>
<div id="overdispersion" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Overdispersion</h3>
<p>One restriction of the Poisson model is that the restriction on its moments is often unrealistic. We tackle the problem of <strong>overdispersion</strong>, for which the variability in the counts is larger than that implied by a Poisson distribution.</p>
<p>Overdispersion is not the only problem encountered in practice: sometimes, the variance is smaller than the mean, typically due to excess of zeros. Hurdle models and zero-inflated models can be used to deal with this issue. The most common framework for handling overdispersion is to consider that the mean <span class="math inline">\(\lambda\)</span> of the Poisson distribution is itself a positive random variable with mean <span class="math inline">\(\mu\)</span>. By the laws of iterated expectation and iterative variance,
<span class="math display">\[\begin{align*}
\mathsf{E}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda\} = \mathsf{E}(\Lambda) = \mu\\
\mathsf{Va}(Y) &amp;= \mathsf{E}_{\Lambda}\{\mathsf{Va}(Y \mid \Lambda)\} + \mathsf{Va}_{\Lambda}\{\mathsf{E}(Y \mid \Lambda)\} = \mathsf{E}(\Lambda) + \mathsf{Va}(\Lambda) = \mu + \mathsf{Va}(\Lambda).
\end{align*}\]</span>
This hierarchical model thus necessarily yield a model in which the variance is higher than the mean. If <span class="math inline">\(\Lambda\)</span> follows a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distribution</a> with scale <span class="math inline">\(k\mu\)</span> and rate <span class="math inline">\(k&gt;0\)</span>, <span class="math inline">\(\Lambda \sim \mathsf{Ga}(k\mu, k)\)</span>. This yields a marginal distribution for the response <span class="math inline">\(Y\)</span> which follows a negative binomial distribution, whose mean and variance are
<span class="math display">\[\begin{align*}
\mathsf{E}(Y) = \mu, \qquad \mathsf{Va}(Y) = \mu + k \mu^2.
\end{align*}\]</span>
The term <span class="math inline">\(k\)</span> is a dispersion parameter, which is fixed for all observations. As <span class="math inline">\(k \to 0\)</span>, the distribution of <span class="math inline">\(\Lambda\)</span> degenerates to a constant at <span class="math inline">\(\mu\)</span> and we recover th e Poisson model. While both negative binomial and Poisson models are nested, the likelihood ratio test is non-regular because the restriction we test lies on the boundary of the parameter space, as <span class="math inline">\(k \in (0, \infty)\)</span>. The consequence is that, in large samples, if the data are truly Poisson and we fit a negative binomial, we expect the maximum likelihood estimator of the dispersion parameter <span class="math inline">\(k\)</span> to be exactly zero half of the time and
to be positive and behave like a <span class="math inline">\(\chi_1^2\)</span> random variable the other half of the time.</p>
<p>Strictly speaking, the negative binomial expansion presented above is not a generalized linear model unless <span class="math inline">\(k\)</span> is known, but we can write down the log-likelihood and fit the model by alternating maximization of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(k\)</span> and perform likelihood-based inference as usual.</p>

<div class="example">
<span id="exm:unnamed-chunk-2" class="example"><strong>Example 4.11  </strong></span>We can ascertain the finite sample properties of the likelihood ratio test by simulating repeatedly from a Poisson regression model, fitting the negative binomial model and calculating the likelihood ratio statistic. Since the negative binomial model yields <span class="math inline">\(k&gt;0\)</span>, we include <span class="math inline">\(k=0\)</span> by fitting a Poisson model and keeping the latter whenever it yields a higher log likelihood than the one found by maximizing the negative binomial model.
</div>
<div class="figure" style="text-align: center"><span id="fig:simunb"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/simunb-1.png" alt="Simulated null distribution of the likelihood ratio test comparing the Poisson and negative binomial regression models. The quantile-quantile plot show the distribution of positive statistics for sizes $n=100, 500, 1000, 2000$ relative to the asymptotic $\frac{1}{2}\chi^2_1$." width="70%" />
<p class="caption">
Figure 4.8: Simulated null distribution of the likelihood ratio test comparing the Poisson and negative binomial regression models. The quantile-quantile plot show the distribution of positive statistics for sizes <span class="math inline">\(n=100, 500, 1000, 2000\)</span> relative to the asymptotic <span class="math inline">\(\frac{1}{2}\chi^2_1\)</span>.
</p>
</div>
<p>The percentage of likelihood ratio statistics that are zero is <span class="math inline">\(69\)</span>%, <span class="math inline">\(60\)</span>%, <span class="math inline">\(59\)</span>% and <span class="math inline">\(54\)</span>% for respectively <span class="math inline">\(n=100\)</span>, <span class="math inline">\(500\)</span>, <span class="math inline">\(1000\)</span> and <span class="math inline">\(2000\)</span>. We can also see from the quantile-quantile plots that the empirical distribution of the positive values is right skewed relative to the <span class="math inline">\(\chi^2_1\)</span> reference. This strongly suggest that asymptotic null distribution, consisting of a point mass with probability half at <span class="math inline">\(0\)</span> and <span class="math inline">\(\chi^2_1\)</span> with probability half is seemingly valid only for very large samples; bear in mind that these findings also don’t account for model misspecification.</p>
</div>
</div>
<div id="modelling-proportions" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Modelling proportions</h2>
<p>Proportions arise when we condition on the total number of observations for a scenario. With Poisson counts, this gives rise to a multinomial distribution for the proportions. We can also consider counts as the fraction of total population, including the total population if counts are not comparable using an offset term.</p>
<p>For the first instance, we can consider <span class="math inline">\(D\)</span> individual counts as Poisson. There is a direct link with (multinomial) logistic model: if we condition on the sum of the counts, we recover a multinomial distribution; for simplicity, we restrict attention to the binomial case when <span class="math inline">\(D=2\)</span>.</p>
<p>Consider independent Poisson counts <span class="math inline">\(Y_1, Y_2\)</span> with respective means <span class="math inline">\(\lambda_1, \lambda_2\)</span>. Suppose the total counts <span class="math inline">\(Y_1 + Y_2 = m\)</span>.
The distribution of <span class="math inline">\(Y_2 \mid Y_1 + Y_2=m \sim \mathsf{Bin}\{m, \lambda_2/(\lambda_1 + \lambda_2)\}\)</span>.
If we model each mean parameter as, <span class="math inline">\(\ln(\lambda_i) = \alpha + \mathbf{x}_i^\top\boldsymbol{\beta}\)</span> where <span class="math inline">\(\mathbf{x}_i\)</span> is a <span class="math inline">\(p\)</span> row vector, then
<span class="math display">\[\begin{align*}
\frac{\lambda_2}{\lambda_1 + \lambda_2} = \frac{\exp\{(\mathbf{x}_2-\mathbf{x}_1)^\top\boldsymbol{\beta}\}}{1+\exp\{(\mathbf{x}_2-\mathbf{x}_1)^\top\boldsymbol{\beta}\}}.
\end{align*}\]</span>
We could thus estimate <span class="math inline">\(\boldsymbol{\beta}\)</span> using a logistic regression, but the common intercept <span class="math inline">\(\alpha\)</span> that appears in the mean of the Poisson counts is not estimable based on this conditional likelihood.</p>

<div class="example">
<span id="exm:swedenex2" class="example"><strong>Example 4.12  </strong></span>
Looking again at the data in Example <a href="generalized-linear-models.html#exm:roadsweden">4.10</a>, we condition on the sum of the number of accidents in 1961 and 1962, <span class="math inline">\(Y_{i1} + Y_{i2}\)</span>, and adjust the same mean model for the Poisson data.
</div>
<p>Recall
<span class="math display">\[\begin{align*}
\frac{\lambda_{i2}}{
\lambda_{i1} + \lambda_{i2}} &amp;= \frac{\exp(\delta_i + \gamma + \alpha \texttt{limit}_{i2})}{(\exp(\delta_i + \gamma + \alpha \texttt{limit}_{i2}) + \exp(\delta_i + \alpha \texttt{limit}_{i1})} \\&amp;= \frac{\exp\{\gamma + \alpha(\texttt{limit}_{i2}-\texttt{limit}_{i1})\}}{1+{\exp\{\gamma + \alpha(\texttt{limit}_{i2}-\texttt{limit}_{i1})\}}}, \qquad i=1, \ldots, 92. 
\end{align*}\]</span></p>
<p>In this case, we were not interested in the nuisance parameters <span class="math inline">\(\delta_1, \ldots, \delta_{92}\)</span>, which were there merely for the sake of controlling for daily changes. The binomial logistic model has only two parameters, but 92 observations rather than 184 since we regroup the counts.
The parameter estimates (standard errors) based on the logistic regression are <span class="math inline">\(\widehat{\alpha} = -0.292 (0.043)\)</span> and <span class="math inline">\(\widehat{\gamma}=-0.029(0.035)\)</span>, so enforcing a speed limit reduces the accident rate by <span class="math inline">\(\exp(-0.292)=0.75\)</span>, corresponding to a 25% decrease. The 95% profile likelihood confidence interval for this reduction, <span class="math inline">\(\exp(\alpha)\)</span>, is <span class="math inline">\([0.60, 0.81]\)</span>, and there are thus strong evidence that the reduction in the number of accident is real. The confidence interval for <span class="math inline">\(\gamma\)</span>, on the other hand, is <span class="math inline">\([-0.097, 0.039]\)</span> includes zero; the yearly change is not significative. Since the inference are exactly the same, one may wonder what the benefit of using the logistic model is; it mostly lies in the fact our model has 2 parameters and not 94…</p>
<p>We present an example of incorrect marginalization that leads to Simpson’s paradox. Models for multi-way contingency tables and proportions are covered in a dedicated section.</p>
<div id="simpsons-paradox" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Simpson’s paradox</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="likelihood.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlated-longitudinal-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60604A_Statistical_modelling.pdf"],
"toc": {
"collapse": "section",
"split_by": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
