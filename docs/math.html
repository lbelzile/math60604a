<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B Supplementary material | Statistical Modelling</title>
  <meta name="description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="generator" content="bookdown 0.30 and GitBook 2.6.7" />

  <meta property="og:title" content="B Supplementary material | Statistical Modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  <meta name="github-repo" content="lbelzile/math60604a" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B Supplementary material | Statistical Modelling" />
  
  <meta name="twitter:description" content="This is a web complement to MATH 60604A (Statistical Modelling), a course given in the M.Sc. in management (Data Science and Business Analytics) at HEC Montréal." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="complement.html"/>
<link rel="next" href="r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css, css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary remarks</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to statistical inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#tests"><i class="fa fa-check"></i><b>1.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#eda"><i class="fa fa-check"></i><b>1.2</b> Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>2.2</b> Ordinary least squares</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#interpretation-of-the-model-parameters"><i class="fa fa-check"></i><b>2.3</b> Interpretation of the model parameters</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#test-lm"><i class="fa fa-check"></i><b>2.4</b> Tests for parameters of the linear model</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#coefR2"><i class="fa fa-check"></i><b>2.5</b> Coefficient of determination</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#predictions-lm"><i class="fa fa-check"></i><b>2.6</b> Predictions</a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#interactions"><i class="fa fa-check"></i><b>2.7</b> Interactions</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#collinearity"><i class="fa fa-check"></i><b>2.8</b> Collinearity</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#graphical-analysis-of-residuals"><i class="fa fa-check"></i><b>2.9</b> Graphical analysis of residuals</a></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#transformation-response"><i class="fa fa-check"></i><b>2.10</b> Transformation of the response</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="likelihood.html"><a href="likelihood.html"><i class="fa fa-check"></i><b>3</b> Likelihood-based inference</a>
<ul>
<li class="chapter" data-level="3.1" data-path="likelihood.html"><a href="likelihood.html#maximum-likelihood"><i class="fa fa-check"></i><b>3.1</b> Maximum likelihood</a></li>
<li class="chapter" data-level="3.2" data-path="likelihood.html"><a href="likelihood.html#liktests"><i class="fa fa-check"></i><b>3.2</b> Likelihood-based tests</a></li>
<li class="chapter" data-level="3.3" data-path="likelihood.html"><a href="likelihood.html#profile-likelihood"><i class="fa fa-check"></i><b>3.3</b> Profile likelihood</a></li>
<li class="chapter" data-level="3.4" data-path="likelihood.html"><a href="likelihood.html#information-criteria"><i class="fa fa-check"></i><b>3.4</b> Information criteria</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#basic-principles"><i class="fa fa-check"></i><b>4.1</b> Basic principles</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#theory-of-generalized-linear-models"><i class="fa fa-check"></i><b>4.2</b> Theory of generalized linear models</a></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>4.3</b> Binary responses</a></li>
<li class="chapter" data-level="4.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#count-data"><i class="fa fa-check"></i><b>4.4</b> Count data</a></li>
<li class="chapter" data-level="4.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#modelling-proportions"><i class="fa fa-check"></i><b>4.5</b> Modelling proportions</a></li>
<li class="chapter" data-level="4.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#rates"><i class="fa fa-check"></i><b>4.6</b> Rates</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="correlated-longitudinal-data.html"><a href="correlated-longitudinal-data.html"><i class="fa fa-check"></i><b>5</b> Correlated and longitudinal data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="correlated-longitudinal-data.html"><a href="correlated-longitudinal-data.html#longitudinal-data"><i class="fa fa-check"></i><b>5.1</b> Longitudinal data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Linear mixed models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#partial-pooling"><i class="fa fa-check"></i><b>6.1</b> Partial pooling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>7</b> Survival analysis</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="complement.html"><a href="complement.html"><i class="fa fa-check"></i><b>A</b> Complements</a>
<ul>
<li class="chapter" data-level="A.1" data-path="complement.html"><a href="complement.html#population-sample"><i class="fa fa-check"></i><b>A.1</b> Population and samples</a></li>
<li class="chapter" data-level="A.2" data-path="complement.html"><a href="complement.html#random-variable"><i class="fa fa-check"></i><b>A.2</b> Random variable</a></li>
<li class="chapter" data-level="A.3" data-path="complement.html"><a href="complement.html#law-large-numbers"><i class="fa fa-check"></i><b>A.3</b> Laws of large numbers</a></li>
<li class="chapter" data-level="A.4" data-path="complement.html"><a href="complement.html#CLT"><i class="fa fa-check"></i><b>A.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>B</b> Supplementary material</a>
<ul>
<li class="chapter" data-level="B.1" data-path="math.html"><a href="math.html#ols"><i class="fa fa-check"></i><b>B.1</b> Derivation of the ordinary least squares estimator</a></li>
<li class="chapter" data-level="B.2" data-path="math.html"><a href="math.html#derivationR2"><i class="fa fa-check"></i><b>B.2</b> Derivation of the coefficient of determination</a></li>
<li class="chapter" data-level="B.3" data-path="math.html"><a href="math.html#restricted-estimation-maximum-likelihood"><i class="fa fa-check"></i><b>B.3</b> Restricted estimation maximum likelihood</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="r.html"><a href="r.html"><i class="fa fa-check"></i><b>C</b> <strong>R</strong></a>
<ul>
<li class="chapter" data-level="C.1" data-path="r.html"><a href="r.html#basics-of-r"><i class="fa fa-check"></i><b>C.1</b> Basics of <strong>R</strong></a></li>
<li class="chapter" data-level="C.2" data-path="r.html"><a href="r.html#rlmfunc"><i class="fa fa-check"></i><b>C.2</b> Linear models in <strong>R</strong> using the <code>lm</code> function</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="math" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">B</span> Supplementary material<a href="math.html#math" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section regroups optional derivations and additional details which are provided for the sake of completeness.</p>
<div id="ols" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">B.1</span> Derivation of the ordinary least squares estimator<a href="math.html#ols" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the optimization problem
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}=\min_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span>
We can compute the derivative of the right hand side with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, set it to zero and solve for <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>,
<span class="math display">\[\begin{align*}
\mathbf{0}_n&amp;=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&amp;=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
\\&amp;=\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span>
using the <a href="http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf">chain rule</a>. Distributing the terms leads to the so-called <em>normal equation</em>
<span class="math display">\[\begin{align*}
\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&amp;=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}\]</span>
If the <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is full-rank, the quadratic form <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible and we obtain the solution to the least square problems provided in Equation <a href="linear-regression.html#eq:ols">(2.3)</a>.</p>
</div>
<div id="derivationR2" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">B.2</span> Derivation of the coefficient of determination<a href="math.html#derivationR2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Because of the orthogonal decomposition <span class="math inline">\(\boldsymbol{y}=\widehat{\boldsymbol{y}} + \boldsymbol{e}\)</span> and provided that the design matrix includes an intercept of <span class="math inline">\(\mathbf{1}_n \in \mathcal{S}(\mathbf{X})\)</span>, then <span class="math inline">\(\overline{\boldsymbol{e}}=0\)</span> and the average of the response and of the fitted values is the same. Since <span class="math inline">\(n^{-1}\sum_{i=1}^n \widehat{y}_i = n^{-1}\sum_{i=1}^n ({y}_i-e_i)=\overline{y}\)</span>,
<span class="math display">\[\begin{align*}
\widehat{\mathsf{Cor}}\left(\widehat{\boldsymbol{y}}, \boldsymbol{y}\right)
&amp;= \frac{(\boldsymbol{y} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n) +
\boldsymbol{e}^\top(\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n)}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{\|\widehat{\boldsymbol{y}} - \overline{y}\mathbf{1}_n\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \frac{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\| - \|\boldsymbol{e}\|}
{\|\boldsymbol{y} - \overline{y}\mathbf{1}_n\|}
\\&amp;= \sqrt{\frac{\mathsf{SS}_c-\mathsf{SS}_e}{\mathsf{SS}_c}}= \mathrm{R}.
\end{align*}\]</span>
This justifies the claim of <a href="linear-regression.html#coefR2">Section 2.5</a> that the squared correlation between the fitted values and the response is equal to <span class="math inline">\(R^2\)</span>.</p>
<div id="optimization-for-generalized-linear-models" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">B.2.1</span> Optimization for generalized linear models<a href="math.html#optimization-for-generalized-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is generally no closed-form expression for the maximum likelihood
estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> in generalized linear models
and the score equation is typically nonlinear in <span class="math inline">\(\boldsymbol{\beta}\)</span>
and <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> must be obtained through iterative
numerical routines.</p>
<p>Starting from Equation <a href="generalized-linear-models.html#eq:expofam">(4.1)</a>, we differentiate the log likelihood function <span class="math inline">\(\ell = \sum_{i=1}^n \ell_i\)</span> with respect to
<span class="math inline">\(\boldsymbol{\beta}\)</span>. For simplicity, we consider each likelihood
contribution and coefficient in turn. By the chain rule, <span class="math display">\[\begin{align*}
\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial
\eta_i}{\partial \beta_j} \frac{\partial \mu_i}{\partial \eta_i}
\frac{\partial \theta_i}{\partial \mu_i}\frac{\partial
\ell_i}{\partial \theta_i}
\end{align*}\]</span>
and the earlier derivations show <span class="math inline">\(\partial \ell_i/\partial \theta_i = (y_j-\mu_i)/a_i(\phi)\)</span> and <span class="math inline">\(\partial \mu_i / \partial \theta_i = b&#39;&#39;(\theta_i) = \mathsf{Va}(Y_i)/a_i(\phi)\)</span>.
The derivative of the linear predictor, <span class="math inline">\(\partial \eta_i / \partial \beta_j = \mathrm{X}_{ij}\)</span>. The only missing term, <span class="math inline">\(\partial \mu_i/\partial \eta_i\)</span>, depends on the choice of link function through <span class="math inline">\(\eta_i = g(\mu_i)\)</span>, but it is unity for the canonical link function.</p>
<p>Let
<span class="math display">\[\begin{align*}
U(\boldsymbol{\beta}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}}, \qquad j(\boldsymbol{\beta}) = - \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}
\end{align*}\]</span>
denote respectively the gradient and the hessian of the log likelihood function; summing all the likelihood contribution, the <span class="math inline">\(j\)</span>th element of the score vector
<span class="math inline">\(\boldsymbol{U}\)</span> is
<span class="math display">\[\begin{align*}
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \frac{(y_i-\mu_i)\mathrm{X}_{ij}}{g&#39;(\mu_i)V(\mu_i)a_i(\phi)}, \qquad j=0, \ldots, p.
\end{align*}\]</span>
In general, <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> solves the score equation <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})=\boldsymbol{0}_{p+1}\)</span>, so we can device a Newton–Raphson algorithm to obtain the maximum likelihood
estimates. This amounts to a first order Taylor series expansion of
the score vector <span class="math inline">\(U(\widehat{\boldsymbol{\beta}})\)</span> around <span class="math inline">\(\boldsymbol{\beta}\)</span>,
<span class="math display">\[\begin{align*}
\boldsymbol{0}_{p+1} = U(\widehat{\boldsymbol{\beta}}) \stackrel{\cdot}{=} U(\beta) - j(\boldsymbol{\beta}) (\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})
\end{align*}\]</span>
If the <span class="math inline">\((p+1)\)</span> matrix <span class="math inline">\(j(\boldsymbol{\beta}^{(t)})\)</span> is
invertible, we can thus device an iterative algorithm: starting from
some initial value <span class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span>, we compute at step <span class="math inline">\(t+1\)</span>
<span class="math display">\[\begin{align*}
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + j^{-1}(\boldsymbol{\beta}^{(t)})U(\boldsymbol{\beta}^{(t)}).
\end{align*}\]</span>
and iterate this formula until convergence. Most softwares
implement a variant of this algorithm, in which the negative hessian
<span class="math inline">\(j(\boldsymbol{\beta})\)</span> is replaced by its absolute value
<span class="math inline">\(i(\boldsymbol{\beta})\)</span>: the resulting algorithm is known as Fisher
scoring. For generalized linear models, these recursions can be done by
repeatedly computing a variant of least squares known as iteratively
reweighted least squares.</p>
</div>
<div id="residuals-glm" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">B.2.2</span> Residuals for generalized linear models<a href="math.html#residuals-glm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the individual contributions to the deviance and Pearson
<span class="math inline">\(X^2\)</span> statistic to build residuals for generalized linear models. By considering
<span class="math inline">\(D = \sum_{i=1}^n d_i^2\)</span>, where
<span class="math display">\[\begin{align*}
d_i &amp;= \mathrm{sign}(\widetilde{\eta}_i - \widehat{\eta}_i) \{2\ell(\widetilde{\eta}_i; y_i) - 2\ell(\widehat{\eta}_i; y_i)\}^{1/2}
\end{align*}\]</span>
and the calculations simplify upon replacing the formula of
the log likelihood for the generic exponential family member,
<span class="math display">\[\begin{align*}
d_i^2=2 \left\{y_i (\widetilde{\theta}_i - \widehat{\theta}_i) - b(\widetilde{\theta}_i) + b(\widehat{\theta}_i)\right\}
\end{align*}\]</span>
The terms <span class="math inline">\(d_i\)</span> are called deviance residuals, whereas
Pearson residuals are based on the score contributions
<span class="math inline">\(u_i(\widehat{\beta}) w_i(\widehat{\beta})^{-1/2}\)</span>, where the score statistic <span class="math inline">\(u(\boldsymbol{\beta})\)</span> and the weights <span class="math inline">\(w_i\)</span> are
<span class="math display">\[\begin{align*}
u_i &amp;= \frac{\partial \theta_i}{\partial \eta_i} \frac{\partial \ell_i(\theta_i)}{\partial \theta_i} = \frac{y_i - \mu_i}{g&#39;(\mu_i)a_i(\phi)V(\mu_i)}\\
w_i &amp;= \left(\frac{\partial \theta_i}{\partial \eta_i}\right)^2 \frac{\partial^2 \ell_i(\theta_i)}{\partial \theta_i^2} = \frac{1}{g&#39;(\mu_i)^2 a_i(\phi)V(\mu_i)}
\end{align*}\]</span></p>
<p>In practice, these residuals are heteroscedastic and it is better to
standardize them by considering instead
<span class="math display">\[\begin{align*}
r_{D_i} = \frac{d_i}{(1-h_{ii})^2}, \qquad r_{P_i} = \frac{u_i(\widehat{\beta})}{\{w_i(\widehat{\beta})(1-h_{ii})\}^{1/2}},
\end{align*}\]</span>
both are scaled by <span class="math inline">\((1-h_{ii})^{1/2}\)</span>, a formula remniscent
of the linear model framework. In the above formulas, the leverage
<span class="math inline">\(h_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of the matrix <span class="math display">\[\begin{align*}
\mathbf{H}_{\mathbf{X}} = \mathbf{W}^{1/2}\mathbf{X}(\mathbf{X}^\top\mathbf{W}\mathbf{X})^{-1} \mathbf{X}^\top\mathbf{W}^{1/2};
\end{align*}\]</span>
since the terms of
<span class="math inline">\(\mathbf{W}=\mathrm{diag}\{w_1, \ldots, w_n\}\)</span> depend on the unknown
coefficient, the latter is estimated by replacing <span class="math inline">\(\boldsymbol{\beta}\)</span>
by <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>
<p>The standardized deviance residuals <span class="math inline">\(\{r_{D_i}\}\)</span> and standardized
Pearson residuals <span class="math inline">\(\{r_{P_i}\}\)</span> should have an approximate standard
normal distribution in large samples, but their distribution can be
skewed. Like in the linear regression, we will work with the
<strong>jackknifed deviance residuals</strong> for residual plots <span class="math display">\[\begin{align*}
r_{J_i} &amp;= \mathrm{sign}(y_i - \widehat{\mu}_i) \left\{ (1-{h_ii})r^2_{D_i} + h_{ii}r^2_{P_i}\right\}^{1/2}
\end{align*}\]</span></p>
<p>For ordinary linear regression, both <span class="math inline">\(r_{D_i}\)</span> and <span class="math inline">\(r_{P_i}\)</span> reduce to
the standardized residuals <span class="math inline">\(t_i=e_i\{S^2(1-h_{ii})\}^{-1/2}\)</span>.</p>
<p>There are clear parallels between generalized linear models and linear
models: we have so far derived an analog of residuals and leverage.
Collinearity is also an issue for generalized linear model; for the
latter, we define the Cook statistic as the change in the deviance,
<span class="math display">\[\begin{align*}
C = \frac{1}{p} 2\{\ell(\widehat{\boldsymbol{\beta}}) - \ell(\widehat{\boldsymbol{\beta}}_{-j})\},
\end{align*}\]</span>
where <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{-j}\)</span> is the estimate
obtained by dropping the <span class="math inline">\(j\)</span>th observation from the sample. This
requires fitting <span class="math inline">\(n\)</span> different models, which is computationally
prohibitive. In the linear regression, we can calculate the Cook
distance from the formula <span class="math inline">\(C_j = (p+1)^{-1}t_i^2h_{ii}/(1-h_{ii})\)</span>,
where <span class="math inline">\(t_i\)</span> are the standardized residuals defined in the previous
section. For generalized linear models, no such expression exists,
although a good approximation is
<span class="math inline">\(C_j \approx (p+1)^{-1}r_{P_i}^2h_{ii}/(1-h_{ii})\)</span>.</p>
<p>Diagnostic plots for generalized linear models are harder to interpret
because of the lack of orthogonality. It is customary to plot the
jackknife deviance residuals against the linear predictor
<span class="math inline">\(\widehat{\eta}_i\)</span>, produce normal quantile-quantile plots of
standardized deviance residuals and the approximate Cook statistics
against the <span class="math inline">\(h_{ii}/(1-h_{ii})\)</span>. We will show examples of such plots for
particular models.</p>
</div>
<div id="diag-plots-binary" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">B.2.3</span> Diagnostic plots for binary data<a href="math.html#diag-plots-binary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are also other issues arising from the discreteness of the
observations. Since observations are 0/1, residuals are often separated.
Figure <a href="math.html#fig:diagplotsbirthwt1">B.1</a> shows residual plots for Example
<a href="generalized-linear-models.html#exm:birthweightex">4.8</a>: both plots are of limited use to assess
goodness-of-fit and model assumptions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diagplotsbirthwt1"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagplotsbirthwt1-1.png" alt="Diagnostic plots for the binary regression for the birth weight data: jackknife deviance residuals against linear predictor (left) and quantile-quantile plot of ordered deviance residuals (right)." width="70%" />
<p class="caption">
Figure B.1: Diagnostic plots for the binary regression for the birth weight data: jackknife deviance residuals against linear predictor (left) and quantile-quantile plot of ordered deviance residuals (right).
</p>
</div>
<p>Another strategy is use of quantile residuals
<span class="citation">(<a href="references.html#ref-Brillinger/Preisler:1983" role="doc-biblioref">Brillinger and Preisler 1983</a>)</span>. If <span class="math inline">\(Y \sim F\)</span>, the quantile transform gives <span class="math inline">\(U=F(Y) \sim \mathsf{U}(0,1)\)</span>, meaning uniformly distributed on <span class="math inline">\((0,1)\)</span>. Replacing the unknown distribution <span class="math inline">\(F\)</span> by <span class="math inline">\(\widehat{F}\)</span> should yield approximately uniform observations. With <span class="math inline">\(\widehat{\pi}_i\)</span> denoting the probability of success, we take
<span class="math display">\[\begin{align*}
U_i = U_{i1} Y_i + U_{i2}(1-Y_i), \qquad U_{i1} \sim \mathsf{U}(0, \widehat{\pi}_i), \quad U_{i2} \sim \mathsf{U}(\widehat{\pi}_i, 1)
\end{align*}\]</span>
and the resulting <strong>uniform residuals</strong> will be, as their name hints, approximately uniform on <span class="math inline">\((0,1)\)</span>. The drawback of this approach lies in the randomness of the residuals. Figure <a href="math.html#fig:diagplotbirthwgt">B.2</a> shows the diagnostic plots based on the uniform residuals (top two rows): there is no overall indication of poor fit, except for seemingly too few low/high residuals. The two observations with high leverage correspond to non-smoker mothers with no premature labour record: one with hypertension and a weight of 95lbs (unusual combination), the other weighting 200 lbs, with no hypertension and presence of uterine
irritability.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diagplotbirthwgt"></span>
<img src="MATH60604A_Statistical_modelling_files/figure-html/diagplotbirthwgt-1.png" alt="Diagnostic plots based on uniform residuals for the birth weight data. From left to right, top to bottom: residuals against explanatory and linear predictors, histogram of uniform residuals and Tukey's quantile-quantile plot, Cook statistic against weighted leverage and case number. " width="70%" />
<p class="caption">
Figure B.2: Diagnostic plots based on uniform residuals for the birth weight data. From left to right, top to bottom: residuals against explanatory and linear predictors, histogram of uniform residuals and Tukey’s quantile-quantile plot, Cook statistic against weighted leverage and case number.
</p>
</div>
</div>
</div>
<div id="restricted-estimation-maximum-likelihood" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">B.3</span> Restricted estimation maximum likelihood<a href="math.html#restricted-estimation-maximum-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most common estimation technique for fitting linear mixed models or fixed effect models in which the variance of the error term is not <span class="math inline">\(\sigma^2\mathbf{I}_n\)</span> is the restricted estimation maximum likelihood (REML) method.</p>
<p>For the linear mixed model, the marginal distribution of the <span class="math inline">\(n\)</span> response vector is
<span class="math inline">\(\boldsymbol{Y} \sim \mathsf{No}_n\{\mathbf{X} \boldsymbol{\beta}, \boldsymbol{\Upsilon}^{-1}(\boldsymbol{\psi})\}\)</span>, where <span class="math inline">\(\boldsymbol{\Upsilon}^{-1}(\boldsymbol{\psi}) = \mathbf{Z}\boldsymbol{\Omega}\mathbf{Z}^\top + \mathbf{R}\)</span> and <span class="math inline">\(\mathbf{R} = \mathrm{blockdiag}(\mathbf{R}_1, \ldots, \mathbf{R}_m)\)</span>, the covariance matrix of the errors, is block-diagonal. The multivariate normal distribution has covariance parameters <span class="math inline">\(\boldsymbol{\psi}\)</span> and mean parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>The log likelihood for the model is, up to proportionality constants
<span class="math display">\[\begin{align*}
\ell(\boldsymbol{\beta},\boldsymbol{\psi}; \boldsymbol{y}) \propto  \frac{1}{2} \ln |\boldsymbol{\Upsilon}|- \frac{1}{2} (\boldsymbol{y}- \mathbf{X}\boldsymbol{\beta})^\top\boldsymbol{\Upsilon}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span></p>
<p>For known <span class="math inline">\(\boldsymbol{\psi}\)</span>, the precision matrix <span class="math inline">\(\boldsymbol{\Upsilon}\)</span> is fully resolved; the restricted maximum likelihood for the parameters of the mean vector <span class="math inline">\(\boldsymbol{\beta}\)</span> is
<span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} = (\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\Upsilon}\boldsymbol{y}.
\end{align*}\]</span>
We can show that <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} \sim \mathsf{No}_{p+1}\{\boldsymbol{\beta}, (\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X})^{-1}\}\)</span> and thus write the joint density of the response given mean and variance parameters as
<span class="math display">\[\begin{align*}
f(\boldsymbol{y}; \boldsymbol{\psi}, \boldsymbol{\beta}) = f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}, \boldsymbol{\beta}, \boldsymbol{\psi}) f(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}; \boldsymbol{\psi}, \boldsymbol{\beta}).
\end{align*}\]</span>
Given <span class="math inline">\(\boldsymbol{\psi}\)</span>, <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}\)</span> is a sufficient statistic for <span class="math inline">\(\boldsymbol{\beta}\)</span>, the conditional density depends on <span class="math inline">\(\boldsymbol{\beta}\)</span> only through <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}\)</span>, so <span class="math inline">\(f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}, \boldsymbol{\beta}, \boldsymbol{\psi}) = f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}, \boldsymbol{\psi})\)</span>.
Rather than using the full likelihood, the REML maximizes the conditional log likelihood
<span class="math display">\[\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi})= \ln f(\boldsymbol{y}; \widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}},  \boldsymbol{\psi}) = \ln f(\boldsymbol{y}; \boldsymbol{\psi}, \boldsymbol{\beta}) - \ln f(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}; \boldsymbol{\psi}, \boldsymbol{\beta}).
\end{align*}\]</span>
The solution thus differs from that of the maximum log likelihood. The log conditional density is
<span class="math display">\[\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi})
&amp;= \frac{1}{2} \ln |\boldsymbol{\Upsilon}| - \frac{1}{2} (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top\boldsymbol{\Upsilon}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\\&amp; \quad - \frac{1}{2} \ln \left|\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X}\right| + \frac{1}{2} \left(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}-\boldsymbol{\beta}\right)^\top \mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X} \left(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}}-\boldsymbol{\beta}\right)
\end{align*}\]</span>
and upon simplifying terms using the fact <span class="math inline">\(\boldsymbol{y} - \mathbf{X}\boldsymbol{\beta} = \boldsymbol{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} + \mathbf{X}(\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}} - \boldsymbol{\beta})\)</span>, we retrieve the REML log likelihood,
<span class="math display">\[\begin{align*}
\ell_{\mathrm{r}}(\boldsymbol{\psi}) = \frac{1}{2}|\boldsymbol{\Upsilon}| - \frac{1}{2} \ln \left|\mathbf{X}^\top\boldsymbol{\Upsilon}\mathbf{X}\right|  - \frac{1}{2}(\boldsymbol{y} -\mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}})^\top \boldsymbol{\Upsilon}(\boldsymbol{y} -\mathbf{X}\widehat{\boldsymbol{\beta}}_{\boldsymbol{\psi}})
\end{align*}\]</span>
Once we have obtained <span class="math inline">\(\widehat{\boldsymbol{\psi}}_{\mathrm{r}}= \max_{\boldsymbol{\psi}} \ell_{\mathrm{r}}(\boldsymbol{\psi})\)</span>, we replace the resulting <span class="math inline">\(\widehat{\boldsymbol{\Upsilon}}\)</span> value in
<span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\widehat{\boldsymbol{\psi}}_{\mathrm{r}}}\)</span>
to get the REML estimates.
Because the estimation of <span class="math inline">\(\boldsymbol{\psi}\)</span> is based on only part of the full likelihood, estimators for the covariance parameter differ from their counterpart based on the full likelihood; in practice, the latter are often more biased, which explains the popularity of REML. The sufficient statistic <span class="math inline">\(\widehat{\boldsymbol{\beta}}_{\widehat{\boldsymbol{\psi}}}\)</span> depends on the particular model matrix <span class="math inline">\(\mathbf{X}\)</span>, so models with different fixed effects cannot be compared based on their maximum restricted likelihood values.</p>
<p>It should be noted that optimisation for linear mixed model is challenging for large <span class="math inline">\(n\)</span> and direct matrix inversion must be avoided, as inverting a square <span class="math inline">\(n \times n\)</span> matrix requires <span class="math inline">\(\mathrm{O}(n^3)\)</span> flops. Software for mixed models must use clever tricks to capitalize on the sparsity of <span class="math inline">\(\boldsymbol{\Upsilon}\)</span>, which often follows from that of <span class="math inline">\(\mathbf{R}\)</span>; oftentimes <span class="math inline">\(\mathbf{R} = \sigma^2 \mathbf{I}_n\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="complement.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH60604A_Statistical_modelling.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"split_by": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
