<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a web complement for MATH 60604A Statistical Modelling, a master course offered at HEC Montréal.">

<title>1&nbsp; Introduction – MATH 60604A - Statistical Modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./inference.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="css/style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./introduction.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH 60604A - Statistical Modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math60604a/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH60604A.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihood-based inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linearmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#population-sample" id="toc-population-sample" class="nav-link active" data-scroll-target="#population-sample"><span class="header-section-number">1.1</span> Population and samples</a>
  <ul class="collapse">
  <li><a href="#variable-type" id="toc-variable-type" class="nav-link" data-scroll-target="#variable-type"><span class="header-section-number">1.1.1</span> Variable type</a></li>
  </ul></li>
  <li><a href="#random-variable" id="toc-random-variable" class="nav-link" data-scroll-target="#random-variable"><span class="header-section-number">1.2</span> Random variable</a></li>
  <li><a href="#discrete-distributions" id="toc-discrete-distributions" class="nav-link" data-scroll-target="#discrete-distributions"><span class="header-section-number">1.3</span> Discrete distributions</a></li>
  <li><a href="#continuous-distributions" id="toc-continuous-distributions" class="nav-link" data-scroll-target="#continuous-distributions"><span class="header-section-number">1.4</span> Continuous distributions</a></li>
  <li><a href="#graphs" id="toc-graphs" class="nav-link" data-scroll-target="#graphs"><span class="header-section-number">1.5</span> Graphs</a></li>
  <li><a href="#law-large-numbers" id="toc-law-large-numbers" class="nav-link" data-scroll-target="#law-large-numbers"><span class="header-section-number">1.6</span> Laws of large numbers</a></li>
  <li><a href="#CLT" id="toc-CLT" class="nav-link" data-scroll-target="#CLT"><span class="header-section-number">1.7</span> Central Limit Theorem</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="intro" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter reviews some basic notions of probability and statistics that are normally covered in undergraduate or college.</p>
<section id="population-sample" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="population-sample"><span class="header-section-number">1.1</span> Population and samples</h2>
<p>Statistics is the science of uncertainty quantification: of paramount importance is the notion of randomness. Generally, we will seek to estimate characteristics of a population using only a sample (a sub-group of the population of smaller size).</p>
<p>The <strong>population of interest</strong> is a collection of individuals which the study targets. For example, the Labour Force Survey (LFS) is a monthly study conducted by Statistics Canada, who define the target population as “all members of the selected household who are 15 years old and older, whether they work or not.” Asking every Canadian meeting this definition would be costly and the process would be long: the characteristic of interest (employment) is also a snapshot in time and can vary when the person leaves a job, enters the job market or become unemployed.</p>
<p>In general, we therefore consider only <strong>samples</strong> to gather the information we seek to obtain. The purpose of <strong>statistical inference</strong> is to draw conclusions about the population, but using only a share of the latter and accounting for sources of variability. George Gallup made this great analogy between sample and population:</p>
<blockquote class="blockquote">
<p>One spoonful can reflect the taste of the whole pot, if the soup is well-stirred</p>
</blockquote>
<p>A <strong>sample</strong> is a random sub-group of individuals drawn from the population. Creation of sampling plans is a complex subject and semester-long sampling courses would be required to evens scratch the surface of the topic. Even if we won’t be collecting data, keep in mind the following information: for a sample to be good, it must be representative of the population under study. Selection bias must be avoided, notably samples of friends or of people sharing opinions.</p>
<p>Because the individuals are selected at <strong>random</strong> to be part of the sample, the measurement of the characteristic of interest will also be random and change from one sample to the next. However, larger samples of the same quality carry more information and our estimator will be more precise. Sample size is not guarantee of quality, as the following example demonstrates.</p>
<div id="exm-Galluppoll" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Polling for the 1936 USA Presidential Election)</strong></span> <em>The Literary Digest</em> surveyed 10 millions people by mail to know voting preferences for the 1936 USA Presidential Election. A sizeable share, 2.4 millions answered, giving Alf Landon (57%) over incumbent President Franklin D. Roosevelt (43%). The latter nevertheless won in a landslide election with 62% of votes cast, a 19% forecast error. <a href="https://www.jstor.org/stable/2749114">Biased sampling and differential non-response are mostly responsible for the error:</a> the sampling frame was built using “phone number directories, drivers’ registrations, club memberships, etc.”, all of which skewed the sample towards rich upper class white people more susceptible to vote for the GOP.</p>
<p>In contrast, Gallup correctly predicted the outcome by polling (only) 50K inhabitants. <a href="https://ozanozbey.medium.com/two-lessons-of-sampling-bias-from-1936-us-election-e4e96bd42be">Read the full story here.</a></p>
</div>
<section id="variable-type" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="variable-type"><span class="header-section-number">1.1.1</span> Variable type</h3>
<ul>
<li>a <strong>variable</strong> represents a characteristic of the population, for example the sex of an individual, the price of an item, etc.</li>
<li>an <strong>observation</strong> is a set of measures (variables) collected under identical conditions for an individual or at a given time.</li>
</ul>
<div class="cell" data-layout-align="center">
<div id="tbl-data-renfe" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-data-renfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: First lines of the <code>renfe</code> database, which contains the price of 10K train tickets between Madrid and Barcelona. The columns <code>price</code> and <code>duration</code> represent continuous variables, all others are categorical.
</figcaption>
<div aria-describedby="tbl-data-renfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 9%">
<col style="width: 12%">
<col style="width: 16%">
<col style="width: 13%">
<col style="width: 26%">
<col style="width: 13%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;">price</th>
<th style="text-align: left;">type</th>
<th style="text-align: left;">class</th>
<th style="text-align: left;">fare</th>
<th style="text-align: left;">dest</th>
<th style="text-align: right;">duration</th>
<th style="text-align: left;">wday</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">143.4</td>
<td style="text-align: left;">AVE</td>
<td style="text-align: left;">Preferente</td>
<td style="text-align: left;">Promo</td>
<td style="text-align: left;">Barcelona-Madrid</td>
<td style="text-align: right;">190</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: right;">181.5</td>
<td style="text-align: left;">AVE</td>
<td style="text-align: left;">Preferente</td>
<td style="text-align: left;">Flexible</td>
<td style="text-align: left;">Barcelona-Madrid</td>
<td style="text-align: right;">190</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">86.8</td>
<td style="text-align: left;">AVE</td>
<td style="text-align: left;">Preferente</td>
<td style="text-align: left;">Promo</td>
<td style="text-align: left;">Barcelona-Madrid</td>
<td style="text-align: right;">165</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td style="text-align: right;">86.8</td>
<td style="text-align: left;">AVE</td>
<td style="text-align: left;">Preferente</td>
<td style="text-align: left;">Promo</td>
<td style="text-align: left;">Barcelona-Madrid</td>
<td style="text-align: right;">190</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="odd">
<td style="text-align: right;">69.0</td>
<td style="text-align: left;">AVE-TGV</td>
<td style="text-align: left;">Preferente</td>
<td style="text-align: left;">Promo</td>
<td style="text-align: left;">Barcelona-Madrid</td>
<td style="text-align: right;">175</td>
<td style="text-align: left;">4</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The choice of statistical model and test depends on the underlying type of the data collected. There are many choices: quantitative (discrete or continuous) if the variables are numeric, or qualitative (binary, nominal, ordinal) if they can be described using an adjective; I prefer the term categorical, which is more evocative.</p>
<p>Most of the models we will deal with are so-called regression models, in which the mean of a quantitative variable is a function of other variables, termed explanatories. There are two types of numerical variables</p>
<ul>
<li>a discrete variable takes a finite or countable number of values, prime examples being binary variables or count variables.</li>
<li>a continuous variable can take (in theory) an infinite possible number of values, even when measurements are rounded or measured with a limited precision (time, width, mass). In many case, we could also consider discrete variables as continuous if they take enough values (e.g., money).</li>
</ul>
<p>Categorical variables take only a finite of values. They are regrouped in two groups,</p>
<ul>
<li>nominal if there is no ordering between levels (sex, color, country of origin) or</li>
<li>ordinal if they are ordered (Likert scale, salary scale) and this ordering should be reflected in graphs or tables.</li>
</ul>
<p>We will bundle every categorical variable using arbitrary encoding for the levels: for modelling, these variables taking <span class="math inline">\(K\)</span> possible values (or levels) must be transformed into a set of <span class="math inline">\(K-1\)</span> binary 0/1 variables, the omitted level corresponding to a baseline. Failing to declare categorical variables in your favorite software is a common mistake, especially when these are saved in the database using integers rather than strings.</p>
</section>
</section>
<section id="random-variable" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="random-variable"><span class="header-section-number">1.2</span> Random variable</h2>
<p>Suppose we wish to describe the behaviour of a stochastic phenomenon. To this effect, one should enumerate the set of possible values taken by the variable of interest and their probability: this is what is encoded in the distribution.</p>
<p>Random variables are denoted using capital letters: for example <span class="math inline">\(Y \sim \mathsf{normal}(\mu, \sigma^2)\)</span> indicates that <span class="math inline">\(Y\)</span> follows a normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma&gt;0.\)</span> If the values of the latter are left unspecified, we talk about the family of distributions. When the values are given, for example <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>, we deal with a single distribution for which a function encode the probability of the underlying variable.</p>
<div id="def-cdf" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Distribution function, mass function and density)</strong></span> The (cumulative) distribution function <span class="math inline">\(F(y)\)</span> gives the cumulative probability that an event doesn’t exceed a given numerical value <span class="math inline">\(y\)</span>, <span class="math inline">\(F(y) = \mathsf{Pr}(Y \leq y).\)</span></p>
<p>If <span class="math inline">\(Y\)</span> is discrete, then it has atoms of non-zero probability and we call <span class="math inline">\(f\)</span> the mass function, and <span class="math inline">\(f(y)=\mathsf{Pr}(Y=y)\)</span> gives the probability of each outcome <span class="math inline">\(y.\)</span> In the continuous case, no numerical value has non-zero probability and so we consider intervals instead. The density function <span class="math inline">\(f(x)\)</span> is non-negative and satisfies <span class="math inline">\(\int_{\mathbb{R}} f(x) \mathrm{d}x=1\)</span>: the integral over a set <span class="math inline">\(B\)</span> (the area under the curve) gives the probability of <span class="math inline">\(Y\)</span> falling inside <span class="math inline">\(B \in \mathbb{R}.\)</span> It follows that the distribution function of a continuous random variable is simply <span class="math inline">\(F(y) = \int_{-\infty}^y f(x) \mathrm{d} x.\)</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/02-ttest-DF_illustration.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>(Cumulative) distribution functions (top) and density/mass functions (bottom) of continuous (left) and discrete (right) random variables.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>One of the first topics covered in introductory statistics is descriptive statistics such as the mean and standard deviation. These are estimators of (centered) moments, which characterise a random variable. In the case of the standard normal distribution, the expectation and variance fully characterize the distribution.</p>
<div id="def-moments" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Moments)</strong></span> Let <span class="math inline">\(Y\)</span> be a random variable with density (or mass) function <span class="math inline">\(f(x).\)</span> The <strong>expectation</strong> (or theoretical mean) of a continuous random variable <span class="math inline">\(Y\)</span> is <span class="math display">\[\begin{align*}
\mathsf{E}(Y)=\int_{\mathbb{R}} x f(x) \mathrm{d} x.
\end{align*}\]</span> In the discrete case, we set rather <span class="math inline">\(\mu = \mathsf{E}(Y)=\sum_{x \in \mathcal{X}} x \mathsf{Pr}(X=x)\)</span>, where <span class="math inline">\(\mathcal{X}\)</span> denotes the support of <span class="math inline">\(Y\)</span>, the set of numerical values at which the probability of <span class="math inline">\(Y\)</span> is non-zero. More generally, we can look at the expectation of a function <span class="math inline">\(g(x)\)</span> for <span class="math inline">\(Y\)</span>, which is nothing but the integral (or sum in the discrete case) of <span class="math inline">\(g(x)\)</span> weighted by the density or mass function of <span class="math inline">\(f(x).\)</span> In the same fashion, provided the integral is finite, the variance is <span class="math display">\[\begin{align*}
\mathsf{Va}(Y)=\mathsf{E}\{Y-\mathsf{E}(Y)\}^2 \equiv \int_{\mathbb{R}} (x-\mu)^2 f(x) \mathrm{d} x.
\end{align*}\]</span> The <strong>standard deviation</strong> is the square root of the variance, <span class="math inline">\(\mathsf{sd}(Y)=\sqrt{\mathsf{Va}(Y)}\)</span>: it units are the same as those of <span class="math inline">\(Y\)</span> and are thus more easily interpreted.</p>
<p>The notion of moments can be extended to higher dimensions. <!--
If we assume data are independent, their joint distribution factorizes into the product of the individual mass function/density of each sample point. Otherwise, a multivariate mass function or density function describes the behaviour of the random vector.
--> Consider an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\boldsymbol{Y}.\)</span> In the regression setting, the response <span class="math inline">\(\boldsymbol{Y}\)</span> would usually comprise repeated measures on an individual, or even observations from a group of individuals.</p>
<p>The expected value (theoretical mean) of the vector <span class="math inline">\(\boldsymbol{Y}\)</span> is calculated componentwise, i.e., <span class="math display">\[\begin{align*}
\mathsf{E}(\boldsymbol{Y}) &amp;= \boldsymbol{\mu}=
\begin{pmatrix}
\mathsf{E}(Y_1) &amp;
\cdots  &amp;
\mathsf{E}(Y_n)
\end{pmatrix}^\top
\end{align*}\]</span> whereas the second moment of <span class="math inline">\(\boldsymbol{Y}\)</span> is encoded in the <span class="math inline">\(n \times n\)</span> <strong>covariance</strong> matrix <span class="math display">\[\begin{align*}
\mathsf{Va}(\boldsymbol{Y}) &amp;= \boldsymbol{\Sigma} = \begin{pmatrix} \mathsf{Va}(Y_1) &amp; \mathsf{Co}(Y_1, Y_2)  &amp; \cdots &amp; \mathsf{Co}(Y_1, Y_n) \\
\mathsf{Co}(Y_2, Y_1) &amp; \mathsf{Va}(Y_2) &amp; \ddots &amp; \vdots \\
\vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
\mathsf{Co}(Y_n, Y_1) &amp; \mathsf{Co}(Y_n, Y_2) &amp;\cdots &amp; \mathsf{Va}(Y_n)
\end{pmatrix}
\end{align*}\]</span> The <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, <span class="math inline">\(\sigma_{ii}=\sigma_i^2\)</span>, is the variance of <span class="math inline">\(Y_i\)</span>, whereas the off-diagonal entries <span class="math inline">\(\sigma_{ij}=\sigma_{ji}\)</span> <span class="math inline">\((i \neq j)\)</span> are the covariance of pairwise entries, with <span class="math display">\[\begin{align*}
\mathsf{Co}(Y_i, Y_j) = \int_{\mathbb{R}^2} (y_i-\mu_i)(y_j-\mu_j) f_{Y_i, Y_j}(y_i, y_j) \mathrm{d} y_i \mathrm{d} y_j.
\end{align*}\]</span> The covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is thus symmetric. It is customary to normalize the pairwise dependence so they do not depend on the component variance. The linear <strong>correlation</strong> between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> is <span class="math display">\[\begin{align*}
\rho_{ij}=\mathsf{Cor}(Y_i,Y_j)=\frac{\mathsf{Co}(Y_i, Y_j)}{\sqrt{\mathsf{Va}(Y_i)}\sqrt{\mathsf{Va}(Y_j)}}=\frac{\sigma_{ij}}{\sigma_i\sigma_j}.
\end{align*}\]</span> The correlation matrix of <span class="math inline">\(\boldsymbol{Y}\)</span> is an <span class="math inline">\(n\times n\)</span> symmetric matrix with ones on the diagonal and the pairwise correlations off the diagonal, <span class="math display">\[\begin{align*}
\mathsf{Cor}(\boldsymbol{Y})=
\begin{pmatrix}
1 &amp; \rho_{12} &amp; \rho_{13} &amp; \cdots &amp; \rho_{1n}\\
\rho_{21} &amp; 1 &amp; \rho_{23} &amp; \cdots &amp; \rho_{2n} \\
\rho_{31} &amp; \rho_{32} &amp; 1 &amp; \ddots &amp; \rho_{3n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \\
\rho_{n1} &amp; \rho_{n2} &amp; \rho_{n3} &amp; \cdots &amp; 1
\end{pmatrix}.
\end{align*}\]</span> One of the most important parts of modelling correlated (or longitudinal) data is the need to account for within-group correlations. This basically comes down to modelling a covariance matrix for observations within the same group (or within the same individual in the case of repeated measures), which is the object of <a href="correlated-longitudinal-data">Chapter 5</a>.</p>
</div>
<div id="def-bias" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 (Bias)</strong></span> The bias of an estimator <span class="math inline">\(\hat{\theta}\)</span> for a parameter <span class="math inline">\(\theta\)</span> is <span class="math display">\[\begin{align*}
\mathsf{bias}(\hat{\theta})=\mathsf{E}(\hat{\theta})- \theta
\end{align*}\]</span> The estimator is unbiased if its bias is zero.</p>
</div>
<div id="exm-unbiased-estimator" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Unbiased estimators)</strong></span> The unbiased estimator of the mean and the variance of <span class="math inline">\(Y\)</span> are <span class="math display">\[\begin{align*}
\overline{Y}_n &amp;= n^{-1} \sum_{i=1}^n Y_i\\
S_n &amp;= (n-1)^{-1} \sum_{i=1}^n (Y_i-\overline{Y})^2.
\end{align*}\]</span></p>
</div>
<p>While unbiasedness is a desirable property, there may be cases where no unbiased estimator exists for a parameter! Often, rather, we seek to balance bias and variance: recall that an estimator is a function of random variables and thus it is itself random: even if it is unbiased, the numerical value obtained will vary from one sample to the next.</p>
<div id="def-mse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4</strong></span> We often seek an estimator that minimises the <strong>mean squared error</strong>, <span class="math display">\[\begin{align*}
\mathsf{MSE}(\hat{\theta}) = \mathsf{E}\{(\hat{\theta}-\theta)^2\}=\mathsf{Va}(\hat{\theta}) + \{\mathsf{E}(\hat{\theta})\}^2.
\end{align*}\]</span> The mean squared error is an objective function consisting of the sum of the squared bias and the variance.</p>
</div>
<p>Most estimators we will considered are so-called maximum likelihood estimator. These estimator are asymptotically efficient, in the sense that they have the lowest mean squared error of all estimators for large samples. Other properties of maximum likelihood estimators also make them attractive default choice for estimation.</p>
</section>
<section id="discrete-distributions" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="discrete-distributions"><span class="header-section-number">1.3</span> Discrete distributions</h2>
<p>Many distributions for discrete random variables have a simple empirical justification, stemming from simple combinatorial arguments (counting). We revisit the most common ones.</p>
<div id="def-bernoullidist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 (Bernoulli distribution)</strong></span> We consider a binary event such as coin toss (heads/tails). In general, the two events are associated with success/failure. By convention, failures are denoted by zeros and successes by ones, the probability of success being <span class="math inline">\(p\)</span> so <span class="math inline">\(\mathsf{Pr}(Y=1)=p\)</span> and <span class="math inline">\(\mathsf{Pr}(Y=0)=1-p\)</span> (complementary event). The mass function of the <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> is thus <span class="math display">\[\begin{align*}
\mathsf{Pr}(Y=y) = p^y (1-p)^{1-y}, \quad y=0, 1.
\end{align*}\]</span></p>
</div>
<p>A rapid calculation shows that <span class="math inline">\(\mathsf{E}(Y)=p\)</span> and <span class="math inline">\(\mathsf{Va}(Y)=p(1-p).\)</span> Indeed, <span class="math display">\[\begin{align*}
\mathsf{E}(Y) = \mathsf{E}(Y^2) = p \cdot 1 + (1-p) \cdot 0 = p.
\end{align*}\]</span></p>
<p>Many research questions have binary responses, for example:</p>
<ul>
<li>did a potential client respond favourably to a promotional offer?</li>
<li>is the client satisfied with service provided post-purchase?</li>
<li>will a company go bankrupt in the next three years?</li>
<li>did a study participant successfully complete a task?</li>
</ul>
<p>Oftentimes, we will have access to aggregated data.</p>
<div id="def-binomialdist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.6 (Binomial distribution)</strong></span> If we consider the sum of independent and identically distributed Bernoulli events, the number of sucesses <span class="math inline">\(Y\)</span> out of <span class="math inline">\(m\)</span> trials is <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial</a>, denoted <span class="math inline">\(\mathsf{Bin}(m, p)\)</span>; the mass function of the binomial distribution is <span class="math display">\[\begin{align*}
\mathsf{Pr}(Y=y) = \binom{m}{y}p^y (1-p)^{1-y}, \quad y=0, 1.
\end{align*}\]</span> The likelihood of a sample from a binomial distribution is (up to a normalizing constant that doesn’t depend on <span class="math inline">\(p\)</span>) the same as that of <span class="math inline">\(m\)</span> independent Bernoulli trials. The expectation of the binomial random variable is <span class="math inline">\(\mathsf{E}(Y)=mp\)</span> and its variance <span class="math inline">\(\mathsf{Va}(Y)=mp(1-p).\)</span></p>
</div>
<p>As examples, we could consider the number of successful candidates out of <span class="math inline">\(m\)</span> who passed their driving license test or the number of customers out of <span class="math inline">\(m\)</span> total which spent more than 10$ in a store.</p>
<p>More generally, we can also consider count variables whose realizations are integer-valued, for examples the number of</p>
<ul>
<li>insurance claims made by a policyholder over a year,</li>
<li>purchases made by a client over a month on a website,</li>
<li>tasks completed by a study participant in a given time frame.</li>
</ul>
<!--

:::{#def-geomdist}

## Geometric distribution

The [geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) describes the number of Bernoulli trials with probability of success $p$ required to obtain a first success. The mass function of  $Y \sim \mathsf{Geo}(p)$ is
\begin{align*}
\mathsf{Pr}(Y=y) = p (1-p)^{y-1}, \quad y=1,2, \ldots
\end{align*}

:::

For example, we could model the numbers of visits for a house on sale before the first offer is made using a geometric distribution.

-->
<div id="def-poissondist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.7 (Poisson distribution)</strong></span> If the probability of success <span class="math inline">\(p\)</span> of a Bernoulli event is small in the sense that <span class="math inline">\(mp \to \lambda\)</span> when the number of trials <span class="math inline">\(m\)</span> increases, then the number of success follows approximately a Poisson distribution with mass function <span class="math display">\[\begin{align*}
\mathsf{Pr}(Y=y) = \frac{\exp(-\lambda)\lambda^y}{\Gamma(y+1)}, \quad y=0, 1, 2, \ldots
\end{align*}\]</span> where <span class="math inline">\(\Gamma(\cdot)\)</span> denotes the gamma function. The parameter <span class="math inline">\(\lambda\)</span> of the Poisson distribution is both the expectation and the variance of the distribution, meaning <span class="math inline">\(\mathsf{E}(Y)=\mathsf{Va}(Y)=\lambda.\)</span></p>
</div>
<div id="def-negbindist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.8 (Negative binomial distribution)</strong></span> The negative binomial distribution arises if we consider the number of Bernoulli trials with probability of success <span class="math inline">\(p\)</span> until we obtain <span class="math inline">\(m\)</span> success. Let <span class="math inline">\(Y\)</span> denote the number of failures: the order of success and failure doesn’t matter, except for the latest trial which must be a success. The mass function of the negative binomial is <span class="math display">\[\begin{align*}
\mathsf{Pr}(Y=y)= \binom{m-1+y}{y} p^m (1-p)^{y}.
\end{align*}\]</span></p>
<p>The negative binomial distribution also appears as the unconditional distribution of a two-stage hierarchical gamma-Poisson model, in which the mean of the Poisson distribution is random and follows a gamma distribution. In notation, this is <span class="math inline">\(Y \mid \Lambda=\lambda \sim \mathsf{Po}(\lambda)\)</span> and <span class="math inline">\(\Lambda\)</span> follows a gamma distribution with shape <span class="math inline">\(r\)</span> and scale <span class="math inline">\(\theta\)</span>, whose density is <span class="math display">\[\begin{align*}
f(x) = \theta^{-r}x^{r-1}\exp(-x/\theta)/\Gamma(r).
\end{align*}\]</span> The unconditional number of success is then negative binomial.</p>
<p>In the context of generalized linear models, we will employ yet another parametrisation of the distribution, with the mass function <span class="math display">\[\begin{align*}
\mathsf{Pr}(Y=y)=\frac{\Gamma(y+r)}{\Gamma(y+1)\Gamma(r)} \left(\frac{r}{r + \mu} \right)^{r} \left(\frac{\mu}{r+\mu}\right)^y, y=0, 1, \ldots, \mu,r &gt;0,
\end{align*}\]</span> where <span class="math inline">\(\Gamma\)</span> is the gamma function and the parameter <span class="math inline">\(r&gt;0\)</span> is not anymore integer valued. The expectation and variance of <span class="math inline">\(Y\)</span> are <span class="math inline">\(\mathsf{E}(Y)=\mu\)</span> et <span class="math inline">\(\mathsf{Va}(Y)=\mu+k\mu^2\)</span>, where <span class="math inline">\(k=1/r.\)</span> The variance of the negative binomial distribution is thus higher than its expectation, which justifies the use of the negative binomial distribution for modelling overdispersion.</p>
</div>
</section>
<section id="continuous-distributions" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="continuous-distributions"><span class="header-section-number">1.4</span> Continuous distributions</h2>
<p>We will encounter many continuous distributions that arise as (asymptotic) null distribution of test statistics because of the central limit theorem, or that follow from transformation of Gaussian random variables.</p>
<div id="def-loibeta" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.9 (Beta distribution)</strong></span> The beta distribution <span class="math inline">\(\mathsf{Beta}(\alpha, \beta)\)</span> is a distribution supported on the unit interval <span class="math inline">\([0,1]\)</span> with shape parameters <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\beta&gt;0.\)</span> It’s density is <span class="math display">\[\begin{align*}
f(x) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}x^{\alpha-1}(1-x)^{1-\beta}, \qquad x \in [0,1].
\end{align*}\]</span> The case <span class="math inline">\(\alpha=\beta=1\)</span>, also denoted <span class="math inline">\(\mathsf{unif}(0,1)\)</span>, corresponds to a standard uniform distribution.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-densite-beta" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-densite-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-densite-beta-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-densite-beta-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Density fonction of uniform (left) and beta(2, 3/4) random variables on the unit interval.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="def-exponentialdist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.10 (Exponential distribution)</strong></span> The exponential distribution plays a prominent role in the study of waiting time of Poisson processes, and in survival analysis. One caracteristic of the distribution is it’s absence of memory: <span class="math inline">\(\Pr(Y \geq y + u \mid Y &gt; u) = \Pr(Y &gt; u)\)</span> for <span class="math inline">\(y, u&gt;0.\)</span></p>
<p>The distribution function of the exponential distribution with scale <span class="math inline">\(\lambda&gt;0\)</span>, denoted <span class="math inline">\(Y \sim \mathsf{Exp}(\lambda)\)</span>, is <span class="math inline">\(F(x) = 1-\exp(-x/\lambda)\)</span> and the corresponding density function is <span class="math inline">\(f(x) =\lambda^{-1}\exp(-x/\lambda)\)</span> for <span class="math inline">\(x &gt;0.\)</span> The expected value of <span class="math inline">\(Y\)</span> is simply <span class="math inline">\(\lambda.\)</span></p>
</div>
<div id="def-normaldist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.11 (Normal distribution)</strong></span> Ths most well known distribution, the normal distribution is ubiquitous in statistics because of the central limit theorem (CLT), which describes the behaviour of the sample mean in large sample.The parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma&gt;0\)</span> that fully characterize the distribution of the normal distribution and they correspond to the expectation and standard deviation. The density of a normal distribution is symmetric around <span class="math inline">\(\mu\)</span>, while <span class="math inline">\(\sigma\)</span> describes the dispersion around this mode. The bell-shaped density function is <span class="math display">\[\begin{align*}
f(x) = (2\pi\sigma^2)^{-1/2} \exp \left\{ - \frac{(x-\mu)^2}{2\sigma^2}\right\}, \qquad x \in \mathbb{R}.
\end{align*}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-normal-loc-scale" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal-loc-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-normal-loc-scale-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-loc-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Densities of normal distributions with different mean parameters (left) and different scale parameters (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p>The distribution function of the normal distribution is not available in closed-form. The normal distribution is a location-scale distribution: if <span class="math inline">\(Y \sim \mathsf{normal}(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Z = (Y-\mu)/\sigma \sim \mathsf{normale}(0,1).\)</span> Conversely, if <span class="math inline">\(Z \sim \mathsf{normal}(0,1)\)</span>, then <span class="math inline">\(Y = \mu + \sigma Z \sim \mathsf{normal}(\mu, \sigma^2).\)</span></p>
<p>We will also encounter the multivariate normal distribution; for a <span class="math inline">\(d\)</span> dimensional vector <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{normal}_d(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, the density is <span class="math display">\[\begin{align*}
f(\boldsymbol{x}) = (2\pi)^{-d/2} |\boldsymbol{\Sigma}|^{-1/2} \exp \left\{ - \frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}
\end{align*}\]</span></p>
<p>The mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> is the vector of expectation of individual observations, whereas <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the <span class="math inline">\(d \times d\)</span> covariance matrix of <span class="math inline">\(\boldsymbol{Y}.\)</span> A unique property of the multivariate normal distribution is the link between independence and the covariance matrix: if <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> are independent, the <span class="math inline">\((i,j)\)</span> off-diagonal entry of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is zero.</p>
</div>
<div id="def-chisquare-dist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.12 (Chi-square distribution)</strong></span> The chi-square distribution with <span class="math inline">\(\nu&gt;0\)</span> degrees of freedom, denoted <span class="math inline">\(\chi^2_\nu\)</span> or <span class="math inline">\(\mathsf{chi-square}(\nu).\)</span> It’s density is <span class="math display">\[\begin{align*}
f(x; \nu) = \frac{1}{2^{\nu/2}\Gamma(\nu/2)}x^{\nu/2-1}\exp(-x/2),\qquad x &gt;0.
\end{align*}\]</span> It can be obtained for <span class="math inline">\(\nu\)</span> integer by considering the following: if we consider <span class="math inline">\(k\)</span> independent and identically distributed standard normal variables, <span class="math inline">\(Y_i \sim \mathsf{normal}(0, 1)\)</span>, then <span class="math inline">\(\sum_{i=1}^k Y_i^2\)</span> follows a chi-square distribution with <span class="math inline">\(k\)</span> degrees of freedom, denote <span class="math inline">\(\chi^2_k.\)</span> The square of a standard normal variate likewise follows a <span class="math inline">\(\chi^2_1\)</span> distribution. The expectation of <span class="math inline">\(\chi^2_k\)</span> random variable is <span class="math inline">\(k.\)</span></p>
</div>
<p>If we consider a sample of <span class="math inline">\(n\)</span> normally distributed observations, the scaled sample variance <span class="math inline">\((n-1)S^2/\sigma^2 \sim \chi^2_{n-1}.\)</span></p>
<div id="def-studentdist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.13 (Student-<span class="math inline">\(t\)</span> distribution)</strong></span> The Student-<span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\nu&gt;0\)</span> degrees of freedom is a location-scale family. The standard version is denoted by <span class="math inline">\(\mathsf{Student}(\nu).\)</span></p>
<p>The name “Student” comes from the pseudonym used by William Gosset in <span class="citation" data-cites="Student:1908">Gosset (<a href="references.html#ref-Student:1908" role="doc-biblioref">1908</a>)</span>, who introduced the asymptotic distribution of the <span class="math inline">\(t\)</span>-statistic. <!--If $X \sim \mathsf{normal}(0,1)$ independent of  $Y \sim \chi^{2}_\nu$, then the $t$-statistic
\begin{align*}
T = \frac{X}{\sqrt{Y/\nu}}
\end{align*}
follows a Student-$t$ distribution with $\nu$ degrees of freedom.
--> The density of the standard <span class="math inline">\(T\)</span> with <span class="math inline">\(\nu\)</span> degrees of freedom is <span class="math display">\[\begin{align*}
f(y; \nu) = \frac{\Gamma \left( \frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)
\sqrt{\nu\pi}}\left(1+\frac{y^{2}}{\nu}\right)^{-\frac{\nu+1}{2}}
\end{align*}\]</span> the distribution has polynomial tails, is symmetric around <span class="math inline">\(0\)</span> and unimodal. As <span class="math inline">\(\nu \to \infty\)</span>, the Student distribution converges to a normal distribution. It has heavier tails than the normal distribution and only the first <span class="math inline">\(\nu-1\)</span> moments of the distribution exist, so a Student distribution with <span class="math inline">\(\nu=2\)</span> degrees of freedom has infinite variance.</p>
<p>For normally distributed data, the centered sample mean divided by the sample variance, <span class="math inline">\((\overline{Y}-\mu)/S^2\)</span> follows a Student-<span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom, which explains the terminology <span class="math inline">\(t\)</span>-tests.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-student-density" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-student-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-student-density-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-student-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Comparison between the Student-<span class="math inline">\(t\)</span> density for varying degrees of freedom, with <span class="math inline">\(\nu=2\)</span> (dotted), <span class="math inline">\(\nu=10\)</span> (dashed) and the normal density (<span class="math inline">\(\nu = \infty).\)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="def-Fdist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.14 (Fisher distribution)</strong></span> The Fisher or <span class="math inline">\(F\)</span> distribution is used to determine the large sample behaviour of test statistics for comparing different group averages (in analysis of variance) assuming data are normally distributed.</p>
<p>The <span class="math inline">\(F\)</span> distribution, denoted <span class="math inline">\(\mathsf{Fisher}(\nu_1, \nu_2)\)</span>, is obtained by dividing two independent chi-square random variables with respective degrees of freedom <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2.\)</span> Specifically, if <span class="math inline">\(Y_1 \sim \chi^2_{\nu_1}\)</span> and <span class="math inline">\(Y_2 \sim \chi^2_{\nu_2}\)</span>, then <span class="math display">\[\begin{align*}
F = \frac{Y_1/\nu_1}{Y_2/\nu_2} \sim \mathsf{Fisher}(\nu_1, \nu_2)
\end{align*}\]</span></p>
<p>The Fisher distribution tends to a <span class="math inline">\(\chi^2_{\nu_1}\)</span> when <span class="math inline">\(\nu_2 \to \infty.\)</span></p>
</div>
</section>
<section id="graphs" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="graphs"><span class="header-section-number">1.5</span> Graphs</h2>
<p>This section reviews the main graphical representation of random variables, depending on their type.</p>
<p>The main type of graph for representing categorical variables is bar plot (and modifications thereof). In a bar plot, the frequency of each category is represented in the <span class="math inline">\(y\)</span>-axis as a function of the (ordered) levels on the <span class="math inline">\(x\)</span>-axis. This representation is superior to the <a href="http://www.perceptualedge.com/articles/08-21-07.pdf">ignominious pie chart</a>, a nuisance that ought to be banned (humans are very bad at comparing areas and a simple rotation changes the perception of the graph)!</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-barplotrenfe" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-barplotrenfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-barplotrenfe-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-barplotrenfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: Bar plot of ticket class for Renfe tickets data
</figcaption>
</figure>
</div>
</div>
</div>
<p>Continuous variables can take as many distinct values as there are observations, so we cannot simply count the number of occurences by unique values. Instead, we bin them into distinct intervals so as to obtain an histogram. The number of class depends on the number of observations: as a rule of thumb, the number of bins should not exceed <span class="math inline">\(\sqrt{n}\)</span>, where <span class="math inline">\(n\)</span> is the sample size. We can then obtain the frequency in each class, or else normalize the histogram so that the area under the bands equals one: this yields a discrete approximation of the underlying density function. Varying the number of bins can help us detect patterns (rounding, asymmetry, multimodality).</p>
<p>Since we bin observations together, it is sometimes difficult to see where they fall. Adding rugs below or above the histogram will add observation about the range and values taken, where the heights of the bars in the histogram carry information about the (relative) frequency of the intervals.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-histrenfe" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-histrenfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-histrenfe-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-histrenfe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Histogram of Promo tickets for Renfe ticket data
</figcaption>
</figure>
</div>
</div>
</div>
<p>If we have a lot of data, it sometimes help to focus only on selected summary statistics.</p>
<div id="def-boxplot" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.15 (Box-and-whiskers plot)</strong></span> A box-and-whiskers plot (or boxplot) represents five numbers</p>
<ul>
<li>The box gives the quartiles <span class="math inline">\(q_1, q_2, q_3\)</span> of the distribution. The middle bar <span class="math inline">\(q_2\)</span> is thus the median, so 50% of the observations are smaller or larger than this number.</li>
<li>The length of the whiskers is up to <span class="math inline">\(1.5\)</span> times the interquartiles range <span class="math inline">\(q_3-q_1\)</span> (the whiskers extend until the latest point in the interval, so the largest observation that is smaller than <span class="math inline">\(q_3+1.5(q_3-q_1)\)</span>, etc.)</li>
<li>Observations beyond the whiskers are represented by dots or circles, sometimes termed outliers. However, beware of this terminology: the larger the sample size, the more values will fall outside the whiskers. This is a drawback of boxplots, which was conceived at a time where the size of data sets was much smaller than what is current standards.</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-boxplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boxplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/01-intro-boxplot.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boxplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Box-and-whiskers plot
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>We can represent the distribution of a response variable as a function of a categorical variable by drawing a boxplot for each category and laying them side by side. A third variable, categorical, can be added via a color palette, as shown in <a href="#fig-histboxplot" class="quarto-xref">Figure&nbsp;<span>1.7</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-histboxplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-histboxplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-histboxplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-histboxplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: Box-and-whiskers plots for Promo fare tickets as a function of class and type for the Renfe tickets data.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Scatterplots are used to represent graphically the co-variation between two continuous variables: each tuple gives the coordinate of the point. If only a handful of large values are visible on the graph, a transformation may be useful: oftentimes, you will encounter graphs where the <span class="math inline">\(x\)</span>- or <span class="math inline">\(y\)</span>-axis is on the log-scale when the underlying variable is positive. If the number of data points is too large, it is hard to distinguish points because they are overlaid: adding transparency, or binning using a two-dimensional histogram with the frequency represented using color are potential solutions. The left panel of <a href="#fig-scatterplot" class="quarto-xref">Figure&nbsp;<span>1.8</span></a> shows the 100 simulated observations, whereas the right-panel shows a larger sample of 10 000 points using hexagonal binning, an analog of the bivariate density.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-scatterplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-scatterplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-scatterplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-scatterplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.8: Scatterplot (left) and hexagonal heatmap of bidimensional bin counts (right) of simulated data.
</figcaption>
</figure>
</div>
</div>
</div>
<!-- Sometimes, continuous data have a particular structure, mostly when observations are collected over space or time. Time series are ordered and the response should be plotted on the $y$-axis as a function of time (on the $x$-axis). It is customary to draw segments between observations, but this display is sometimes misleading. -->
<p>Models are (at best) an approximation of the true data generating mechanism and we will want to ensure that our assumptions are reasonable and the quality of the fit decent.</p>
<div id="def-qqplot" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.16 (Quantiles-quantiles plots)</strong></span> Quantile-quantile plots are graphical goodness-of-fit diagnostics that are based on the following principle: if <span class="math inline">\(Y\)</span> is a continuous random variable with distribution function <span class="math inline">\(F\)</span>, then the mapping <span class="math inline">\(F(Y) \sim \mathsf{unif}(0,1)\)</span> yields standard uniform variables. Similarly, the quantile transform applied to a uniform variable provides a mean to simulating samples from <span class="math inline">\(F\)</span>, viz.&nbsp;<span class="math inline">\(F^{-1}(U).\)</span> Consider then a random sample of size <span class="math inline">\(n\)</span> from the uniform distribution ordered from smallest to largest, with <span class="math inline">\(U_{(1)} \leq \cdots \leq U_{(n)}.\)</span> One can show these ranks have marginally a Beta distribution, <span class="math inline">\(U_{(k)} \sim \mathsf{beta}(k, n+1-k)\)</span> with expectation <span class="math inline">\(k/(n+1).\)</span></p>
<p>In practice, we don’t know <span class="math inline">\(F\)</span> and, even if we did, one would need to estimate the parameters. We consider some estimator <span class="math inline">\(\widehat{F}\)</span> for the model and apply the inverse transform to an approximate uniform sample <span class="math inline">\(\{i/(n+1)\}_{i=1}^n.\)</span> The quantile-quantile plot shows the data as a function of the (first moment) of the transformed order statistics:</p>
<ul>
<li>on the <span class="math inline">\(x\)</span>-axis, the theoretical quantiles <span class="math inline">\(\widehat{F}^{-1}\{\mathrm{rank}(y_i)/(n+1)\}\)</span></li>
<li>on the <span class="math inline">\(y\)</span>-axis, the empirical quantiles <span class="math inline">\(y_i\)</span></li>
</ul>
<p>If the model is adequate, the ordered values should follow a straight line with unit slope passing through the origin.</p>
</div>
<!--
Whether points fall on a 45 degree line is difficult to judge by eye and so it is advisable to ease the interpretation to subtract the slope. Tukey's mean difference plot takes the plotting positions of the quantile-quantile plot, say ($x_i, y_i$), and shows instead the mean and difference of these coordinates ($\{x_i + y_i\}/2, y_i - x_i$). @fig-diagrammeqq2 shows two representations of the same data using simulated samples from a standard normal distribution.

-->
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-diagrammeqq2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diagrammeqq2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-diagrammeqq2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagrammeqq2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.9: Probability-probability plot (left) on uniform margins, and ormal quantile-quantile plot (right) for the same dataset.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Even if we knew the true distribution of the data, the sample variability makes it very difficult to spot if deviations from the model are abnormal or compatible with the model. A simple point estimate with no uncertainty measure can lead to wrong conclusions. As such, we add approximate pointwise or simultaneous confidence intervals. The simplest way to do this is by simulation, by repeating the following steps <span class="math inline">\(B\)</span> times:</p>
<ol type="1">
<li>simulate a sample <span class="math inline">\(\{Y^{(b)}_{i}\} (i=1,\ldots, n)\)</span> from <span class="math inline">\(\widehat{F}\)</span></li>
<li>re-estimate the parameters of <span class="math inline">\(F\)</span> to obtain <span class="math inline">\(\widehat{F}_{(b)}\)</span></li>
<li>calculate and save the plotting positions <span class="math inline">\(\widehat{F}^{-1}_{(b)}\{i/(n+1)\}.\)</span></li>
</ol>
<p>The result of this operation is an <span class="math inline">\(n \times B\)</span> matrix of simulated data. We obtain a symmetric (<span class="math inline">\(1-\alpha\)</span>) confidence interval by keeping the empirical quantile of order <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> from each row. The number <span class="math inline">\(B\)</span> should be larger than 999, say, and be chosen so that <span class="math inline">\(B/\alpha\)</span> is an integer.</p>
<p>For the pointwise interval, each order statistic from the sample is a statistic and so the probability of any single one falling outside the confidence interval is approximately <span class="math inline">\(\alpha.\)</span> However, order statistics are not independent (they are ordered), so its common to see neighbouring points falling outside of their respective intervals. The intervals shown in <a href="#fig-diagrammeqq2" class="quarto-xref">Figure&nbsp;<span>1.9</span></a> are pointwise and derived (magically) using a simple function. The uniform order statistics have larger variability as we move away from 0.5, but the uncertainty in the quantile-quantile plot largely depends on <span class="math inline">\(F.\)</span></p>
<!--
[It is also possible to use the bootstrap samples to derive an (approximate) simultaneous confidence intervals, in which we expected values to fall $100(1-\alpha)$\% of the time inside the bands in repeated samples; [see Section 4.4.3 of these course notes](https://lbelzile.github.io/lineaRmodels/qqplot.html).
-->
<p>Interpretation of quantile-quantile plots requires practice and experience: <a href="https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot/101290#101290">this post by <em>Glen_b</em> on StackOverflow</a> nicely summarizes what can be detected (or not) from them.</p>
</section>
<section id="law-large-numbers" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="law-large-numbers"><span class="header-section-number">1.6</span> Laws of large numbers</h2>
<p>An estimator for a parameter <span class="math inline">\(\theta\)</span> is <strong>consistent</strong> if the value obtained as the sample size increases (to infinity) converges to the true value of <span class="math inline">\(\theta.\)</span> Mathematically speaking, this translates into convergence in probability, meaning <span class="math inline">\(\hat{\theta} \stackrel{\mathsf{Pr}}{\to} \theta.\)</span> In common language, we say that the probability that <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta\)</span> differ becomes negligible as <span class="math inline">\(n\)</span> gets large.</p>
<p>Consistency is the <em>a minima</em> requirement for an estimator: when we collect more information, we should approach the truth. The law of large number states that the sample mean of <span class="math inline">\(n\)</span> (independent) observations with common mean <span class="math inline">\(\mu\)</span>, say <span class="math inline">\(\overline{Y}_n\)</span>, converges to <span class="math inline">\(\mu\)</span>, denoted <span class="math inline">\(\overline{Y}_n \rightarrow \mu.\)</span> Roughly speaking, our approximation becomes less variable and asymptotically unbiased as the sample size (and thus the quantity of information available for the parameter) increases. The law of large number is featured in Monte Carlo experiments: we can approximate the expectation of some (complicated) function <span class="math inline">\(g(x)\)</span> by simulating repeatedly independent draws from <span class="math inline">\(Y\)</span> and calculating the sample mean <span class="math inline">\(n^{-1} \sum_{i=1}^n g(Y_i).\)</span></p>
<p>If the law of large number tells us what happens in the limit (we get a single numerical value), the result doesn’t contain information about the rate of convergence and the uncertainty at finite levels.</p>
</section>
<section id="CLT" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="CLT"><span class="header-section-number">1.7</span> Central Limit Theorem</h2>
<p>The central limit theorem gives the approximate large sample distribution of the sample mean. Consider a random sample of size <span class="math inline">\(n\)</span> <span class="math inline">\(\{Y_i\}_{i=1}^n\)</span> of independent random variables with common expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2.\)</span> The sample mean <span class="math inline">\(\overline{Y} = n^{-1}\sum_{i=1}^n Y_i\)</span> converges to <span class="math inline">\(\mu\)</span> by the law of large number, but we also have that</p>
<ul>
<li>the estimator <span class="math inline">\(\overline{Y}\)</span> is centered around <span class="math inline">\(\mu\)</span>,</li>
<li>the standard error is <span class="math inline">\(\sigma/\sqrt{n}\)</span>; the rate of convergence is thus <span class="math inline">\(\sqrt{n}.\)</span> For a sample of size 100, the standard error of the sample mean will be 10 times smaller than that of the underlying random variable.</li>
<li>the sample mean, once properly scaled, follows approximately a normal distribution</li>
</ul>
<p>Mathematically, the central limit theorem states <span class="math inline">\(\sqrt{n}(\overline{Y}-\mu) \stackrel{\mathrm{d}}{\rightarrow} \mathsf{normal}(0, \sigma^2).\)</span> If <span class="math inline">\(n\)</span> is large (a rule of thumb is <span class="math inline">\(n&gt;30\)</span>, but this depends on the underlying distribution of <span class="math inline">\(Y\)</span>), then <span class="math inline">\(\overline{Y} \stackrel{\cdot}{\sim} \mathsf{normal}(\mu, \sigma^2/n).\)</span></p>
<p>How do we make sense of this result? Let us consider the mean travel time of high speed Spanish trains (AVE) between Madrid and Barcelona that are operated by Renfe.</p>
<div class="cell" data-layout-align="center" data-fit.height="5">
<div class="cell-output-display">
<div id="fig-renfeclt" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-renfeclt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-renfeclt-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-renfeclt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.10: Empirical distribution of travel times of high speed trains.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Our exploratory data analysis showed previously that the duration is the one advertised on the ticket: there are only 15 unique travel time. Based on 9603 observations, we estimate the mean travel time to be 170 minutes and 41 seconds. <a href="#fig-renfeclt" class="quarto-xref">Figure&nbsp;<span>1.10</span></a> shows the empirical distribution of the data.</p>
<p>Consider now samples of size <span class="math inline">\(n=10\)</span>, drawn repeatedly from the population: in the first sample, the sample mean is 169.3 minutes, whereas we get an estimate of 167 minutes in our second , 157.9 minutes in the third, etc.</p>
<div class="cell" data-layout-align="center" data-fit.height="10">
<div class="cell-output-display">
<div id="fig-renfemeanCLT" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-renfemeanCLT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="introduction_files/figure-html/fig-renfemeanCLT-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-renfemeanCLT-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.11: Graphical representation of the central limit theorem. The upper left panel shows a sample of 20 observations with its sample mean (vertical red). The three other panels show the histograms of the sample mean from repeated samples of size 5 (top right), 20 (bottom left) and 20, 50 and 100 overlaid, with the density approximation provided by the central limit theorem.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We draw <span class="math inline">\(B=1000\)</span> different samples, each of size <span class="math inline">\(n=5\)</span>, from two millions records, and calculate the sample mean in each of them. The top right panel of <a href="#fig-renfemeanCLT" class="quarto-xref">Figure&nbsp;<span>1.11</span></a> is a histogram of the sample means when <span class="math inline">\(n=5\)</span>, whereas the bottom left panel shows the same thingfor <span class="math inline">\(n=20.\)</span> The last graph of <a href="#fig-renfemeanCLT" class="quarto-xref">Figure&nbsp;<span>1.11</span></a> shows the impact of the increase in sample size: whereas the normal approximation is okay-ish for <span class="math inline">\(n=5\)</span>, it is indistinguishable from the normal approximation for <span class="math inline">\(n=20.\)</span> As <span class="math inline">\(n\)</span> increases and the sample size gets bigger, the quality of the approximation improves and the curve becomes more concentrated around the true mean. Even if the distribution of the travel time is discrete, the mean is approximately normal.</p>
<p>We considered a single distribution in the example, but you could play with other distributions and vary the sample size to see when the central limit theorem kicks in usng this <a href="http://195.134.76.37/applets/AppletCentralLimit/Appl_CentralLimit2.html">applet</a>.</p>
<p>The central limit theorem underlies why scaled test statistics which have sample mean zero and sample variance 1 have a standard null distribution in large sample: this is what guarantees the validity of our inference!</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Student:1908" class="csl-entry" role="listitem">
Gosset, William Sealy. 1908. <span>“The Probable Error of a Mean.”</span> <em>Biometrika</em> 6 (1): 1–25. <a href="https://doi.org/10.1093/biomet/6.1.1">https://doi.org/10.1093/biomet/6.1.1</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/math60604a\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Welcome">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./inference.html" class="pagination-link" aria-label="Statistical inference">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical inference</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>All right reserved (Léo Belzile)</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/introduction.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>