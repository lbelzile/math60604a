<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a web complement for MATH 60604A Statistical Modelling, a master course offered at HEC Montréal.">

<title>4&nbsp; Linear regression models – MATH 60604A - Statistical Modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./likelihood.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<script src="site_libs/jquery-3.5.1/jquery.min.js"></script>
<link href="site_libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="css/style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linearmodels.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH 60604A - Statistical Modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math60604a/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH60604A.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihood-based inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linearmodels.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#programming" id="toc-programming" class="nav-link" data-scroll-target="#programming"><span class="header-section-number">4.1.1</span> Programming</a></li>
  <li><a href="#motivating-examples" id="toc-motivating-examples" class="nav-link" data-scroll-target="#motivating-examples"><span class="header-section-number">4.1.2</span> Motivating examples</a></li>
  </ul></li>
  <li><a href="#mean-model-specification" id="toc-mean-model-specification" class="nav-link" data-scroll-target="#mean-model-specification"><span class="header-section-number">4.2</span> Mean model specification</a>
  <ul class="collapse">
  <li><a href="#interpretation-of-regression-coefficients" id="toc-interpretation-of-regression-coefficients" class="nav-link" data-scroll-target="#interpretation-of-regression-coefficients"><span class="header-section-number">4.2.1</span> Interpretation of regression coefficients</a></li>
  <li><a href="#what-explanatories" id="toc-what-explanatories" class="nav-link" data-scroll-target="#what-explanatories"><span class="header-section-number">4.2.2</span> What explanatories?</a></li>
  <li><a href="#continuous-explanatories" id="toc-continuous-explanatories" class="nav-link" data-scroll-target="#continuous-explanatories"><span class="header-section-number">4.2.3</span> Continuous explanatories</a></li>
  <li><a href="#categorical-covariates" id="toc-categorical-covariates" class="nav-link" data-scroll-target="#categorical-covariates"><span class="header-section-number">4.2.4</span> Categorical covariates</a></li>
  </ul></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="header-section-number">4.3</span> Parameter estimation</a>
  <ul class="collapse">
  <li><a href="#ols" id="toc-ols" class="nav-link" data-scroll-target="#ols"><span class="header-section-number">4.3.1</span> Ordinary least squares estimator</a></li>
  <li><a href="#fitting-linear-models-with-software" id="toc-fitting-linear-models-with-software" class="nav-link" data-scroll-target="#fitting-linear-models-with-software"><span class="header-section-number">4.3.2</span> Fitting linear models with software</a></li>
  </ul></li>
  <li><a href="#coefR2" id="toc-coefR2" class="nav-link" data-scroll-target="#coefR2"><span class="header-section-number">4.4</span> Coefficient of determination</a></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">4.5</span> Concluding remarks</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/linearmodels.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="linmod" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>The linear regression model, or linear model, is one of the most versatile workshorse for statistical inference. Linear regression is used primarily to evaluate the effects of explanatories (oftentimes treatment in an experimental setting) on the mean response of the response, or for prediction. It combines a formulation for the mean of a <strong>response variable</strong> <span class="math inline">\(Y_i\)</span> of a random sample of size <span class="math inline">\(n\)</span> as a <strong>linear function</strong> of observed <strong>explanatories</strong> (also called predictors or covariates) <span class="math inline">\(X_1, \ldots, X_p\)</span>, <span class="math display">\[\begin{align}
\underset{\text{conditional mean}}{\mathsf{E}(Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i)}=\mu_i=\underset{\text{linear combination of explanatories}}{\beta_0 + \beta_1x_{i1} + \cdots + \beta_p x_{ip}}\equiv \mathbf{x}_i\boldsymbol{\beta}.
\end{align}\]</span> where <span class="math inline">\(\mathbf{x}_i = (1, x_{i1}, \ldots, x_{ip})\)</span> is a <span class="math inline">\((p+1)\)</span> row vector containing a constant and the explanatories of observation <span class="math inline">\(i\)</span>, and <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)^\top\)</span> is a <span class="math inline">\(p+1\)</span> column vector of coefficients for the mean. The model formulation is conditional on the values of the observed explanatories; this amounts to treating the <span class="math inline">\(p\)</span> explanatory variables <span class="math inline">\(X_1, \ldots, X_p\)</span> as non-random quantities, or known in advance. The regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> is the same for all observations, but the vector of explanatories <span class="math inline">\(\mathbf{x}_i\)</span> may change from one observation to the next. The model is <strong>linear</strong> in the coefficients <span class="math inline">\(\beta_0, \ldots, \beta_p\)</span>, and <span class="math inline">\(\beta_0\)</span> is the <strong>intercept</strong>.</p>
<p>For notational simplicity, we aggregate observations into an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\boldsymbol{Y}\)</span> and the explanatories into an <span class="math inline">\(n \times (p+1)\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> by concatenating a column of ones and the <span class="math inline">\(p\)</span> column vectors <span class="math inline">\(\boldsymbol{X}_1, \ldots, \boldsymbol{X}_p\)</span>, each containing the <span class="math inline">\(n\)</span> observations of the respective explanatories. The matrix <span class="math inline">\(\mathbf{X}\)</span> is termed <strong>model matrix</strong> (or sometimes design matrix in experimental settings), and it’s <span class="math inline">\(i\)</span>th row is <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>We suppose, in addition to the mean specification, that the response variables are independent and identically distributed, drawn from a mean-zero distribution with constant variance <span class="math inline">\(\sigma^2\)</span>. The variance term <span class="math inline">\(\sigma^2\)</span> is included to take into account the fact that no exact linear relationship links <span class="math inline">\(\boldsymbol{X}_i\)</span> and <span class="math inline">\(Y_i\)</span>, or that measurements of <span class="math inline">\(Y_i\)</span> are subject to error.</p>
<p>In the Gaussian linear model, responses follow a normal distribution and <span class="math inline">\(Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)\)</span>. The normal distribution is a location-scale family, so <span class="math inline">\(Y \sim \mathsf{normal}(\mu, \sigma^2)\)</span> is equal in distribution with <span class="math inline">\(\mu + \sigma Z\)</span> for <span class="math inline">\(Z \sim \mathsf{normal}(0, 1)\)</span>. We may thus rewrite the linear model in terms of the mean plus an error term, <span class="math display">\[\begin{align*}
\underset{\text{observation}\vphantom{\mu_i}}{Y_i} = \underset{\text{mean } \mu_i}{\vphantom{Y_i}\mathbf{x}_i\boldsymbol{\beta}} + \underset{\text{error term}\vphantom{\mu_i}}{\vphantom{Y_i}\varepsilon_i},
\end{align*}\]</span> where <span class="math inline">\(\varepsilon_i \sim \mathsf{normal}(0,\sigma^2)\)</span> is the error term specific to observation <span class="math inline">\(i\)</span>, and we assume that the errors <span class="math inline">\(\varepsilon_1, \ldots, \varepsilon_n\)</span> are independent and identically distributed. We fix the expectation or theoretical mean of <span class="math inline">\(\varepsilon_i\)</span> to zero to encode the fact we do not believe the model is systematically off, so <span class="math inline">\(\mathsf{E}(\varepsilon_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i)=0\)</span> <span class="math inline">\((i=1, \ldots, n)\)</span>.</p>
<section id="programming" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="programming"><span class="header-section-number">4.1.1</span> Programming</h3>
<p>The function <code>lm</code> is the workshorse for fitting linear models in <strong>R</strong>. It takes as input a formula: suppose you have a data frame containing columns <code>x1</code> and <code>x2</code> (explanatories) and <code>y</code> (response); you can then call <code>lm(y ~ x1 + x2)</code> to fit the linear model with conditional mean <span class="math inline">\(\beta_0 + \beta_1 x_1 + \beta_2x_2\)</span>. The explanatory variable <code>y</code> is on the left hand side of the equation, while the right hand side should contain the predictors, separated by a <code>+</code> sign if there are more than one. If you provide the data frame name using <code>data</code>, then the shorthand <code>y ~ .</code> (tilde followed by a period) fits a model including all of the columns of the data frame (but excluding <code>y</code>) as explanatories.</p>
<p>The type of effect included by the software depends on the class of the variable. Any numeric or integer variable will be treated as continuous, including dummies with 0/1 entries. To include categorical variables, these should be cast to factors (both for strings and for variables that are encoded using integers (sex, revenue class, level of education, marital status, etc.)</p>
</section>
<section id="motivating-examples" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="motivating-examples"><span class="header-section-number">4.1.2</span> Motivating examples</h3>
<p>We present some motivating examples that are discussed in the sequel.</p>
<div id="exm-lee-choi1" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 (Consistency of product description)</strong></span> Study 1 of <span class="citation" data-cites="Lee.Choi:2019">Lee and Choi (<a href="references.html#ref-Lee.Choi:2019" role="doc-biblioref">2019</a>)</span> considered descriptors and the impact on the perception of a product on the discrepancy between the text description and the image. In their first experience, a set of six toothbrushes is sold, but the image shows either a pack of six, or a single one). The authors also measured the prior familiarity with the brand of the item. Participants were recruited using an online panel, and the data in <code>LC19_S1</code> includes the results of the <span class="math inline">\(n=96\)</span> participants who passed the attention check (one additional participant response was outlying and removed). We could fit a linear model for the average product evaluation score, <code>prodeval</code>, as a function of the familiarity of the brand <code>familiarity</code>, an integer ranging from 1 to 7, and a dummy variable for the experimental factor <code>consistency</code>, coded <code>0</code> for consistent image/text descriptions and <code>1</code> if inconsistent. The resulting model matrix is then <span class="math inline">\(96 \times 3\)</span>. The <code>prodeval</code> response is heavily discretized, with only 19 unique values ranging between 2.33 and 9.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(LC19_S1, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear model using "lm"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The first argument is a formula of the form y ~ x1 + x2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># where y is the response and x's are explanatories, </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># separated by a plus (+) sign</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(prodeval <span class="sc">~</span> familiarity <span class="sc">+</span> consistency,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> LC19_S1)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the model matrix</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">model.matrix</span>(mod), <span class="at">n =</span> <span class="dv">5</span>L) <span class="co"># first five lines</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    (Intercept) familiarity consistencyinconsistent</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 92           1           6                       1</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 93           1           4                       1</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 94           1           7                       1</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 95           1           7                       1</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 96           1           7                       1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">model.matrix</span>(mod)) <span class="co"># dimension of the model matrix</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 96  3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="exm-college-salary-discrimination" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 (Gender discrimination in a US college)</strong></span> To make concepts and theoretical notions more concrete, we will use observational data collected in a college in the United States. The goal of the administration was to investigate potential gender inequality in the salary of faculty members. The data contains the following variables:</p>
<ul>
<li><code>salary</code>: nine-month salary of professors during the 2008–2009 academic year (in thousands USD).</li>
<li><code>rank</code>: academic rank of the professor (<code>assistant</code>, <code>associate</code> or <code>full</code>).</li>
<li><code>field</code>: categorical variable for the field of expertise of the professor, one of <code>applied</code> or <code>theoretical</code>.</li>
<li><code>sex</code>: binary indicator for sex, either <code>man</code> or <code>woman</code>.</li>
<li><code>service</code>: number of years of service in the college.</li>
<li><code>years</code>: number of years since PhD.</li>
</ul>
<p>Before drafting a model, it is useful to perform an exploratory data analysis. If salary increases with year, there is more heterogeneity in the salary of higher ranked professors: logically, assistant professors are either promoted or kicked out after at most 6 years according to the data. The limited number of years prevents large variability for their salaries.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="linearmodels_files/figure-html/edacollege-1.png" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Exploratory data analysis of <span class="math inline">\(\texttt{college}\)</span> data: salaries of professors as a function of the number of years of service and the academic ranking</figcaption>
</figure>
</div>
</div>
</div>
<p>Salary increases over years of service, but its variability also increases with rank. Note the much smaller number of women in the sample: this will impact our power to detect differences between sex. A contingency table of sex and academic rank can be useful to see if the proportion of women is the same in each rank: women represent 16% of assistant professors and 16% of associate profs, but only 7% of full professors and these are better paid on average.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Contingency table of the number of prof in the college by sex and academic rank.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">assistant</th>
<th style="text-align: right;">associate</th>
<th style="text-align: right;">full</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">man</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">248</td>
</tr>
<tr class="even">
<td style="text-align: left;">woman</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">18</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Some of the potential explanatory variables of the <code>college</code> data are categorical (<code>rank</code>, <code>sex</code>, <code>field</code>), the latter two being binary. The other three variables, <code>years</code> and <code>service</code>, are continuous and probably strongly correlated.</p>
</div>
<div id="exm-teaching-baumann" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 (Teaching to read and pre-post experiments)</strong></span> The <code>BSJ92</code> data in package <code>hecedsm</code> contains the results of an experimental study by <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> on the effectiveness of different reading strategies on understanding of children. These are described in the abstract</p>
<blockquote class="blockquote">
<p>Sixty-six fourth-grade students were randomly assigned to one of three experimental groups: (a) a Think-Aloud (TA) group, in which students were taught various comprehension monitoring strategies for reading stories (e.g., self-questioning, prediction, retelling, rereading) through the medium of thinking aloud; (b) a Directed Reading-Thinking Activity (DRTA) group, in which students were taught a predict-verify strategy for reading and responding to stories; or (c) a Directed Reading Activity (DRA) group, an instructed control, in which students engaged in a noninteractive, guided reading of stories.</p>
</blockquote>
<p>The data are balanced, as there are 22 observations in each of the three subgroups, of which <code>DR</code> is the control. The researchers applied a series of three tests (an error detection task for test 1, a comprehension monitoring questionnaire for test 2, and the <em>Degrees of Reading Power</em> cloze test labelled test 3). Tests 1 and 2 were administered both before and after the intervention: this gives us a change to establish the average <em>improvement</em> in student by adding <code>pretest1</code> as covariate for a regression of <code>posttest</code>, for example. The tests 1 were out of 16, but the one administered after the experiment was made more difficult to avoid cases of students getting near full scores. The correlation between pre-test and post-test 1 is <span class="math inline">\((\widehat{\rho}_1=0.57)\)</span>, much stronger than that for the second test <span class="math inline">\((\widehat{\rho}_2=0.21)\)</span>.</p>
</div>
</section>
</section>
<section id="mean-model-specification" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="mean-model-specification"><span class="header-section-number">4.2</span> Mean model specification</h2>
<p>This section covers the mean model specification, starting with parametrization of models with factors (i.e., categorical explanatories).</p>
<section id="interpretation-of-regression-coefficients" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="interpretation-of-regression-coefficients"><span class="header-section-number">4.2.1</span> Interpretation of regression coefficients</h3>
<p>We can assign meanings to the mean parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> in the linear model by considering what is</p>
</section>
<section id="what-explanatories" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="what-explanatories"><span class="header-section-number">4.2.2</span> What explanatories?</h3>
<p>The first step of an analysis is deciding which explanatory variables should be added to the mean model specification, and under what form. Models are but approximations of reality; Section 2.1 of <span class="citation" data-cites="Venables:2000">Venables (<a href="references.html#ref-Venables:2000" role="doc-biblioref">2000</a>)</span> argues that, if we believe the true mean function linking explanatories <span class="math inline">\(\boldsymbol{X}\)</span> and the response <span class="math inline">\(Y\)</span> is of the form <span class="math inline">\(\mathsf{E}(Y \mid \boldsymbol{X}) = f(\boldsymbol{X})\)</span> for <span class="math inline">\(f\)</span> sufficiently smooth, then the linear model is a first-order approximation. For interpretation purposes, it makes sense to mean-center any continuous explanatory, as this facilitates interpretation.</p>
<p>In an experimental setting, where the experimental group or condition is randomly allocated, we can directly compare the different treatments and draw causal conclusions (since all other things are constant, any detectable difference is due on average to our manipulation). Although we usually refrain from including any other explanatory to keep the design simple, it may be nevertheless helpful to consider some concomitant variables that explain part of the variability to filter background noise and increase power. For example, for the <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> data, our interest is in comparing the average scores as a function of the teaching method, we would include <code>group</code>. In this example, it would also make sense to include the <code>pretest1</code> result as an explanatory. This way, we will model the average difference in improvement from pre-test to post-test rather than the average score.</p>
<p>In an observational setting, people self-select in different groups, so we need to account for differences. Linear models in economics and finance often add control variables to the model to account for potential differences due to socio-demographic variables (age, revenue, etc.) that would be correlated to the group. Any test for coefficients would capture only correlation between the outcome <span class="math inline">\(Y\)</span> and the postulated explanatory factor of interest.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/correlation_causation.jpg" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Difference between experimental and observational studies by Andrew Heiss <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a></figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="continuous-explanatories" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="continuous-explanatories"><span class="header-section-number">4.2.3</span> Continuous explanatories</h3>
<p>Continuous explanatories are typically specified by including a single linear term, leading to the simple linear regression of the form <span class="math inline">\(Y \mid X=x \sim \mathsf{normal}(\beta_0 + \beta x, \sigma^2)\)</span>. In this situation <span class="math inline">\(\beta_0\)</span> is the intercept (the mean value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x=0\)</span>) and <span class="math inline">\(\beta_1\)</span> is the slope, i.e., the average increase of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x\)</span> increases by one unit. <a href="#fig-droitenuage" class="quarto-xref">Figure&nbsp;<span>4.1</span></a> shows such an example of a model with a single explanatory. As revealed by the exploratory data analysis of <a href="#exm-college-salary-discrimination" class="quarto-xref">Example&nbsp;<span>4.2</span></a>, this model is simplistic and clearly insufficient to explain differences in salary.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-droitenuage" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-droitenuage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-droitenuage-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-droitenuage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Simple linear regression model for the salary of professors as a function of the number of years of service.
</figcaption>
</figure>
</div>
</div>
</div>
<p>If the relationship between explanatory <span class="math inline">\(X\)</span> and response <span class="math inline">\(Y\)</span>, as assessed from a scatterplot, is not linear, we may consider more complicated function of the explanatories, as <a href="#exm-auto" class="quarto-xref">Example&nbsp;<span>4.4</span></a> shows.</p>
<div id="exm-auto" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4 (Quadratic curve fo the automobile data)</strong></span> We consider a linear regression model for the fuel autonomy of cars as a function of the power of their motor (measured in horsepower) from the <code>auto</code> dataset. The postulated model, <span class="math display">\[\begin{align*}
\texttt{mpg}_i = \beta_0 + \beta_1 \texttt{horsepower}_i + \beta_2 \texttt{horsepower}_i^2 + \varepsilon_i,
\end{align*}\]</span> includes a quadratic term. <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> shows the scatterplot with the fitted regression line, above which the line for the simple linear regression for horsepower is added.</p>
<p>To fit higher order polynomials, we use the <code>poly</code> as the latter leads to more numerical stability. For general transformations, the <code>I</code> function tells the software interpret the input “as is”. Thus, <code>lm(y~x+I(x^2))</code>, would fit a linear model with design matrix <span class="math inline">\([\boldsymbol{1}_n\, \mathbf{x}\, \mathbf{x}^2]\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-autoquad2d" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoquad2d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-autoquad2d-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoquad2d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Linear regression models for the fuel autonomy of cars as a function of motor power.
</figcaption>
</figure>
</div>
</div>
</div>
<p>It appears graphically that the quadratic model fits better than the simple linear alternative: we will assess this hypothesis formally later. For the degree two polynomial, <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> show that fuel autonomy decreases rapidly when power increases between 50 to 100, then more slow until 189.35 hp. After that, the model postulates that autonomy increases again as evidenced by the scatterplot, but beware of extrapolating (weird things can happen beyond the range of the data, as exemplified by <a href="https://web.archive.org/web/20210315050023/https://livefreeordichotomize.com/2020/05/05/model-detective/">Hassett’s cubic model for the number of daily cases of Covid19 in the USA</a>).</p>
<p>The representation in <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> may seem counter-intuitive given that we fit a linear model, but it is a 2D projection of 3D coordinates for the equation <span class="math inline">\(\beta_0 + \beta_1x-y +\beta_2z =0\)</span>, where <span class="math inline">\(x=\texttt{horsepower}\)</span>, <span class="math inline">\(z=\texttt{horsepower}^2\)</span> and <span class="math inline">\(y=\texttt{mpg}\)</span>. Physics and common sense force <span class="math inline">\(z = x^2\)</span>, and so the fitted values lie on a curve in a 2D subspace of the fitted plan, as shown in grey in the three-dimensional <a href="#fig-hyperplan" class="quarto-xref">Figure&nbsp;<span>4.3</span></a>.</p>
<div class="cell" data-layout-align="center">
<div id="fig-hyperplan" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hyperplan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-e9421466506cd568ba5c" style="width:85%;height:474.624px;"></div>
<script type="application/json" data-for="htmlwidget-e9421466506cd568ba5c">{"x":{"visdat":{"5606600905b0":["function () ","plotlyVisDat"],"56066478b43a":["function () ","data"]},"cur_data":"56066478b43a","attrs":{"5606600905b0":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004},"inherit":true},"5606600905b0.1":{"z":{},"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"inherit":false},"56066478b43a":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","color":["#003C71"],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"horsepower"},"yaxis":{"title":"square of horsepower"},"zaxis":{"title":"fuel autonomy (mpg)"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[130,165,150,150,140,198,220,215,225,190,170,160,150,225,95,95,97,85,88,46,87,90,95,113,90,215,200,210,193,88,90,95,100,105,100,88,100,165,175,153,150,180,170,175,110,72,100,88,86,90,70,76,65,69,60,70,95,80,54,90,86,165,175,150,153,150,208,155,160,190,97,150,130,140,150,112,76,87,69,86,92,97,80,88,175,150,145,137,150,198,150,158,150,215,225,175,105,100,100,88,95,46,150,167,170,180,100,88,72,94,90,85,107,90,145,230,49,75,91,112,150,110,122,180,95,100,100,67,80,65,75,100,110,105,140,150,150,140,150,83,67,78,52,61,75,75,75,97,93,67,95,105,72,72,170,145,150,148,110,105,110,95,110,110,129,75,83,100,78,96,71,97,97,70,90,95,88,98,115,53,86,81,92,79,83,140,150,120,152,100,105,81,90,52,60,70,53,100,78,110,95,71,70,75,72,102,150,88,108,120,180,145,130,150,68,80,58,96,70,145,110,145,130,110,105,100,98,180,170,190,149,78,88,75,89,63,83,67,78,97,110,110,48,66,52,70,60,110,140,139,105,95,85,88,100,90,105,85,110,120,145,165,139,140,68,95,97,75,95,105,85,97,103,125,115,133,71,68,115,85,88,90,110,130,129,138,135,155,142,125,150,71,65,80,80,77,125,71,90,70,70,65,69,90,115,115,90,76,60,70,65,90,88,90,90,78,90,75,92,75,65,105,65,48,48,67,67,67,67,62,132,100,88,72,84,84,92,110,84,58,64,60,67,65,62,68,63,65,65,74,75,75,100,74,80,76,116,120,110,105,88,85,88,88,88,85,84,90,92,74,68,68,63,70,88,75,70,67,67,67,110,85,92,112,96,84,90,86,52,84,79,82],"y":[16900,27225,22500,22500,19600,39204,48400,46225,50625,36100,28900,25600,22500,50625,9025,9025,9409,7225,7744,2116,7569,8100,9025,12769,8100,46225,40000,44100,37249,7744,8100,9025,10000,11025,10000,7744,10000,27225,30625,23409,22500,32400,28900,30625,12100,5184,10000,7744,7396,8100,4900,5776,4225,4761,3600,4900,9025,6400,2916,8100,7396,27225,30625,22500,23409,22500,43264,24025,25600,36100,9409,22500,16900,19600,22500,12544,5776,7569,4761,7396,8464,9409,6400,7744,30625,22500,21025,18769,22500,39204,22500,24964,22500,46225,50625,30625,11025,10000,10000,7744,9025,2116,22500,27889,28900,32400,10000,7744,5184,8836,8100,7225,11449,8100,21025,52900,2401,5625,8281,12544,22500,12100,14884,32400,9025,10000,10000,4489,6400,4225,5625,10000,12100,11025,19600,22500,22500,19600,22500,6889,4489,6084,2704,3721,5625,5625,5625,9409,8649,4489,9025,11025,5184,5184,28900,21025,22500,21904,12100,11025,12100,9025,12100,12100,16641,5625,6889,10000,6084,9216,5041,9409,9409,4900,8100,9025,7744,9604,13225,2809,7396,6561,8464,6241,6889,19600,22500,14400,23104,10000,11025,6561,8100,2704,3600,4900,2809,10000,6084,12100,9025,5041,4900,5625,5184,10404,22500,7744,11664,14400,32400,21025,16900,22500,4624,6400,3364,9216,4900,21025,12100,21025,16900,12100,11025,10000,9604,32400,28900,36100,22201,6084,7744,5625,7921,3969,6889,4489,6084,9409,12100,12100,2304,4356,2704,4900,3600,12100,19600,19321,11025,9025,7225,7744,10000,8100,11025,7225,12100,14400,21025,27225,19321,19600,4624,9025,9409,5625,9025,11025,7225,9409,10609,15625,13225,17689,5041,4624,13225,7225,7744,8100,12100,16900,16641,19044,18225,24025,20164,15625,22500,5041,4225,6400,6400,5929,15625,5041,8100,4900,4900,4225,4761,8100,13225,13225,8100,5776,3600,4900,4225,8100,7744,8100,8100,6084,8100,5625,8464,5625,4225,11025,4225,2304,2304,4489,4489,4489,4489,3844,17424,10000,7744,5184,7056,7056,8464,12100,7056,3364,4096,3600,4489,4225,3844,4624,3969,4225,4225,5476,5625,5625,10000,5476,6400,5776,13456,14400,12100,11025,7744,7225,7744,7744,7744,7225,7056,8100,8464,5476,4624,4624,3969,4900,7744,5625,4900,4489,4489,4489,12100,7225,8464,12544,9216,7056,8100,7396,2704,7056,6241,6724],"z":[18,15,18,16,17,15,14,14,14,15,15,14,15,14,24,22,18,21,27,26,25,24,25,26,21,10,10,11,9,27,28,25,19,16,17,19,18,14,14,14,14,12,13,13,18,22,19,18,23,28,30,30,31,35,27,26,24,25,23,20,21,13,14,15,14,17,11,13,12,13,19,15,13,13,14,18,22,21,26,22,28,23,28,27,13,14,13,14,15,12,13,13,14,13,12,13,18,16,18,18,23,26,11,12,13,12,18,20,21,22,18,19,21,26,15,16,29,24,20,19,15,24,20,11,20,19,15,31,26,32,25,16,16,18,16,13,14,14,14,29,26,26,31,32,28,24,26,24,26,31,19,18,15,15,16,15,16,14,17,16,15,18,21,20,13,29,23,20,23,24,25,24,18,29,19,23,23,22,25,33,28,25,25,26,27,17.5,16,15.5,14.5,22,22,24,22.5,29,24.5,29,33,20,18,18.5,17.5,29.5,32,28,26.5,20,13,19,19,16.5,16.5,13,13,13,31.5,30,36,25.5,33.5,17.5,17,15.5,15,17.5,20.5,19,18.5,16,15.5,15.5,16,29,24.5,26,25.5,30.5,33.5,30,30.5,22,21.5,21.5,43.100000000000001,36.100000000000001,32.799999999999997,39.399999999999999,36.100000000000001,19.899999999999999,19.399999999999999,20.199999999999999,19.199999999999999,20.5,20.199999999999999,25.100000000000001,20.5,19.399999999999999,20.600000000000001,20.800000000000001,18.600000000000001,18.100000000000001,19.199999999999999,17.699999999999999,18.100000000000001,17.5,30,27.5,27.199999999999999,30.899999999999999,21.100000000000001,23.199999999999999,23.800000000000001,23.899999999999999,20.300000000000001,17,21.600000000000001,16.199999999999999,31.5,29.5,21.5,19.800000000000001,22.300000000000001,20.199999999999999,20.600000000000001,17,17.600000000000001,16.5,18.199999999999999,16.899999999999999,15.5,19.199999999999999,18.5,31.899999999999999,34.100000000000001,35.700000000000003,27.399999999999999,25.399999999999999,23,27.199999999999999,23.899999999999999,34.200000000000003,34.5,31.800000000000001,37.299999999999997,28.399999999999999,28.800000000000001,26.800000000000001,33.5,41.5,38.100000000000001,32.100000000000001,37.200000000000003,28,26.399999999999999,24.300000000000001,19.100000000000001,34.299999999999997,29.800000000000001,31.300000000000001,37,32.200000000000003,46.600000000000001,27.899999999999999,40.799999999999997,44.299999999999997,43.399999999999999,36.399999999999999,30,44.600000000000001,33.799999999999997,29.800000000000001,32.700000000000003,23.699999999999999,35,32.399999999999999,27.199999999999999,26.600000000000001,25.800000000000001,23.5,30,39.100000000000001,39,35.100000000000001,32.299999999999997,37,37.700000000000003,34.100000000000001,34.700000000000003,34.399999999999999,29.899999999999999,33,33.700000000000003,32.399999999999999,32.899999999999999,31.600000000000001,28.100000000000001,30.699999999999999,25.399999999999999,24.199999999999999,22.399999999999999,26.600000000000001,20.199999999999999,17.600000000000001,28,27,34,31,29,27,24,36,37,31,38,36,36,36,34,38,32,38,25,38,26,22,32,36,27,27,44,32,28,31],"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004,"line":{"color":"rgba(31,119,180,1)"},"showscale":false},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"mpg<br />surf","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(190,190,190,1)"],["1","rgba(190,190,190,1)"]],"showscale":false,"z":[[38.059191113772322,-47.719700796540657],[100.5507364554749,14.771844545161926]],"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"frame":null},{"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250],"y":[0,1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900,961,1024,1089,1156,1225,1296,1369,1444,1521,1600,1681,1764,1849,1936,2025,2116,2209,2304,2401,2500,2601,2704,2809,2916,3025,3136,3249,3364,3481,3600,3721,3844,3969,4096,4225,4356,4489,4624,4761,4900,5041,5184,5329,5476,5625,5776,5929,6084,6241,6400,6561,6724,6889,7056,7225,7396,7569,7744,7921,8100,8281,8464,8649,8836,9025,9216,9409,9604,9801,10000,10201,10404,10609,10816,11025,11236,11449,11664,11881,12100,12321,12544,12769,12996,13225,13456,13689,13924,14161,14400,14641,14884,15129,15376,15625,15876,16129,16384,16641,16900,17161,17424,17689,17956,18225,18496,18769,19044,19321,19600,19881,20164,20449,20736,21025,21316,21609,21904,22201,22500,22801,23104,23409,23716,24025,24336,24649,24964,25281,25600,25921,26244,26569,26896,27225,27556,27889,28224,28561,28900,29241,29584,29929,30276,30625,30976,31329,31684,32041,32400,32761,33124,33489,33856,34225,34596,34969,35344,35721,36100,36481,36864,37249,37636,38025,38416,38809,39204,39601,40000,40401,40804,41209,41616,42025,42436,42849,43264,43681,44100,44521,44944,45369,45796,46225,46656,47089,47524,47961,48400,48841,49284,49729,50176,50625,51076,51529,51984,52441,52900,53361,53824,54289,54756,55225,55696,56169,56644,57121,57600,58081,58564,59049,59536,60025,60516,61009,61504,62001,62500],"z":[56.900099702112968,56.435140608266387,55.972642586621355,55.51260563717787,55.055029759935941,54.599914954895553,54.147261222056706,53.697068561419421,53.249336972983677,52.804066456749474,52.361257012716827,51.920908640885727,51.483021341256169,51.047595113828166,50.614629958601711,50.184125875576804,49.756082864753438,49.330500926131627,48.907380059711357,48.486720265492636,48.06852154347547,47.652783893659851,47.239507316045774,46.828691810633245,46.420337377422264,46.014444016412838,45.611011727604946,45.210040510998617,44.811530366593828,44.415481294390588,44.021893294388896,43.630766366588752,43.242100510990156,42.855895727593108,42.472152016397608,42.090869377403656,41.712047810611253,41.335687316020397,40.961787893631083,40.590349543443317,40.221372265457106,39.854856059672443,39.490800926089321,39.129206864707754,38.770073875527736,38.413401958549258,38.059191113772322,37.707441341196947,37.358152640823114,37.011325012650836,36.666958456680106,36.325052972910918,35.98560856134327,35.648625221977177,35.31410295481264,34.982041759849636,34.652441637088195,34.325302586528288,34.000624608169936,33.678407702013132,33.358651868057876,33.041357106304169,32.726523416752002,32.414150799401391,32.104239254252327,31.796788781304805,31.491799380558838,31.189271052014412,30.889203795671541,30.591597611530215,30.296452499590437,30.0037684598522,29.713545492315518,29.425783596980388,29.140482773846802,28.857643022914758,28.577264344184265,28.299346737655327,28.023890203327923,27.750894741202078,27.480360351277781,27.212287033555029,26.946674788033818,26.683523614714161,26.422833513596057,26.16460448467949,25.908836527964478,25.655529643451015,25.404683831139099,25.156299091028725,24.910375423119902,24.666912827412631,24.425911303906901,24.187370852602726,23.951291473500095,23.717673166599013,23.486515931899476,23.25781976940149,23.031584679105052,22.807810661010155,22.586497715116813,22.36764584142502,22.151255039934775,21.937325310646067,21.725856653558917,21.516849068673316,21.310302555989253,21.106217115506745,20.904592747225784,20.705429451146369,20.508727227268505,20.314486075592189,20.122705996117421,19.933386988844195,19.746529053772523,19.5621321909024,19.380196400233814,19.200721681766787,19.023708035501304,18.849155461437373,18.67706395957498,18.507433529914142,18.340264172454855,18.175555887197106,18.013308674140912,17.853522533286267,17.696197464633169,17.541333468181609,17.388930543931608,17.238988691883154,17.091507912036239,16.946488204390882,16.803929568947069,16.663832005704805,16.526195514664082,16.391020095824917,16.258305749187297,16.128052474751218,16.000260272516694,15.874929142483715,15.752059084652288,15.631650099022409,15.51370218559406,15.398215344367276,15.285189575342038,15.174624878518348,15.066521253896209,14.960878701475615,14.857697221256569,14.756976813239056,14.658717477423107,14.562919213808705,14.469582022395848,14.378705903184542,14.290290856174785,14.204336881366572,14.120843978759897,14.03981214835478,13.961241390151212,13.885131704149195,13.811483090348723,13.740295548749799,13.671569079352423,13.605303682156581,13.541499357162301,13.480156104369563,13.421273923778379,13.364852815388744,13.310892779200657,13.259393815214111,13.210355923429105,13.163779103845663,13.119663356463761,13.078008681283414,13.038815078304609,13.002082547527358,12.967811088951649,12.936000702577481,12.906651388404867,12.879763146433802,12.855335976664286,12.833369879096324,12.813864853729903,12.796820900565031,12.782238019601692,12.770116210839916,12.760455474279688,12.753255809921008,12.748517217763876,12.746239697808292,12.746423250054256,12.749067874501748,12.754173571150808,12.761740340001417,12.771768181053574,12.784257094307272,12.799207079762525,12.816618137419319,12.836490267277654,12.858823469337544,12.88361774359899,12.910873090061976,12.940589508726518,12.972766999592601,13.007405562660232,13.044505197929396,13.084065905400131,13.126087685072406,13.170570536946229,13.217514461021601,13.266919457298521,13.318785525776988,13.37311266645699,13.429900879338554,13.489150164421666,13.550860521706326,13.615031951192528,13.681664452880284,13.750758026769574,13.822312672860427,13.896328391152821,13.97280518164677,14.05174304434226,14.133141979239305,14.217001986337891,14.303323065638018,14.392105217139701,14.483348440842931,14.577052736747717,14.673218104854044,14.771844545161926,14.872932057671349,14.976480642382299,15.082490299294818,15.190961028408893,15.301892829724508,15.415285703241665,15.531139648960377,15.649454666880644,15.770230757002437,15.893467919325786,16.019166153850691,16.147325460577136,16.277945839505136,16.411027290634692,16.546569813965775,16.684573409498412,16.825038077232591,16.967963817168339,17.113350629305614,17.261198513644459],"type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,60,113,1)","line":{"color":"rgba(0,60,113,1)"},"showscale":false},"textfont":{"color":"rgba(0,60,113,1)"},"error_y":{"color":"rgba(0,60,113,1)"},"error_x":{"color":"rgba(0,60,113,1)"},"line":{"color":"rgba(0,60,113,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly",".hideLegend":true},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hyperplan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: 3D graphical representation of the linear regression model for the <code>auto</code> data.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="rem-discretization" class="proof remark">
<p><span class="proof-title"><em>Remark 4.1</em> (Discretization of continuous covariates). </span>Another option is to transform a continuous variable <span class="math inline">\(X\)</span> into a categorical variable by discretizing into bins and fitting a piecewise-linear function of <span class="math inline">\(X\)</span>. The prime example of such option is treating a Likert scale as a categorical variable. While this allows one to fit more flexible functional relations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, this comes at the cost of additional coefficients for the same estimation budget (fewer observations to estimate the effect of <span class="math inline">\(X\)</span> results in lower precision of the coefficients).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-auto-discre" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-auto-discre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-auto-discre-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-auto-discre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Piecewise-linear model for the fuel autonomy of cars as a function of motor power.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="categorical-covariates" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="categorical-covariates"><span class="header-section-number">4.2.4</span> Categorical covariates</h3>
<p>Dummies are variables (columns of explanatories from the model matrix) which only include <span class="math inline">\(-1\)</span>, <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> to give indicator of the level of groups. For a binary outcome, we can create a column that has entries <span class="math inline">\(1\)</span> for the treatment and <span class="math inline">\(0\)</span> for the control group.</p>
<div id="exm-moon" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5 (Linear models with a single binary variable)</strong></span> <span class="citation" data-cites="Moon.VanEpps:2023">Moon and VanEpps (<a href="references.html#ref-Moon.VanEpps:2023" role="doc-biblioref">2023</a>)</span> consider the impact of providing suggested amounts for donations to a charity (as opposed to an open-ended request). In Study 1, participants were given the chance of winning 25$ and giving part of this amount to charity.</p>
<p>Consider for example a linear model that includes the <code>amount</code> (in dollars, from 0 for people who did not donate, up to 25 dollars). We fit a model as a function of <span class="math display">\[\begin{align*}\texttt{condition} = \begin{cases} 0 , &amp; \text{open-ended},\\
1, &amp; \text{suggested quantity}
\end{cases}
\end{align*}\]</span> The equation of the simple linear model that includes the binary variable <code>condition</code> is <span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{amount} \mid \texttt{condition})&amp;= \beta_0 + \beta_1 \mathbf{1}_{\texttt{condition}=\texttt{quantity}}.
\\&amp;= \begin{cases}
\beta_0, &amp; \texttt{condition}=0, \\
\beta_0 + \beta_1 &amp; \texttt{condition}=1.
\end{cases}
\end{align*}\]</span> Let <span class="math inline">\(\mu_0\)</span> denote the theoretical average amount for the open-ended amount and <span class="math inline">\(\mu_1\)</span> that of participants of the treatment <code>quantity</code> group. We can write the equation for the conditional expectation for each experimental <code>condition</code></p>
<p>A linear model that only contains a binary variable <span class="math inline">\(\mathrm{X}\)</span> as regressor amounts to specifying a different mean for each of two groups: the average of the treatment group is <span class="math inline">\(\beta_0 + \beta_1 = \mu_1\)</span> and <span class="math inline">\(\beta_1=\mu_1-\mu_0\)</span> represents the difference between the average donation amount of people given <code>open-ended</code> amounts and those who are offered suggested amounts (<code>quantity</code>), including zeros for the amount of people who did not donate. The parametrization of the linear model with <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is in terms of pairwise differences relative to the baseline category and is particularly useful if we want to test for mean difference between the groups, as this amounts to testing <span class="math inline">\(\mathscr{H}_0: \beta_1=0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-donation-moon" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-donation-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-donation-moon-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-donation-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Simple linear model for the <code>MV23_S1</code> data using the binary variable <code>condition</code> as explanatory even if the equation defines a line, only its values in <span class="math inline">\(0/1\)</span> are realistic.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Even if the linear model defines a line, the latter is only meaningful when evaluated at <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>; <a href="#fig-donation-moon" class="quarto-xref">Figure&nbsp;<span>4.5</span></a> shows it in addition to sample observations (jittered horizontally) and a density estimate for each condition. The colored dot represents the mean, which will coincide with the estimates.</p>
<p>It is clear that the data are heavily discretized, with lots of ties and zeros. However, given the sample size of 869 observations, we can easily draw conclusions in each group.</p>
</div>
<p>Let us consider categorical variables with <span class="math inline">\(K &gt; 2\)</span> levels, which in <strong>R</strong> are of class <code>factor</code>. The default parametrization for factors are in terms of treatment contrast: the reference level of the factor (by default, the first value in alphanumerical order) will be treated as the reference category and assimilated to the intercept. The software will then create a set of <span class="math inline">\(K-1\)</span> dummy variables for a factor with <span class="math inline">\(K\)</span> levels, each of which will have ones for the relevant value and zero otherwise.</p>
<div id="exm-baumann-dummies" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6 (Dummy coding for categorical variables)</strong></span> Consider the <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> study and the sole inclusion of the <code>group</code> variable. The data are ordered by group: the first 22 observations are for group <code>DR</code>, the 22 next ones for group <code>DRTA</code> and the last 22 for <code>TA</code>. If we fit a model with <code>group</code> as categorical variables</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(BSJ92<span class="sc">$</span>group) <span class="co"># Check that group is a factor</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] "factor"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(BSJ92<span class="sc">$</span>group) <span class="co"># First level shown is reference</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] "DR"   "DRTA" "TA"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print part of the model matrix </span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (three individuals from different groups)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">model.matrix</span>(<span class="sc">~</span> group, <span class="at">data =</span> BSJ92)[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">23</span>,<span class="dv">47</span>),]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    (Intercept) groupDRTA groupTA</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1            1         0       0</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 23           1         1       0</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 47           1         0       1</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with levels of factors recorded</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>BSJ92<span class="sc">$</span>group[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">23</span>,<span class="dv">47</span>)]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] DR   DRTA TA  </span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Levels: DR DRTA TA</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The mean model specification is <span class="math display">\[\mathsf{E}(Y \mid \texttt{group})= \beta_0 + \beta_1\mathbf{1}_{\texttt{group}=\texttt{DRTA}} + \beta_2\mathbf{1}_{\texttt{group}=\texttt{TA}}.\]</span> Since the variable <code>group</code> is categorical with <span class="math inline">\(K=3\)</span> levels, we need <span class="math inline">\(K-1 = 2\)</span> dummy explanatories to include the effect and obtain one average per group. With the default parametrization, we obtain</p>
<ul>
<li><span class="math inline">\(\mathbf{1}_{\texttt{group}=\texttt{DRTA}}=1\)</span> if <code>group=DRTA</code> and zero otherwise.</li>
<li><span class="math inline">\(\mathbf{1}_{\texttt{group}=\texttt{TA}}=1\)</span> if <code>group=TA</code> and zero otherwise.</li>
</ul>
<p>Because the model includes an intercept and the model ultimately describes three group averages, we only need two additional variables. With the treatment parametrization, the group mean of the reference group equals the intercept coefficient, <span class="math inline">\(\mu_{\texttt{DR}}=\beta_0\)</span>,</p>
<div class="cell" data-layout-align="center">
<div id="tbl-dummies-tr" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dummies-tr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: Parametrization of dummies for a categorical variable with the default treatment contrasts.
</figcaption>
<div aria-describedby="tbl-dummies-tr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">groupDRTA</th>
<th style="text-align: right;">groupTA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DR</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">DRTA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>When <code>group</code>=<code>DR</code> (baseline), both indicator variables <code>groupDRTA</code> and <code>groupTA</code> are zero. The average in each group is <span class="math inline">\(\mu_{\texttt{DR}} = \beta_0\)</span>, <span class="math inline">\(\mu_{\texttt{DRTA}}=\beta_0 + \beta_1\)</span> and <span class="math inline">\(\mu_{\texttt{TA}} = \beta_0 + \beta_2\)</span>. We thus find that <span class="math inline">\(\beta_1\)</span> is the difference in mean between group <code>DRTA</code> and group <code>DR</code>, and similarly <span class="math inline">\(\beta_2=\mu_{\texttt{TA}}- \mu_{\texttt{DR}}\)</span>.</p>
</div>
<div id="rem-sumtozero" class="proof remark">
<p><span class="proof-title"><em>Remark 4.2</em> (Sum-to-zero constraints). </span>The parametrization discussed above, which is the default for the <code>lm</code> function, isn’t the only one available. We consider an alternative ones: rather than comparing each group mean with that of a baseline category, the default parametrization for analysis of variance models is in terms of sum-to-zero constraints, whereby the intercept is the equiweighted average of every group, and the parameters <span class="math inline">\(\beta_1, \ldots, \beta_{K-1}\)</span> are differences to this average.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">model.matrix</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span> group, </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> BSJ92, </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">contrasts.arg =</span> <span class="fu">list</span>(<span class="at">group =</span> <span class="st">"contr.sum"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-sum2zero" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sum2zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.2: Parametrization of dummies for the sum-to-zero constraints for a categorical variable.
</figcaption>
<div aria-describedby="tbl-sum2zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">group1</th>
<th style="text-align: right;">group2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DR</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">DRTA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-1</td>
<td style="text-align: right;">-1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>In the sum-to-zero constraint, we again only get two dummy variables, labelled <code>group1</code> and <code>group2</code>, along with the intercept. The value of <code>group1</code> is <span class="math inline">\(1\)</span> if <code>group=DR</code>, <span class="math inline">\(0\)</span> if <code>group=DRTA</code> and <span class="math inline">\(-1\)</span> if <code>group=TA</code>. Using the invariance property, we find <span class="math inline">\(\mu_{\texttt{DR}} = \beta_0 + \beta_1\)</span>, <span class="math inline">\(\mu_{\texttt{DRTA}}=\beta_0 + \beta_2 \beta_1\)</span> and <span class="math inline">\(\mu_{\texttt{TA}} = \beta_0 - \beta_1 - \beta_2\)</span> (more generally, the intercept minus the sum of all the other mean coefficients). Some algebraic manipulation reveals that <span class="math inline">\(\beta_0 = (\mu_{\texttt{DR}} +\mu_{\texttt{DRTA}}+\mu_{\texttt{TA}})/3\)</span>.</p>
<p>If we removed the intercept, then we could include three dummies for each treatment group and each parameter would correspond to the average. This isn’t recommended in <strong>R</strong> because the software treats models without the intercept differently and some output will be nonsensical (e.g., the coefficient of determination will be wrong).</p>
</div>
</section>
</section>
<section id="parameter-estimation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">4.3</span> Parameter estimation</h2>
<p>The linear model includes <span class="math inline">\(p+1\)</span> mean parameters and a standard deviation <span class="math inline">\(\sigma\)</span>, which is assumed constant for all observations.</p>
<section id="ols" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="ols"><span class="header-section-number">4.3.1</span> Ordinary least squares estimator</h3>
<p>Given a design or model matrix <span class="math inline">\(\mathbf{X}\)</span> and a linear model formulation <span class="math inline">\(\mathsf{E}(Y_i) = \mathbf{x}_i\boldsymbol{\beta}\)</span>, we can try to find the parameter vector <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^{p+1}\)</span> that minimizes the mean squared error, i.e., the average squared vertical distance between the fitted values <span class="math inline">\(\widehat{y}_i=\mathbf{x}_i\widehat{\boldsymbol{\beta}}\)</span> and the observations <span class="math inline">\(y_i\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-vertdist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vertdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-vertdist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vertdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Ordinary residuals <span class="math inline">\(e_i\)</span> (vertical vectors) added to the regression line in the scatter <span class="math inline">\((x, y)\)</span> (left) and the fit of response <span class="math inline">\(y_i\)</span> against fitted values <span class="math inline">\(\\widehat{y}_i\)</span>. The ordinary least squares line minimizes the average squared length of the ordinary residuals.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="prp-ols-mle" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1 (Ordinary least squares)</strong></span> Consider the optimization problem <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}&amp;=\mathrm{arg min}_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (y_i-\mathbf{x}_i\boldsymbol{\beta})^2
\\&amp;=(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span> We can compute the derivative of the right hand side with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, set it to zero and solve for <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, <span class="math display">\[\begin{align*}
\mathbf{0}_n&amp;=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&amp;=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
\\&amp;=2\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span> using the <a href="http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf">chain rule</a>. Distributing the terms leads to the so-called <em>normal equation</em> <span class="math display">\[\begin{align*}
\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&amp;=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}\]</span> If the <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is full-rank, meaning that it’s columns are not linear combinations of one another, the quadratic form <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible and we obtain the solution to the least square problems, <span id="eq-ols"><span class="math display">\[
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1}\mathbf{X}^\top \boldsymbol{y}.
\tag{4.1}\]</span></span> This is the <strong>ordinary least squares estimator</strong> (OLS). The explicit solution means that no numerical optimization is needed for linear models.</p>
</div>
<p>We could also consider maximum likelihood estimation. <a href="#prp-ols-mle" class="quarto-xref">Proposition&nbsp;<span>4.1</span></a> shows that, assuming normality of the errors, the least square estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> coincide with the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<div id="prp-mle-normal-linmod" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.2 (Maximum likelihood estimation of the normal linear model)</strong></span> The linear regression model specifies that the observations <span class="math inline">\(Y_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)\)</span> are independent. The linear model has <span class="math inline">\(p+2\)</span> parameters (<span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>) and the log likelihood is, abstracting from constant terms, <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\beta}, \sigma)&amp;\propto-\frac{n}{2} \ln (\sigma^2) -\frac{1}{2\sigma^2}\left\{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\right\}^2.
\end{align*}\]</span> Maximizing the log likelihood with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> is equivalent to minimizing the sum of squared errors <span class="math inline">\(\sum_{i=1}^n (y_i - \mathbf{x}_i\boldsymbol{\beta})^2\)</span>, regardless of the value of <span class="math inline">\(\sigma\)</span>, and we recover the OLS estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>. The maximum likelihood estimator of the variance <span class="math inline">\(\widehat{\sigma}^2\)</span> is thus <span class="math display">\[\begin{align*}
\widehat{\sigma}^2=\mathrm{arg max}_{\sigma^2} \ell(\widehat{\boldsymbol{\beta}}, \sigma^2).
\end{align*}\]</span> The profile log likelihood for <span class="math inline">\(\sigma^2\)</span>, excluding constant terms that don’t depend on <span class="math inline">\(\sigma^2\)</span>, is <span class="math display">\[\begin{align*}
\ell_{\mathrm{p}}(\sigma^2)
&amp;\propto-\frac{1}{2}\left\{n\ln\sigma^2+\frac{1}{\sigma^2}(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})\right\}.
\end{align*}\]</span> Differentiating each term with respect to <span class="math inline">\(\sigma^2\)</span> and setting the gradient equal to zero yields the maximum likelihood estimator <span class="math display">\[\begin{align*}
\widehat{\sigma}^2&amp;=\frac{1}{n}(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})\\&amp;= \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i\widehat{\boldsymbol{\beta}})^2\\&amp;= \frac{\mathsf{SS}_e}{n};
\end{align*}\]</span> where <span class="math inline">\(\mathsf{SS}_e\)</span> is the sum of squared residuals. The usual unbiased estimator of <span class="math inline">\(\sigma^2\)</span> calculated by software is <span class="math inline">\(S^2=\mathsf{SS}_e/(n-p-1)\)</span>, where the denominator is the sample size <span class="math inline">\(n\)</span> minus the number of mean parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(p+1\)</span>.</p>
</div>
<div id="rem-invariance" class="proof remark">
<p><span class="proof-title"><em>Remark 4.3</em> (Invariance). </span>One direct consequence of likelihood estimation is that the fitted values <span class="math inline">\(\widehat{y}_i\)</span> for two model matrices <span class="math inline">\(\mathbf{X}_a\)</span> and <span class="math inline">\(\mathbf{X}_b\)</span>, are the same if they generate the same linear span, as in <a href="#exm-baumann-dummies" class="quarto-xref">Example&nbsp;<span>4.6</span></a>. The interpretation of the coefficients will however change. If we include an intercept term, then we get the same output if the columns of explanatory are mean-centered.</p>
</div>
<p>The value of <span class="math inline">\(\boldsymbol{\beta}\)</span> is such that it will maximize the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\widehat{Y}\)</span>. In the case of a single categorical variable, we will obtain fitted values <span class="math inline">\(\widehat{y}\)</span> that correspond to the sample mean of each group.</p>
<div id="rem-geometry" class="proof remark">
<p><span class="proof-title"><em>Remark 4.4</em> (Geometry). </span>The vector of fitted values <span class="math inline">\(\boldsymbol{y} =\mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{H}_{\mathbf{X}}\boldsymbol{y}\)</span> is the projection of the response vector <span class="math inline">\(\boldsymbol{y}\)</span> on the linear span generated by the columns of <span class="math inline">\(\mathbf{X}\)</span>. The matrix <span class="math inline">\(\mathbf{H}_{\mathbf{X}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>, often called hat matrix, is an orthogonal projection matrix, so <span class="math inline">\(\mathbf{H}_{\mathbf{X}}=\mathbf{H}_{\mathbf{X}}^\top\)</span> and <span class="math inline">\(\mathbf{H}_{\mathbf{X}}\mathbf{H}_{\mathbf{X}} = \mathbf{H}_{\mathbf{X}}\)</span> and <span class="math inline">\(\mathbf{H}_{\mathbf{X}}\mathbf{X} = \mathbf{X}\)</span>. Since the vector of residuals <span class="math inline">\(\boldsymbol{e} = (e_1, \ldots, e_n)^\top\)</span>, which appear in the sum of squared errors, is defined as <span class="math inline">\(\boldsymbol{y} - \widehat{\boldsymbol{y}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}=\mathbf{X}\boldsymbol{\beta}\)</span>, simple algebraic manipulations show that the inner product between ordinary residuals and fitted values is zero, since <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{y}}^\top\boldsymbol{e} &amp;= \widehat{\boldsymbol{\beta}}^\top \mathbf{X}^\top (\boldsymbol{y}- \mathbf{X} \widehat{\boldsymbol{\beta}})
\\&amp;= \boldsymbol{y}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y})\\&amp;=\boldsymbol{y}^\top\mathbf{H}_{\mathbf{X}}\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}
\\&amp;= 0
\end{align*}\]</span> where we use the definition of <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and <span class="math inline">\(\boldsymbol{e} = \boldsymbol{y} - \widehat{\boldsymbol{y}}\)</span> on the first line, then substitute the OLS estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\boldsymbol{y}\)</span> and distribute terms. Similarly, <span class="math inline">\(\mathbf{X}^\top\boldsymbol{e}=\boldsymbol{0}_p\)</span>. The ordinary residuals are thus orthogonal to both the model matrix <span class="math inline">\(\mathbf{X}\)</span> and to the fitted values.</p>
<p>A direct consequence of this fact is that the sample linear correlation between <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> is zero; we will use this property to build graphical diagnostics. , and is also used to construct visual diagnostics of goodness-of-fit.</p>
<p>Since the inner product is zero, the mean of <span class="math inline">\(\boldsymbol{e}\)</span> must be zero provided that <span class="math inline">\(\mathbf{1}_n\)</span> is in the linear span of <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<div id="prp-info-normal" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.3 (Information matrix for normal linear regression models)</strong></span> The entries of the observed information matrix of the normal linear model are <span class="math display">\[\begin{align*}
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \boldsymbol{\beta}^\top} &amp;= \frac{1}{\sigma^2} \frac{\partial \mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^\top} =  \frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \sigma^2} &amp;=- \frac{\mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^4}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial (\sigma^2)^2} &amp;= -\frac{n}{2\sigma^4} + \frac{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^6}.
\end{align*}\]</span> If we evaluate the observed information at the MLE, we get <span class="math display">\[\begin{align*}
j(\widehat{\boldsymbol{\beta}}, \widehat{\sigma^2}) =
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\widehat{\sigma^2}} &amp; \boldsymbol{0}_p \\  \boldsymbol{0}_p^\top &amp; \frac{n}{2\widehat{\sigma^4}}
\end{pmatrix}
\end{align*}\]</span> since <span class="math inline">\(\widehat{\sigma}^2=\mathsf{SS}_e/n\)</span> and the residuals are orthogonal to the model matrix. Since <span class="math inline">\(\mathsf{E}(Y \mid \mathbf{X})=\mathbf{X}\boldsymbol{\beta}\)</span>, the Fisher information is <span class="math display">\[\begin{align*}
i(\boldsymbol{\beta}, \sigma^2) =
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2} &amp; \boldsymbol{0}_p \\  \boldsymbol{0}_p^\top &amp; \frac{n}{2\sigma^4}
\end{pmatrix}
\end{align*}\]</span> Since zero off-correlations in normal models amount to independence, the MLE for <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> are independent. Provided <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span> is invertible, the large-sample variance of the ordinary least squares estimator is <span class="math inline">\(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span> and that of the MLE of the variance is <span class="math inline">\(2\sigma^4/n\)</span>.</p>
</div>
</section>
<section id="fitting-linear-models-with-software" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="fitting-linear-models-with-software"><span class="header-section-number">4.3.2</span> Fitting linear models with software</h3>
<p>Although we could build the model matrix ourselves and use the least square formula of <a href="#eq-ols" class="quarto-xref">Equation&nbsp;<span>4.1</span></a>, the numerical routines implemented in software are typically better behaved. The <code>lm</code> function in <strong>R</strong> fits <strong>linear models</strong>, as does <code>glm</code> with the default arguments. Objects of class <code>lm</code> have multiple methods allow you to extract specific objects from <code>lm</code> objects. For example, the functions <code>coef</code>, <code>resid</code>, <code>fitted</code>, <code>model.matrix</code> will return the coefficients <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, the ordinary residuals <span class="math inline">\(\boldsymbol{e}\)</span>, the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and the model matrix <span class="math inline">\(\mathbf{X}\)</span>, respectively.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(college, <span class="at">package =</span> <span class="st">"hecstatmod"</span>) <span class="co">#load data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(college) <span class="co"># Check that categorical variables are factors</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the linear regression</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>linmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> sex <span class="sc">+</span> rank <span class="sc">+</span> service <span class="sc">+</span> field, </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> college)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(linmod) <span class="co"># beta coefficients</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>vcov_beta <span class="ot">&lt;-</span> <span class="fu">vcov</span>(linmod) <span class="co"># Covariance matrix of betas</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod) <span class="co"># summary table</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>beta_ci <span class="ot">&lt;-</span> <span class="fu">confint</span>(linmod) <span class="co"># Wald confidence intervals for betas</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(linmod) <span class="co"># fitted values</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(linmod) <span class="co"># ordinary residuals</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Check OLS formula</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(linmod) <span class="co"># model matrix</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> college<span class="sc">$</span>salary</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="fu">isTRUE</span>(<span class="fu">all.equal</span>(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(<span class="fu">coef</span>(linmod))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>summary</code> method is arguably the most useful: it will print mean parameter estimates along with standard errors, <span class="math inline">\(t\)</span> values for the Wald test of the hypothesis <span class="math inline">\(\mathscr{H}_0: \beta_i=0\)</span> and the associated <span class="math inline">\(P\)</span>-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table. Note that the <code>lm</code> function uses the unbiased estimator of the variance <span class="math inline">\(\sigma^2\)</span>.</p>
<div id="rem-linearity" class="proof remark">
<p><span class="proof-title"><em>Remark 4.5</em> (Linearity). </span>The model is linear in the coefficients <span class="math inline">\(\beta\)</span>, so the quadratic curve <span class="math inline">\(\beta_0 + \beta_1x + \beta_2 x^2\)</span> is a linear model because it is a sum of coefficients times functions of explanatories. By contrast, the model <span class="math inline">\(\beta_0 + \beta_1x^{\beta_2}\)</span> is nonlinear in <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>It is sometimes possible to linearize a model: consider for example the Cobb–Douglas production function <span class="citation" data-cites="Douglas:1976">(<a href="references.html#ref-Douglas:1976" role="doc-biblioref">Douglas 1976</a>)</span>, which specifies that economic output <span class="math inline">\(Y\)</span> is related to labour <span class="math inline">\(L\)</span> and capital <span class="math inline">\(C\)</span> via <span class="math inline">\(\mathsf{E}(Y \mid L, C) = \beta_0C^{\beta}L^{1-\beta}\)</span> with <span class="math inline">\(\beta \in (0,1)\)</span>. If we take logarithms on both sides (since all arguments are positive), then <span class="math inline">\(\mathsf{E}(\ln Y \mid L, C) = \beta_0^* + \beta_1 \ln C + (1-\beta_1)\ln L\)</span>. We could fit a linear model with response <span class="math inline">\(\ln Y - \ln L\)</span> and explanatory variable <span class="math inline">\(\ln C - \ln L\)</span>, to obtain an estimate of the coefficient <span class="math inline">\(\beta_1\)</span>, while <span class="math inline">\(\beta_0^*=\ln \beta_0\)</span>. A constrained optimization would be potentially necessary to estimate the model parameters of the resulting linear model if the estimates lie outside of the parameter space.</p>
</div>
</section>
</section>
<section id="coefR2" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="coefR2"><span class="header-section-number">4.4</span> Coefficient of determination</h2>
<p>When we specify a model, the error term <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> accounts for the fact no perfect linear relationship characterizes the data (if it did, we wouldn’t need statistic to begin with). Once we have fitted a model, we estimate the variance <span class="math inline">\(\sigma^2\)</span>; one may then wonder which share of the total variance in the sample is explained by the model.</p>
<p>The total sum of squares, defined as the sum of squared residuals from the intercept-only model, serves as comparison — the simplest model we could come up with would involving every observation by the sample mean of the response and so this gives (up to scale) the variance of the response, <span class="math inline">\(\mathsf{SS}_c = \sum_{i=1}^n (y_i - \overline{y})^2\)</span>. We can then compare the variance of the original data with that of the residuals from the model with covariate matrix <span class="math inline">\(\mathbf{X}\)</span>, defined as <span class="math inline">\(\mathsf{SS}_e =\sum_{i=1}^n e_i^2\)</span> with <span class="math inline">\(e_i = y_i - \widehat{\beta}_0 - \sum_{j=1}^p \widehat{\beta}_j\mathrm{X}_j\)</span>. We define the coefficient of determination, or squared multiple correlation coefficient of the model, <span class="math inline">\(R^2\)</span>, as <span class="math display">\[\begin{align*}
R^2 &amp;=1- \frac{\mathsf{SS}_e}{\mathsf{SS}_c} = \frac{\sum_{i=1}^n (y_i - \overline{y})^2- \sum_{i=1}^n e_i^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
\end{align*}\]</span> An alternative decomposition shows that <span class="math inline">\(R^2 = \mathsf{cor}^2(\boldsymbol{y}, \widehat{\boldsymbol{y}})\)</span>, i.e., the coefficient of determination can be interpreted as the square of <a href="moments">Pearson’s linear correlation</a> between the response <span class="math inline">\(\boldsymbol{y}\)</span> and the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span>.</p>
<p>Its important to note that <span class="math inline">\(R^2\)</span> is not a goodness-of-fit criterion, like the log likelihood: some phenomena are inherently noisy and even a good model will fail to account for much of the response’s variability. Moreover, one can inflate the value of <span class="math inline">\(R^2\)</span> by including more explanatory variables and making the model more complex, thereby improving the likelihood and <span class="math inline">\(R^2\)</span>. Indeed, the coefficient is non-decreasing in the dimension of <span class="math inline">\(\mathbf{X}\)</span>, so a model with <span class="math inline">\(p+1\)</span> covariate will necessarily have a higher <span class="math inline">\(R^2\)</span> values than only <span class="math inline">\(p\)</span> of the explanatories. For model comparisons, it is better to employ information criteria or else rely on the predictive performance if this is the purpose of the regression. Lastly, a model with a high <span class="math inline">\(R^2\)</span> may imply high correlation, but <a href="http://www.tylervigen.com/spurious-correlations">the relation may be spurious</a>: linear regression does not yield causal models!</p>
<div id="prp-linearity" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.4 (Log linear models)</strong></span> &nbsp;</p>
</div>
<!--
1. 
[X] Motivating examples from management sciences
[X] Models with explanatories
[X] What to put in the model?
[X] Linearity: what does it mean?
IJLR - Gaussian models as workhorse
[X] Since Gaussian is a location-scale family, write in terms of response = mean + error


Estimation of OLS
OLS and MLE of the mean parameter coefficients
OLS does not require the assumption of normality (only constant variance, independence and mean-zero errors).

Residuals as vertical distance
Fitted model
MLE of the mean for Yi (Rsquared as correlation between Yhat and Y)
fitted mean + residuals != mean + error (notion of n + p+2 unknowns)
Understanding the output - ingredients and components in the output

Parameter interpretation
Marginal effects
[X] Simple cases with binary/continuous/categorical
Invariance of reparametrizations
[X] Experimental vs observational
Confounding variables



Interactions and interaction plots
Linearity and interpretation of effects - added variable plots
Estimation and testing
t-tests
F-tests
Linear contrasts
Comparisons between t/F distributions and MLE
Prediction from the model
Colinearity and identifiability
R-squared


Model assumptions
Residual diagnostics
Plots and remedies/ a primer on other models


Some adaptation of known results from likelihood, specialized to the case of normal data - t vs normal, F vs chi-square


The flexibility of explanatories
The linearity is in the betas

Building models: dummies, categorical variables, continuous covariates
Parameter interpretation
Marginal effects as derivatives
Reparametrization and invariance


Estimation and links with likelihood (observed information, expected information)


Interpretation as simple linear regression from FWL theorem
OLS as BLUE


Testing


Normal model and tacit assumptions
-->
</section>
<section id="concluding-remarks" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">4.5</span> Concluding remarks</h2>
<p>Linear regression is the most famous and the most widely used statistical model around. The name may appear reductive, but many tests statistics (<em>t</em>-tests, ANOVA, Wilcoxon, Kruskal–Wallis) <a href="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf">can be formulated using a linear regression</a>, while <a href="https://threadreaderapp.com/thread/1286420597505892352.html">models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise</a>. What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of explanatory variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Baumann:1992" class="csl-entry" role="listitem">
Baumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. <span>“Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.”</span> <em>Journal of Reading Behavior</em> 24 (2): 143–72. <a href="https://doi.org/10.1080/10862969209547770">https://doi.org/10.1080/10862969209547770</a>.
</div>
<div id="ref-Douglas:1976" class="csl-entry" role="listitem">
Douglas, Paul H. 1976. <span>“The <span>Cobb–Douglas</span> Production Function Once Again: Its History, Its Testing, and Some New Empirical Values.”</span> <em>Journal of Political Economy</em> 84 (5): 903–15. <a href="http://www.jstor.org/stable/1830435">http://www.jstor.org/stable/1830435</a>.
</div>
<div id="ref-Lee.Choi:2019" class="csl-entry" role="listitem">
Lee, Kiljae, and Jungsil Choi. 2019. <span>“Image-Text Inconsistency Effect on Product Evaluation in Online Retailing.”</span> <em>Journal of Retailing and Consumer Services</em> 49: 279–88. <a href="https://doi.org/10.1016/j.jretconser.2019.03.015">https://doi.org/10.1016/j.jretconser.2019.03.015</a>.
</div>
<div id="ref-Moon.VanEpps:2023" class="csl-entry" role="listitem">
Moon, Alice, and Eric M VanEpps. 2023. <span>“Giving Suggestions: Using Quantity Requests to Increase Donations.”</span> <em>Journal of Consumer Research</em> 50 (1): 190–210. <a href="https://doi.org/10.1093/jcr/ucac047">https://doi.org/10.1093/jcr/ucac047</a>.
</div>
<div id="ref-Venables:2000" class="csl-entry" role="listitem">
Venables, William N. 2000. <span>“Exegeses on Linear Models.”</span> In <em>S-PLUS User’s Conference</em>. Washington, D.C. <a href="https://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf">https://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/math60604a\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./likelihood.html" class="pagination-link" aria-label="Likelihood-based inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihood-based inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="Bibliography">
        <span class="nav-page-text">Bibliography</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>All right reserved (Léo Belzile)</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/linearmodels.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>