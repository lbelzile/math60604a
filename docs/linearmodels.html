<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="This is a web complement for MATH 60604A Statistical Modelling, a master course offered at HEC Montréal.">

<title>4&nbsp; Linear regression models – MATH 60604A - Statistical Modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./likelihood.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<script src="site_libs/jquery-3.5.1/jquery.min.js"></script>
<link href="site_libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="css/style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linearmodels.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MATH 60604A - Statistical Modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math60604a/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH60604A.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Statistical inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./likelihood.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihood-based inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linearmodels.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">4.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#motivating-examples" id="toc-motivating-examples" class="nav-link" data-scroll-target="#motivating-examples"><span class="header-section-number">4.1.1</span> Motivating examples</a></li>
  <li><a href="#exploratory-data-analysis" id="toc-exploratory-data-analysis" class="nav-link" data-scroll-target="#exploratory-data-analysis"><span class="header-section-number">4.1.2</span> Exploratory data analysis</a></li>
  <li><a href="#mean-model-specification" id="toc-mean-model-specification" class="nav-link" data-scroll-target="#mean-model-specification"><span class="header-section-number">4.1.3</span> Mean model specification</a></li>
  <li><a href="#continuous-explanatories" id="toc-continuous-explanatories" class="nav-link" data-scroll-target="#continuous-explanatories"><span class="header-section-number">4.1.4</span> Continuous explanatories</a></li>
  <li><a href="#categorical-covariates" id="toc-categorical-covariates" class="nav-link" data-scroll-target="#categorical-covariates"><span class="header-section-number">4.1.5</span> Categorical covariates</a></li>
  </ul></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="header-section-number">4.2</span> Parameter estimation</a></li>
  <li><a href="#sec-predictions-lm" id="toc-sec-predictions-lm" class="nav-link" data-scroll-target="#sec-predictions-lm"><span class="header-section-number">4.3</span> Predictions</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">4.4</span> Hypothesis testing</a>
  <ul class="collapse">
  <li><a href="#contrasts" id="toc-contrasts" class="nav-link" data-scroll-target="#contrasts"><span class="header-section-number">4.4.1</span> Contrasts</a></li>
  <li><a href="#examples-of-tests" id="toc-examples-of-tests" class="nav-link" data-scroll-target="#examples-of-tests"><span class="header-section-number">4.4.2</span> Examples of tests</a></li>
  </ul></li>
  <li><a href="#factorial-designs-and-interactions" id="toc-factorial-designs-and-interactions" class="nav-link" data-scroll-target="#factorial-designs-and-interactions"><span class="header-section-number">4.5</span> Factorial designs and interactions</a></li>
  <li><a href="#geometry-of-least-squares" id="toc-geometry-of-least-squares" class="nav-link" data-scroll-target="#geometry-of-least-squares"><span class="header-section-number">4.6</span> Geometry of least squares</a>
  <ul class="collapse">
  <li><a href="#residuals" id="toc-residuals" class="nav-link" data-scroll-target="#residuals"><span class="header-section-number">4.6.1</span> Residuals</a></li>
  <li><a href="#collinearity" id="toc-collinearity" class="nav-link" data-scroll-target="#collinearity"><span class="header-section-number">4.6.2</span> Collinearity</a></li>
  <li><a href="#leverage-and-outliers" id="toc-leverage-and-outliers" class="nav-link" data-scroll-target="#leverage-and-outliers"><span class="header-section-number">4.6.3</span> Leverage and outliers</a></li>
  </ul></li>
  <li><a href="#model-assumptions-and-diagnostics" id="toc-model-assumptions-and-diagnostics" class="nav-link" data-scroll-target="#model-assumptions-and-diagnostics"><span class="header-section-number">4.7</span> Model assumptions and diagnostics</a>
  <ul class="collapse">
  <li><a href="#independence-assumption" id="toc-independence-assumption" class="nav-link" data-scroll-target="#independence-assumption"><span class="header-section-number">4.7.1</span> Independence assumption</a></li>
  <li><a href="#linearity-assumption" id="toc-linearity-assumption" class="nav-link" data-scroll-target="#linearity-assumption"><span class="header-section-number">4.7.2</span> Linearity assumption</a></li>
  <li><a href="#constant-variance-assumption" id="toc-constant-variance-assumption" class="nav-link" data-scroll-target="#constant-variance-assumption"><span class="header-section-number">4.7.3</span> Constant variance assumption</a></li>
  <li><a href="#normality-assumption" id="toc-normality-assumption" class="nav-link" data-scroll-target="#normality-assumption"><span class="header-section-number">4.7.4</span> Normality assumption</a></li>
  </ul></li>
  <li><a href="#extensions-of-the-model" id="toc-extensions-of-the-model" class="nav-link" data-scroll-target="#extensions-of-the-model"><span class="header-section-number">4.8</span> Extensions of the model</a>
  <ul class="collapse">
  <li><a href="#sec-transfo" id="toc-sec-transfo" class="nav-link" data-scroll-target="#sec-transfo"><span class="header-section-number">4.8.1</span> Transformation of the response</a></li>
  </ul></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">4.9</span> Concluding remarks</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/linearmodels.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="linmod" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">4.1</span> Introduction</h2>
<p>The linear regression model, or linear model, is one of the most versatile workhorse for statistical inference. Linear regression is used primarily to evaluate the effects of explanatory variables (oftentimes treatment in an experimental setting) on the mean response of a continuous response, or for prediction. The linear regression specifies the mean of a <strong>response variable</strong> <span class="math inline">\(Y\)</span> of a random sample of size <span class="math inline">\(n\)</span> as a <strong>linear function</strong> of observed <strong>explanatories</strong> (also called predictors or covariates) <span class="math inline">\(X_1, \ldots, X_p\)</span>, <span class="math display">\[\begin{align}
\underset{\text{conditional mean}}{\mathsf{E}(Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i)}=\mu_i=\underset{\text{linear combination of explanatories}}{\beta_0 + \beta_1x_{i1} + \cdots + \beta_p x_{ip}}\equiv \mathbf{x}_i\boldsymbol{\beta}.
\end{align}\]</span> where <span class="math inline">\(\mathbf{x}_i = (1, x_{i1}, \ldots, x_{ip})\)</span> is a <span class="math inline">\((p+1)\)</span> row vector containing a constant and the explanatories of observation <span class="math inline">\(i\)</span>, and <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)^\top\)</span> is a <span class="math inline">\(p+1\)</span> column vector of coefficients for the mean. The model formulation is conditional on the values of the observed explanatories; this amounts to treating the <span class="math inline">\(p\)</span> explanatory variables <span class="math inline">\(X_1, \ldots, X_p\)</span> as non-random quantities, or known in advance. The regression coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> is the same for all observations, but the vector of explanatories <span class="math inline">\(\mathbf{x}_i\)</span> may change from one observation to the next.</p>
<div id="rem-linearity" class="proof remark">
<p><span class="proof-title"><em>Remark 4.1</em> (Linearity). </span>The model is <strong>linear</strong> in the coefficients <span class="math inline">\(\beta_0, \ldots, \beta_p\)</span>. The quadratic curve <span class="math inline">\(\beta_0 + \beta_1x + \beta_2 x^2\)</span> is a linear model because it is a sum of coefficients times functions of explanatories. By contrast, the model <span class="math inline">\(\beta_0 + \beta_1x^{\beta_2}\)</span> is nonlinear in <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
</div>
<p>To simplify the notation, we aggregate observations into an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\boldsymbol{Y}\)</span> and the explanatories into an <span class="math inline">\(n \times (p+1)\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> by concatenating a column of ones and the <span class="math inline">\(p\)</span> column vectors <span class="math inline">\(\boldsymbol{X}_1, \ldots, \boldsymbol{X}_p\)</span>, each containing the <span class="math inline">\(n\)</span> observations of the respective explanatories. The matrix <span class="math inline">\(\mathbf{X}\)</span> is termed <strong>model matrix</strong> (or sometimes design matrix in experimental settings), and it’s <span class="math inline">\(i\)</span>th row is <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>Assuming that the distribution of the response is drawn from a location family, we may rewrite the linear model in terms of the mean plus an error term, <span class="math display">\[\begin{align*}
\underset{\text{observation}\vphantom{\mu_i}}{Y_i} = \underset{\text{mean } \mu_i}{\vphantom{Y_i}\mathbf{x}_i\boldsymbol{\beta}} + \underset{\text{error term}\vphantom{\mu_i}}{\vphantom{Y_i}\varepsilon_i}.
\end{align*}\]</span> where <span class="math inline">\(\varepsilon_i\)</span> is the error term specific to observation <span class="math inline">\(i\)</span>, and we assume that the errors <span class="math inline">\(\varepsilon_1, \ldots, \varepsilon_n\)</span> are independent and identically distributed. We fix the expectation or theoretical mean of <span class="math inline">\(\varepsilon_i\)</span> to zero to encode the fact we do not believe the model is systematically off, so <span class="math inline">\(\mathsf{E}(\varepsilon_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i)=0\)</span> <span class="math inline">\((i=1, \ldots, n)\)</span>. The variance term <span class="math inline">\(\sigma^2\)</span> is included to take into account the fact that no exact linear relationship links <span class="math inline">\(\boldsymbol{X}_i\)</span> and <span class="math inline">\(Y_i\)</span>, or that measurements of <span class="math inline">\(Y_i\)</span> are subject to error.</p>
<p>The normal or Gaussian linear model specifies that responses follow a normal distribution, with <span class="math inline">\(Y_i \mid \boldsymbol{X}_i=\boldsymbol{x}_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)\)</span>. The normal distribution is a location-scale family, so <span class="math inline">\(Y \sim \mathsf{normal}(\mu, \sigma^2)\)</span> is equal in distribution with <span class="math inline">\(\mu + \varepsilon\)</span> for <span class="math inline">\(\varepsilon \sim \mathsf{normal}(0, \sigma^2)\)</span>.</p>
<section id="motivating-examples" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="motivating-examples"><span class="header-section-number">4.1.1</span> Motivating examples</h3>
<p>We present some motivating examples that are discussed in the sequel.</p>
<div id="exm-lee-choi1" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 (Consistency of product description)</strong></span> Study 1 of <span class="citation" data-cites="Lee.Choi:2019">Lee and Choi (<a href="references.html#ref-Lee.Choi:2019" role="doc-biblioref">2019</a>)</span> considered descriptors and the impact on the perception of a product on the discrepancy between the text description and the image. In their first experience, a set of six toothbrushes is sold, but the image shows either a pack of six, or a single one). The authors also measured the prior familiarity with the brand of the item. Participants were recruited using an online panel, and the data in <code>LC19_S1</code> includes the results of the <span class="math inline">\(n=96\)</span> participants who passed the attention check (one additional participant response was outlying and removed). We could fit a linear model for the average product evaluation score, <code>prodeval</code>, as a function of the familiarity of the brand <code>familiarity</code>, an integer ranging from 1 to 7, and a dummy variable for the experimental factor <code>consistency</code>, coded <code>0</code> for consistent image/text descriptions and <code>1</code> if inconsistent. The resulting model matrix is then <span class="math inline">\(96 \times 3\)</span>. The <code>prodeval</code> response is heavily discretized.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(LC19_S1, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>modmat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="co"># extract model matrix</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>     <span class="sc">~</span> familiarity <span class="sc">+</span> consistency,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">data =</span> LC19_S1)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(modmat, <span class="at">n =</span> <span class="dv">5</span>L) <span class="co"># print first five lines</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    (Intercept) familiarity consistencyinconsistent</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 92           1           6                       1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 93           1           4                       1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 94           1           7                       1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 95           1           7                       1</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 96           1           7                       1</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(modmat) <span class="co"># dimension of the model matrix</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 96  3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="exm-teaching-baumann" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 (Teaching to read and pre-post experiments)</strong></span> The <code>BSJ92</code> data in package <code>hecedsm</code> contains the results of an experimental study by <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> on the effectiveness of different reading strategies on understanding of children. These are described in the abstract</p>
<blockquote class="blockquote">
<p>Sixty-six fourth-grade students were randomly assigned to one of three experimental groups: (a) a Think-Aloud (TA) group, in which students were taught various comprehension monitoring strategies for reading stories (e.g., self-questioning, prediction, retelling, rereading) through the medium of thinking aloud; (b) a Directed Reading-Thinking Activity (DRTA) group, in which students were taught a predict-verify strategy for reading and responding to stories; or (c) a Directed Reading Activity (DRA) group, an instructed control, in which students engaged in a noninteractive, guided reading of stories.</p>
</blockquote>
<p>The data are balanced, as there are 22 observations in each of the three subgroups, of which <code>DR</code> is the control. The researchers applied a series of three tests (an error detection task for test 1, a comprehension monitoring questionnaire for test 2, and the <em>Degrees of Reading Power</em> cloze test labelled test 3). Tests 1 and 2 were administered both before and after the intervention: this gives us a change to establish the average <em>improvement</em> in student by adding <code>pretest1</code> as covariate for a regression of <code>posttest</code>, for example. The tests 1 were out of 16, but the one administered after the experiment was made more difficult to avoid cases of students getting near full scores. The correlation between pre-test and post-test 1 is <span class="math inline">\((\widehat{\rho}_1=0.57)\)</span>, much stronger than that for the second test <span class="math inline">\((\widehat{\rho}_2=0.21)\)</span>.</p>
</div>
<div id="exm-college-salary-discrimination" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 (Gender discrimination in a US college)</strong></span> The <code>college</code> database consists of observational data collected in a college in the United States. The goal of the administration was to investigate potential gender inequality in the salary of faculty members. The data contains the following variables:</p>
<ul>
<li><code>salary</code>: nine-month salary of professors during the 2008–2009 academic year (in thousands USD).</li>
<li><code>rank</code>: academic rank of the professor (<code>assistant</code>, <code>associate</code> or <code>full</code>).</li>
<li><code>field</code>: categorical variable for the field of expertise of the professor, one of <code>applied</code> or <code>theoretical</code>.</li>
<li><code>sex</code>: binary indicator for sex, either <code>man</code> or <code>woman</code>.</li>
<li><code>service</code>: number of years of service in the college.</li>
<li><code>years</code>: number of years since PhD.</li>
</ul>
</div>
<div id="exm-moon-vanepps" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4 (Suggestions increase donations to charities)</strong></span> Study 1 of <span class="citation" data-cites="Moon.VanEpps:2023">Moon and VanEpps (<a href="references.html#ref-Moon.VanEpps:2023" role="doc-biblioref">2023</a>)</span> considers proportion of donators to a charity. Participants in the online panel were provided with an opportunity to win 25$ and donate part of this amount to a charity of their choosing. The data provided include only people who did not exceed this amount and indicated donating a non-zero amount.</p>
</div>
<div id="exm-sokolova" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5 (Is additional paper wrapping viewed as more eco-friendly?)</strong></span> <span class="citation" data-cites="Sokolova:2023">Sokolova, Krishna, and Döring (<a href="references.html#ref-Sokolova:2023" role="doc-biblioref">2023</a>)</span> consider consumer bias when assessing how eco-friendly packages are. Items such as cereal are packaged in plastic bags, which themselves are covered in a box. They conjecture (and find) that, paradoxically, consumers tend to view the packaging as being more eco-friendly when the amount of cardboard or paper surrounding the box is larger, relative to the sole plastic package. We consider in the sequel the data from Study 2A, which measures the perceived environmental friendliness (PEF, variable <code>pef</code>) as a function of the <code>proportion</code> of paper wrapping (either none, half of the area of the plastic, equal or twice).</p>
</div>
</section>
<section id="exploratory-data-analysis" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="exploratory-data-analysis"><span class="header-section-number">4.1.2</span> Exploratory data analysis</h3>
<p>Exploratory data analysis (EDA) is an iterative procedure by which we query the data, using auxiliary information, summary statistics and data visualizations, to better inform our modelling.</p>
<p>It is useful to get a better understanding of the features of the data (sampling frame, missing values, outliers), the nature of the observations, whether responses or explanatories and the relationship between them.</p>
<p>See <a href="https://tellingstorieswithdata.com/11-eda.html">Chapter 11 of Alexander (2023)</a> for examples. In particular, it is useful to check that</p>
<ul>
<li>categorical variables are properly code as factors.</li>
<li>missing values are properly declared as such using <code>NA</code> (strings, <code>999</code>, etc.)</li>
<li>there is no missingness patterns (<code>NA</code> for some logical values)</li>
<li>there are enough modalities of each level of categorical variables</li>
<li>there is no explanatory variable derived from the response variable.</li>
<li>the subset of observations used for statistical analysis is adequate.</li>
<li>there are no anomalies or outliers that would distort the results.</li>
</ul>
<div id="exm-college-eda" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6 (Exploratory data analysis of the <code>college</code> data)</strong></span> Before drafting a model, it is useful to perform an exploratory data analysis. If salary increases with year, there is more heterogeneity in the salary of higher ranked professors: logically, assistant professors are either promoted or kicked out after at most 6 years according to the data. The limited number of years prevents large variability for their salaries.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-edacollege" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edacollege-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-edacollege-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edacollege-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Exploratory data analysis of <code>college</code> data: salaries of professors as a function of the number of years of service and the academic ranking
</figcaption>
</figure>
</div>
</div>
</div>
<p>Salary increases over years of service, but its variability also increases with rank. Note the much smaller number of women in the sample: this will impact our power to detect differences between sex. A contingency table of sex and academic rank can be useful to see if the proportion of women is the same in each rank: women represent 16% of assistant professors and 16% of associate profs, but only 7% of full professors and these are better paid on average.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Contingency table of the number of prof in the college by sex and academic rank.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">assistant</th>
<th style="text-align: right;">associate</th>
<th style="text-align: right;">full</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">man</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">54</td>
<td style="text-align: right;">248</td>
</tr>
<tr class="even">
<td style="text-align: left;">woman</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">18</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Some of the potential explanatory variables of the <code>college</code> data are categorical (<code>rank</code>, <code>sex</code>, <code>field</code>), the latter two being binary. The other two continuous variables, <code>years</code> and <code>service</code>, are strongly correlated with a correlation of 0.91.</p>
</div>
<div id="exm-missing-values-eda" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7 (Handling of missing values)</strong></span> The data for <span class="citation" data-cites="Moon.VanEpps:2023">Moon and VanEpps (<a href="references.html#ref-Moon.VanEpps:2023" role="doc-biblioref">2023</a>)</span> should be checked to ensure that the description of the data collection matches the structure of the database. Since people who didn’t donate didn’t fill in the amount field, the latter indicates a missing value. All donation amounts are between $0.25 and $25.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(MV23_S1, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(MV23_S1)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; tibble [869 × 4] (S3: tbl_df/tbl/data.frame)</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ before   : int [1:869] 0 1 0 1 1 1 1 0 1 0 ...</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ donate   : int [1:869] 0 0 0 1 1 0 1 0 0 1 ...</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ condition: Factor w/ 2 levels "open-ended","quantity": 1 1 1 1 2 2 2 1 1 1 ...</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ amount   : num [1:869] NA NA NA 10 5 NA 20 NA NA 25 ...</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(MV23_S1)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      before          donate          condition       amount    </span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Min.   :0.000   Min.   :0.00   open-ended:407   Min.   : 0.2  </span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  1st Qu.:0.000   1st Qu.:0.00   quantity  :462   1st Qu.: 5.0  </span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Median :1.000   Median :1.00                    Median :10.0  </span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Mean   :0.596   Mean   :0.73                    Mean   :10.7  </span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  3rd Qu.:1.000   3rd Qu.:1.00                    3rd Qu.:15.0  </span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Max.   :1.000   Max.   :1.00                    Max.   :25.0  </span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  NA's   :1                                       NA's   :235</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we include <code>amount</code> as response variable, the 235 missing observations will be removed by default by procedures. This is okay if we want to compare the average amount of people who donated, but we need to transform <code>NA</code>s to zeros otherwise. The <code>donate</code> binary variable should not be included as an explanatory variable in a regression model, since it is a perfect predictor of zero amounts.</p>
</div>
</section>
<section id="mean-model-specification" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="mean-model-specification"><span class="header-section-number">4.1.3</span> Mean model specification</h3>
<p>The first step of an analysis is deciding which explanatory variables should be added to the mean model specification, and under what form. Models are but approximations of reality; Section 2.1 of <span class="citation" data-cites="Venables:2000">Venables (<a href="references.html#ref-Venables:2000" role="doc-biblioref">2000</a>)</span> argues that, if we believe the true mean function linking explanatories <span class="math inline">\(\boldsymbol{X}\)</span> and the response <span class="math inline">\(Y\)</span> is of the form <span class="math inline">\(\mathsf{E}(Y \mid \boldsymbol{X}) = f(\boldsymbol{X})\)</span> for <span class="math inline">\(f\)</span> sufficiently smooth, then the linear model is a first-order approximation. For interpretation purposes, it makes sense to mean-center any continuous explanatory, as this facilitates interpretation.</p>
<p>In an experimental setting, where the experimental group or condition is randomly allocated, we can directly compare the different treatments and draw causal conclusions (since all other things are constant, any detectable difference is due on average to our manipulation). Although we usually refrain from including any other explanatory to keep the design simple, it may be nevertheless helpful to consider some concomitant variables that explain part of the variability to filter background noise and increase power. For example, for the <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> data, our interest is in comparing the average scores as a function of the teaching method, we would include <code>group</code>. In this example, it would also make sense to include the <code>pretest1</code> result as an explanatory. This way, we will model the average difference in improvement from pre-test to post-test rather than the average score.</p>
<p>In an observational setting, people self-select in different groups, so we need to account for differences. Linear models in economics and finance often add control variables to the model to account for potential differences due to socio-demographic variables (age, revenue, etc.) that would be correlated to the group. Any test for coefficients would capture only correlation between the outcome <span class="math inline">\(Y\)</span> and the postulated explanatory factor of interest.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/correlation_causation.jpg" class="img-fluid figure-img" style="width:85.0%"></p>
<figcaption>Difference between experimental and observational studies by Andrew Heiss <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a></figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="continuous-explanatories" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="continuous-explanatories"><span class="header-section-number">4.1.4</span> Continuous explanatories</h3>
<p>Continuous explanatories are typically specified by including a single linear term, leading to the simple linear regression of the form <span class="math inline">\(Y \mid X=x \sim \mathsf{normal}(\beta_0 + \beta x, \sigma^2)\)</span>. In this situation <span class="math inline">\(\beta_0\)</span> is the intercept (the mean value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x=0\)</span>) and <span class="math inline">\(\beta_1\)</span> is the slope, i.e., the average increase of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x\)</span> increases by one unit. <a href="#fig-droitenuage" class="quarto-xref">Figure&nbsp;<span>4.2</span></a> shows such an example of a model with a single explanatory. As revealed by the exploratory data analysis of <a href="#exm-college-salary-discrimination" class="quarto-xref">Example&nbsp;<span>4.3</span></a>, this model is simplistic and clearly insufficient to explain differences in salary.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-droitenuage" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-droitenuage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-droitenuage-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-droitenuage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Simple linear regression model for the salary of professors as a function of the number of years of service.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The <strong>intercept</strong> <span class="math inline">\(\beta_0\)</span> is the value when all of <span class="math inline">\(x_1, \ldots, x_p\)</span> are zero. The interpretation of the other mean parameters in the model depends crucially on the parametrization and on potential interactions or higher order terms.</p>
<p>Generally, we can increase <span class="math inline">\(X_j\)</span> by one unit and compare the increase in the mean, here for <span class="math inline">\(X_j\)</span> <span class="math display">\[\begin{align*}
\mathsf{E}(Y \mid X_j=x_j+1, \boldsymbol{X}_{-j} = \boldsymbol{x}_{-j}) - \mathsf{E}(Y \mid X_j=x_j, \boldsymbol{X}_{-j} = \boldsymbol{x}_{-j}) = \beta_j.
\end{align*}\]</span></p>
<p>Another common perspective is to consider the effect of a change in the value of an explanatory variable by looking at the slope. If <span class="math inline">\(\mu=\mathbf{x}\boldsymbol{\beta}\)</span>, then the marginal effect of the <span class="math inline">\(j\)</span> explanatory <span class="math inline">\(X_j=x_j\)</span> <span class="math inline">\((j=1, \ldots, p)\)</span> is the partial derivative of the mean with respect to this value, namely <span class="math inline">\(\partial \mu/\partial x_j\)</span>.</p>
<p>If the relationship between explanatory <span class="math inline">\(X\)</span> and response <span class="math inline">\(Y\)</span>, as assessed from a scatterplot, is not linear, we may consider more complicated function of the explanatories, as <a href="#exm-auto" class="quarto-xref">Example&nbsp;<span>4.8</span></a> shows.</p>
<div id="exm-auto" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8 (Quadratic curve for the automobile data)</strong></span> We consider a linear regression model for the fuel autonomy of cars as a function of the power of their motor (measured in horsepower) from the <code>auto</code> dataset. The postulated model, <span class="math display">\[\begin{align*}
\texttt{mpg}_i = \beta_0 + \beta_1 \texttt{horsepower}_i + \beta_2 \texttt{horsepower}_i^2 + \varepsilon_i,
\end{align*}\]</span> includes a quadratic term. <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.3</span></a> shows the scatterplot with the fitted regression line, above which the line for the simple linear regression for horsepower is added. The marginal effect of an increase of one unit in <code>horsepower</code> is <span class="math inline">\(\beta_1 + 2\beta_2 \texttt{horsepower}\)</span>, which depends on the value of the explanatory.</p>
<p>To fit higher order polynomials, we use the <code>poly</code> as the latter leads to more numerical stability. For general transformations, the <code>I</code> function tells the software interpret the input “as is”. Thus, <code>lm(y~x+I(x^2))</code>, would fit a linear model with design matrix <span class="math inline">\([\boldsymbol{1}_n\, \mathbf{x}\, \mathbf{x}^2]\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-autoquad2d" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoquad2d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-autoquad2d-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoquad2d-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Linear regression models for the fuel autonomy of cars as a function of motor power.
</figcaption>
</figure>
</div>
</div>
</div>
<p>It appears graphically that the quadratic model fits better than the simple linear alternative: we will assess this hypothesis formally later. For the degree two polynomial, <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.3</span></a> show that fuel autonomy decreases rapidly when power increases between 50 to 100, then more slow until 189.35 hp. After that, the model postulates that autonomy increases again as evidenced by the scatterplot, but beware of extrapolating (weird things can happen beyond the range of the data, as exemplified by <a href="https://web.archive.org/web/20210315050023/https://livefreeordichotomize.com/2020/05/05/model-detective/">Hassett’s cubic model for the number of daily cases of Covid19 in the USA</a>).</p>
<p>The representation in <a href="#fig-autoquad2d" class="quarto-xref">Figure&nbsp;<span>4.3</span></a> may seem counter-intuitive given that we fit a linear model, but it is a 2D projection of 3D coordinates for the equation <span class="math inline">\(\beta_0 + \beta_1x-y +\beta_2z =0\)</span>, where <span class="math inline">\(x=\texttt{horsepower}\)</span>, <span class="math inline">\(z=\texttt{horsepower}^2\)</span> and <span class="math inline">\(y=\texttt{mpg}\)</span>. Physics and common sense force <span class="math inline">\(z = x^2\)</span>, and so the fitted values lie on a curve in a 2D subspace of the fitted plan, as shown in grey in the three-dimensional <a href="#fig-hyperplan" class="quarto-xref">Figure&nbsp;<span>4.4</span></a>.</p>
<div class="cell" data-layout-align="center">
<div id="fig-hyperplan" class="cell-output-display quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-hyperplan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="plotly html-widget html-fill-item" id="htmlwidget-e9421466506cd568ba5c" style="width:85%;height:474.624px;"></div>
<script type="application/json" data-for="htmlwidget-e9421466506cd568ba5c">{"x":{"visdat":{"5606600905b0":["function () ","plotlyVisDat"],"56066478b43a":["function () ","data"]},"cur_data":"56066478b43a","attrs":{"5606600905b0":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004},"inherit":true},"5606600905b0.1":{"z":{},"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"inherit":false},"56066478b43a":{"colors":"grey","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"type":"scatter3d","mode":"lines","color":["#003C71"],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"horsepower"},"yaxis":{"title":"square of horsepower"},"zaxis":{"title":"fuel autonomy (mpg)"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[130,165,150,150,140,198,220,215,225,190,170,160,150,225,95,95,97,85,88,46,87,90,95,113,90,215,200,210,193,88,90,95,100,105,100,88,100,165,175,153,150,180,170,175,110,72,100,88,86,90,70,76,65,69,60,70,95,80,54,90,86,165,175,150,153,150,208,155,160,190,97,150,130,140,150,112,76,87,69,86,92,97,80,88,175,150,145,137,150,198,150,158,150,215,225,175,105,100,100,88,95,46,150,167,170,180,100,88,72,94,90,85,107,90,145,230,49,75,91,112,150,110,122,180,95,100,100,67,80,65,75,100,110,105,140,150,150,140,150,83,67,78,52,61,75,75,75,97,93,67,95,105,72,72,170,145,150,148,110,105,110,95,110,110,129,75,83,100,78,96,71,97,97,70,90,95,88,98,115,53,86,81,92,79,83,140,150,120,152,100,105,81,90,52,60,70,53,100,78,110,95,71,70,75,72,102,150,88,108,120,180,145,130,150,68,80,58,96,70,145,110,145,130,110,105,100,98,180,170,190,149,78,88,75,89,63,83,67,78,97,110,110,48,66,52,70,60,110,140,139,105,95,85,88,100,90,105,85,110,120,145,165,139,140,68,95,97,75,95,105,85,97,103,125,115,133,71,68,115,85,88,90,110,130,129,138,135,155,142,125,150,71,65,80,80,77,125,71,90,70,70,65,69,90,115,115,90,76,60,70,65,90,88,90,90,78,90,75,92,75,65,105,65,48,48,67,67,67,67,62,132,100,88,72,84,84,92,110,84,58,64,60,67,65,62,68,63,65,65,74,75,75,100,74,80,76,116,120,110,105,88,85,88,88,88,85,84,90,92,74,68,68,63,70,88,75,70,67,67,67,110,85,92,112,96,84,90,86,52,84,79,82],"y":[16900,27225,22500,22500,19600,39204,48400,46225,50625,36100,28900,25600,22500,50625,9025,9025,9409,7225,7744,2116,7569,8100,9025,12769,8100,46225,40000,44100,37249,7744,8100,9025,10000,11025,10000,7744,10000,27225,30625,23409,22500,32400,28900,30625,12100,5184,10000,7744,7396,8100,4900,5776,4225,4761,3600,4900,9025,6400,2916,8100,7396,27225,30625,22500,23409,22500,43264,24025,25600,36100,9409,22500,16900,19600,22500,12544,5776,7569,4761,7396,8464,9409,6400,7744,30625,22500,21025,18769,22500,39204,22500,24964,22500,46225,50625,30625,11025,10000,10000,7744,9025,2116,22500,27889,28900,32400,10000,7744,5184,8836,8100,7225,11449,8100,21025,52900,2401,5625,8281,12544,22500,12100,14884,32400,9025,10000,10000,4489,6400,4225,5625,10000,12100,11025,19600,22500,22500,19600,22500,6889,4489,6084,2704,3721,5625,5625,5625,9409,8649,4489,9025,11025,5184,5184,28900,21025,22500,21904,12100,11025,12100,9025,12100,12100,16641,5625,6889,10000,6084,9216,5041,9409,9409,4900,8100,9025,7744,9604,13225,2809,7396,6561,8464,6241,6889,19600,22500,14400,23104,10000,11025,6561,8100,2704,3600,4900,2809,10000,6084,12100,9025,5041,4900,5625,5184,10404,22500,7744,11664,14400,32400,21025,16900,22500,4624,6400,3364,9216,4900,21025,12100,21025,16900,12100,11025,10000,9604,32400,28900,36100,22201,6084,7744,5625,7921,3969,6889,4489,6084,9409,12100,12100,2304,4356,2704,4900,3600,12100,19600,19321,11025,9025,7225,7744,10000,8100,11025,7225,12100,14400,21025,27225,19321,19600,4624,9025,9409,5625,9025,11025,7225,9409,10609,15625,13225,17689,5041,4624,13225,7225,7744,8100,12100,16900,16641,19044,18225,24025,20164,15625,22500,5041,4225,6400,6400,5929,15625,5041,8100,4900,4900,4225,4761,8100,13225,13225,8100,5776,3600,4900,4225,8100,7744,8100,8100,6084,8100,5625,8464,5625,4225,11025,4225,2304,2304,4489,4489,4489,4489,3844,17424,10000,7744,5184,7056,7056,8464,12100,7056,3364,4096,3600,4489,4225,3844,4624,3969,4225,4225,5476,5625,5625,10000,5476,6400,5776,13456,14400,12100,11025,7744,7225,7744,7744,7744,7225,7056,8100,8464,5476,4624,4624,3969,4900,7744,5625,4900,4489,4489,4489,12100,7225,8464,12544,9216,7056,8100,7396,2704,7056,6241,6724],"z":[18,15,18,16,17,15,14,14,14,15,15,14,15,14,24,22,18,21,27,26,25,24,25,26,21,10,10,11,9,27,28,25,19,16,17,19,18,14,14,14,14,12,13,13,18,22,19,18,23,28,30,30,31,35,27,26,24,25,23,20,21,13,14,15,14,17,11,13,12,13,19,15,13,13,14,18,22,21,26,22,28,23,28,27,13,14,13,14,15,12,13,13,14,13,12,13,18,16,18,18,23,26,11,12,13,12,18,20,21,22,18,19,21,26,15,16,29,24,20,19,15,24,20,11,20,19,15,31,26,32,25,16,16,18,16,13,14,14,14,29,26,26,31,32,28,24,26,24,26,31,19,18,15,15,16,15,16,14,17,16,15,18,21,20,13,29,23,20,23,24,25,24,18,29,19,23,23,22,25,33,28,25,25,26,27,17.5,16,15.5,14.5,22,22,24,22.5,29,24.5,29,33,20,18,18.5,17.5,29.5,32,28,26.5,20,13,19,19,16.5,16.5,13,13,13,31.5,30,36,25.5,33.5,17.5,17,15.5,15,17.5,20.5,19,18.5,16,15.5,15.5,16,29,24.5,26,25.5,30.5,33.5,30,30.5,22,21.5,21.5,43.100000000000001,36.100000000000001,32.799999999999997,39.399999999999999,36.100000000000001,19.899999999999999,19.399999999999999,20.199999999999999,19.199999999999999,20.5,20.199999999999999,25.100000000000001,20.5,19.399999999999999,20.600000000000001,20.800000000000001,18.600000000000001,18.100000000000001,19.199999999999999,17.699999999999999,18.100000000000001,17.5,30,27.5,27.199999999999999,30.899999999999999,21.100000000000001,23.199999999999999,23.800000000000001,23.899999999999999,20.300000000000001,17,21.600000000000001,16.199999999999999,31.5,29.5,21.5,19.800000000000001,22.300000000000001,20.199999999999999,20.600000000000001,17,17.600000000000001,16.5,18.199999999999999,16.899999999999999,15.5,19.199999999999999,18.5,31.899999999999999,34.100000000000001,35.700000000000003,27.399999999999999,25.399999999999999,23,27.199999999999999,23.899999999999999,34.200000000000003,34.5,31.800000000000001,37.299999999999997,28.399999999999999,28.800000000000001,26.800000000000001,33.5,41.5,38.100000000000001,32.100000000000001,37.200000000000003,28,26.399999999999999,24.300000000000001,19.100000000000001,34.299999999999997,29.800000000000001,31.300000000000001,37,32.200000000000003,46.600000000000001,27.899999999999999,40.799999999999997,44.299999999999997,43.399999999999999,36.399999999999999,30,44.600000000000001,33.799999999999997,29.800000000000001,32.700000000000003,23.699999999999999,35,32.399999999999999,27.199999999999999,26.600000000000001,25.800000000000001,23.5,30,39.100000000000001,39,35.100000000000001,32.299999999999997,37,37.700000000000003,34.100000000000001,34.700000000000003,34.399999999999999,29.899999999999999,33,33.700000000000003,32.399999999999999,32.899999999999999,31.600000000000001,28.100000000000001,30.699999999999999,25.399999999999999,24.199999999999999,22.399999999999999,26.600000000000001,20.199999999999999,17.600000000000001,28,27,34,31,29,27,24,36,37,31,38,36,36,36,34,38,32,38,25,38,26,22,32,36,27,27,44,32,28,31],"type":"scatter3d","mode":"markers","name":"data","opacity":0.80000000000000004,"marker":{"color":"black","size":4,"hoverinfo":"skip","opacity":0.80000000000000004,"line":{"color":"rgba(31,119,180,1)"},"showscale":false},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"mpg<br />surf","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(190,190,190,1)"],["1","rgba(190,190,190,1)"]],"showscale":false,"z":[[38.059191113772322,-47.719700796540657],[100.5507364554749,14.771844545161926]],"type":"surface","x":[46,230],"y":[2116,52900],"name":"Relationship between horsepower and car autonomy","opacity":0.75,"cauto":false,"surfacecolor":[0,0,0],"frame":null},{"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250],"y":[0,1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900,961,1024,1089,1156,1225,1296,1369,1444,1521,1600,1681,1764,1849,1936,2025,2116,2209,2304,2401,2500,2601,2704,2809,2916,3025,3136,3249,3364,3481,3600,3721,3844,3969,4096,4225,4356,4489,4624,4761,4900,5041,5184,5329,5476,5625,5776,5929,6084,6241,6400,6561,6724,6889,7056,7225,7396,7569,7744,7921,8100,8281,8464,8649,8836,9025,9216,9409,9604,9801,10000,10201,10404,10609,10816,11025,11236,11449,11664,11881,12100,12321,12544,12769,12996,13225,13456,13689,13924,14161,14400,14641,14884,15129,15376,15625,15876,16129,16384,16641,16900,17161,17424,17689,17956,18225,18496,18769,19044,19321,19600,19881,20164,20449,20736,21025,21316,21609,21904,22201,22500,22801,23104,23409,23716,24025,24336,24649,24964,25281,25600,25921,26244,26569,26896,27225,27556,27889,28224,28561,28900,29241,29584,29929,30276,30625,30976,31329,31684,32041,32400,32761,33124,33489,33856,34225,34596,34969,35344,35721,36100,36481,36864,37249,37636,38025,38416,38809,39204,39601,40000,40401,40804,41209,41616,42025,42436,42849,43264,43681,44100,44521,44944,45369,45796,46225,46656,47089,47524,47961,48400,48841,49284,49729,50176,50625,51076,51529,51984,52441,52900,53361,53824,54289,54756,55225,55696,56169,56644,57121,57600,58081,58564,59049,59536,60025,60516,61009,61504,62001,62500],"z":[56.900099702112968,56.435140608266387,55.972642586621355,55.51260563717787,55.055029759935941,54.599914954895553,54.147261222056706,53.697068561419421,53.249336972983677,52.804066456749474,52.361257012716827,51.920908640885727,51.483021341256169,51.047595113828166,50.614629958601711,50.184125875576804,49.756082864753438,49.330500926131627,48.907380059711357,48.486720265492636,48.06852154347547,47.652783893659851,47.239507316045774,46.828691810633245,46.420337377422264,46.014444016412838,45.611011727604946,45.210040510998617,44.811530366593828,44.415481294390588,44.021893294388896,43.630766366588752,43.242100510990156,42.855895727593108,42.472152016397608,42.090869377403656,41.712047810611253,41.335687316020397,40.961787893631083,40.590349543443317,40.221372265457106,39.854856059672443,39.490800926089321,39.129206864707754,38.770073875527736,38.413401958549258,38.059191113772322,37.707441341196947,37.358152640823114,37.011325012650836,36.666958456680106,36.325052972910918,35.98560856134327,35.648625221977177,35.31410295481264,34.982041759849636,34.652441637088195,34.325302586528288,34.000624608169936,33.678407702013132,33.358651868057876,33.041357106304169,32.726523416752002,32.414150799401391,32.104239254252327,31.796788781304805,31.491799380558838,31.189271052014412,30.889203795671541,30.591597611530215,30.296452499590437,30.0037684598522,29.713545492315518,29.425783596980388,29.140482773846802,28.857643022914758,28.577264344184265,28.299346737655327,28.023890203327923,27.750894741202078,27.480360351277781,27.212287033555029,26.946674788033818,26.683523614714161,26.422833513596057,26.16460448467949,25.908836527964478,25.655529643451015,25.404683831139099,25.156299091028725,24.910375423119902,24.666912827412631,24.425911303906901,24.187370852602726,23.951291473500095,23.717673166599013,23.486515931899476,23.25781976940149,23.031584679105052,22.807810661010155,22.586497715116813,22.36764584142502,22.151255039934775,21.937325310646067,21.725856653558917,21.516849068673316,21.310302555989253,21.106217115506745,20.904592747225784,20.705429451146369,20.508727227268505,20.314486075592189,20.122705996117421,19.933386988844195,19.746529053772523,19.5621321909024,19.380196400233814,19.200721681766787,19.023708035501304,18.849155461437373,18.67706395957498,18.507433529914142,18.340264172454855,18.175555887197106,18.013308674140912,17.853522533286267,17.696197464633169,17.541333468181609,17.388930543931608,17.238988691883154,17.091507912036239,16.946488204390882,16.803929568947069,16.663832005704805,16.526195514664082,16.391020095824917,16.258305749187297,16.128052474751218,16.000260272516694,15.874929142483715,15.752059084652288,15.631650099022409,15.51370218559406,15.398215344367276,15.285189575342038,15.174624878518348,15.066521253896209,14.960878701475615,14.857697221256569,14.756976813239056,14.658717477423107,14.562919213808705,14.469582022395848,14.378705903184542,14.290290856174785,14.204336881366572,14.120843978759897,14.03981214835478,13.961241390151212,13.885131704149195,13.811483090348723,13.740295548749799,13.671569079352423,13.605303682156581,13.541499357162301,13.480156104369563,13.421273923778379,13.364852815388744,13.310892779200657,13.259393815214111,13.210355923429105,13.163779103845663,13.119663356463761,13.078008681283414,13.038815078304609,13.002082547527358,12.967811088951649,12.936000702577481,12.906651388404867,12.879763146433802,12.855335976664286,12.833369879096324,12.813864853729903,12.796820900565031,12.782238019601692,12.770116210839916,12.760455474279688,12.753255809921008,12.748517217763876,12.746239697808292,12.746423250054256,12.749067874501748,12.754173571150808,12.761740340001417,12.771768181053574,12.784257094307272,12.799207079762525,12.816618137419319,12.836490267277654,12.858823469337544,12.88361774359899,12.910873090061976,12.940589508726518,12.972766999592601,13.007405562660232,13.044505197929396,13.084065905400131,13.126087685072406,13.170570536946229,13.217514461021601,13.266919457298521,13.318785525776988,13.37311266645699,13.429900879338554,13.489150164421666,13.550860521706326,13.615031951192528,13.681664452880284,13.750758026769574,13.822312672860427,13.896328391152821,13.97280518164677,14.05174304434226,14.133141979239305,14.217001986337891,14.303323065638018,14.392105217139701,14.483348440842931,14.577052736747717,14.673218104854044,14.771844545161926,14.872932057671349,14.976480642382299,15.082490299294818,15.190961028408893,15.301892829724508,15.415285703241665,15.531139648960377,15.649454666880644,15.770230757002437,15.893467919325786,16.019166153850691,16.147325460577136,16.277945839505136,16.411027290634692,16.546569813965775,16.684573409498412,16.825038077232591,16.967963817168339,17.113350629305614,17.261198513644459],"type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,60,113,1)","line":{"color":"rgba(0,60,113,1)"},"showscale":false},"textfont":{"color":"rgba(0,60,113,1)"},"error_y":{"color":"rgba(0,60,113,1)"},"error_x":{"color":"rgba(0,60,113,1)"},"line":{"color":"rgba(0,60,113,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly",".hideLegend":true},"evals":[],"jsHooks":[]}</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hyperplan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: 3D graphical representation of the linear regression model for the <code>auto</code> data.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="rem-discretization" class="proof remark">
<p><span class="proof-title"><em>Remark 4.2</em> (Discretization of continuous covariates). </span>Another option is to transform a continuous variable <span class="math inline">\(X\)</span> into a categorical variable by discretizing into bins and fitting a piecewise-linear function of <span class="math inline">\(X\)</span>. The prime example of such option is treating a Likert scale as a categorical variable. While this allows one to fit more flexible functional relations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, this comes at the cost of additional coefficients for the same estimation budget (fewer observations to estimate the effect of <span class="math inline">\(X\)</span> results in lower precision of the coefficients).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-auto-discre" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-auto-discre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-auto-discre-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-auto-discre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Piecewise-linear model for the fuel autonomy of cars as a function of motor power.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="categorical-covariates" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="categorical-covariates"><span class="header-section-number">4.1.5</span> Categorical covariates</h3>
<p>Dummies are variables (columns of explanatories from the model matrix) which only include <span class="math inline">\(-1\)</span>, <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> to give indicator of the level of groups. For a binary outcome, we can create a column that has entries <span class="math inline">\(1\)</span> for the treatment and <span class="math inline">\(0\)</span> for the control group.</p>
<div id="exm-moon" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9 (Linear models with a single binary variable)</strong></span> <span class="citation" data-cites="Moon.VanEpps:2023">Moon and VanEpps (<a href="references.html#ref-Moon.VanEpps:2023" role="doc-biblioref">2023</a>)</span> consider the impact of providing suggested amounts for donations to a charity (as opposed to an open-ended request). In Study 1, participants were given the chance of winning 25$ and giving part of this amount to charity.</p>
<p>Consider for example a linear model that includes the <code>amount</code> (in dollars, from 0 for people who did not donate, up to 25 dollars) as a function of <span class="math display">\[\begin{align*}\texttt{condition} = \begin{cases} 0 , &amp; \text{open-ended},\\
1, &amp; \text{suggested quantity}
\end{cases}
\end{align*}\]</span> The equation of the simple linear model that includes the binary variable <code>condition</code> is <span class="math display">\[\begin{align*}
\mathsf{E}(\texttt{amount} \mid \texttt{condition})&amp;= \beta_0 + \beta_1 \mathbf{1}_{\texttt{condition}=\texttt{quantity}}.
\\&amp;= \begin{cases}
\beta_0, &amp; \texttt{condition}=0, \\
\beta_0 + \beta_1 &amp; \texttt{condition}=1.
\end{cases}
\end{align*}\]</span> Let <span class="math inline">\(\mu_0\)</span> denote the theoretical average amount for the open-ended amount and <span class="math inline">\(\mu_1\)</span> that of participants of the treatment <code>quantity</code> group. A linear model that only contains a binary variable <span class="math inline">\(X\)</span> as regressor amounts to specifying a different mean for each of two groups: the average of the treatment group is <span class="math inline">\(\beta_0 + \beta_1 = \mu_1\)</span> and <span class="math inline">\(\beta_1=\mu_1-\mu_0\)</span> represents the difference between the average donation amount of people given <code>open-ended</code> amounts and those who are offered suggested amounts (<code>quantity</code>), including zeros for the amount of people who did not donate. The parametrization of the linear model with <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is in terms of pairwise differences relative to the baseline category and is particularly useful if we want to test for mean difference between the groups, as this amounts to testing <span class="math inline">\(\mathscr{H}_0: \beta_1=0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-donation-moon" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-donation-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-donation-moon-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-donation-moon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.6: Simple linear model for the <code>MV23_S1</code> data using the binary variable <code>condition</code> as explanatory even if the equation defines a line, only its values in <span class="math inline">\(0/1\)</span> are realistic.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Even if the linear model defines a line, the latter is only meaningful when evaluated at <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>; <a href="#fig-donation-moon" class="quarto-xref">Figure&nbsp;<span>4.6</span></a> shows it in addition to sample observations (jittered horizontally) and a density estimate for each condition. The colored dot represents the mean, which will coincide with the estimates.</p>
<p>It is clear that the data are heavily discretized, with lots of ties and zeros. However, given the sample size of 869 observations, we can easily draw conclusions in each group.</p>
</div>
<p>Let us consider categorical variables with <span class="math inline">\(K &gt; 2\)</span> levels, which in <strong>R</strong> are of class <code>factor</code>. The default parametrization for factors are in terms of treatment contrast: the reference level of the factor (by default, the first value in alphanumerical order) will be treated as the reference category and assimilated to the intercept. The software will then create a set of <span class="math inline">\(K-1\)</span> dummy variables for a factor with <span class="math inline">\(K\)</span> levels, each of which will have ones for the relevant value and zero otherwise.</p>
<div id="exm-baumann-dummies" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.10 (Dummy coding for categorical variables)</strong></span> Consider the <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> study presented in <a href="#exm-teaching-baumann" class="quarto-xref">Example&nbsp;<span>4.2</span></a>, where we only include <code>group</code> as explanatory variable. The data are ordered by group: the first 22 observations are for group <code>DR</code>, the 22 next ones for group <code>DRTA</code> and the last 22 for <code>TA</code>. If we fit a model with <code>group</code> as categorical variables</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(BSJ92<span class="sc">$</span>group) <span class="co"># Check that group is a factor</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] "factor"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(BSJ92<span class="sc">$</span>group) <span class="co"># First level shown is reference</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] "DR"   "DRTA" "TA"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print part of the model matrix </span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># (three individuals from different groups)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">model.matrix</span>(<span class="sc">~</span> group, <span class="at">data =</span> BSJ92)[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">23</span>,<span class="dv">47</span>),]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    (Intercept) groupDRTA groupTA</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1            1         0       0</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 23           1         1       0</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 47           1         0       1</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with levels of factors recorded</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>BSJ92<span class="sc">$</span>group[<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">23</span>,<span class="dv">47</span>)]</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] DR   DRTA TA  </span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Levels: DR DRTA TA</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The mean model specification is <span class="math display">\[\mathsf{E}(Y \mid \texttt{group})= \beta_0 + \beta_1\mathbf{1}_{\texttt{group}=\texttt{DRTA}} + \beta_2\mathbf{1}_{\texttt{group}=\texttt{TA}}.\]</span> Since the variable <code>group</code> is categorical with <span class="math inline">\(K=3\)</span> levels, we need <span class="math inline">\(K-1 = 2\)</span> dummy explanatories to include the effect and obtain one average per group. With the default parametrization, we obtain</p>
<ul>
<li><span class="math inline">\(\mathbf{1}_{\texttt{group}=\texttt{DRTA}}=1\)</span> if <code>group=DRTA</code> and zero otherwise.</li>
<li><span class="math inline">\(\mathbf{1}_{\texttt{group}=\texttt{TA}}=1\)</span> if <code>group=TA</code> and zero otherwise.</li>
</ul>
<p>Because the model includes an intercept and the model ultimately describes three group averages, we only need two additional variables. With the treatment parametrization, the group mean of the reference group equals the intercept coefficient, <span class="math inline">\(\mu_{\texttt{DR}}=\beta_0\)</span>,</p>
<div class="cell" data-layout-align="center">
<div id="tbl-dummies-tr" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dummies-tr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: Parametrization of dummies for a categorical variable with the default treatment contrasts.
</figcaption>
<div aria-describedby="tbl-dummies-tr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">groupDRTA</th>
<th style="text-align: right;">groupTA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DR</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">DRTA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>When <code>group</code>=<code>DR</code> (baseline), both indicator variables <code>groupDRTA</code> and <code>groupTA</code> are zero. The average in each group is <span class="math inline">\(\mu_{\texttt{DR}} = \beta_0\)</span>, <span class="math inline">\(\mu_{\texttt{DRTA}}=\beta_0 + \beta_1\)</span> and <span class="math inline">\(\mu_{\texttt{TA}} = \beta_0 + \beta_2\)</span>. We thus find that <span class="math inline">\(\beta_1\)</span> is the difference in mean between group <code>DRTA</code> and group <code>DR</code>, and similarly <span class="math inline">\(\beta_2=\mu_{\texttt{TA}}- \mu_{\texttt{DR}}\)</span>.</p>
</div>
<div id="rem-sumtozero" class="proof remark">
<p><span class="proof-title"><em>Remark 4.3</em> (Sum-to-zero constraints). </span>The parametrization discussed above, which is the default for the <code>lm</code> function, isn’t the only one available. We consider an alternative ones: rather than comparing each group mean with that of a baseline category, the default parametrization for analysis of variance models is in terms of sum-to-zero constraints, whereby the intercept is the equiweighted average of every group, and the parameters <span class="math inline">\(\beta_1, \ldots, \beta_{K-1}\)</span> are differences to this average.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">model.matrix</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span> group, </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> BSJ92, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">contrasts.arg =</span> <span class="fu">list</span>(<span class="at">group =</span> <span class="st">"contr.sum"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-sum2zero" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sum2zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.2: Parametrization of dummies for the sum-to-zero constraints for a categorical variable.
</figcaption>
<div aria-describedby="tbl-sum2zero-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">group1</th>
<th style="text-align: right;">group2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DR</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">DRTA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-1</td>
<td style="text-align: right;">-1</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>In the sum-to-zero constraint, we again only get two dummy variables, labelled <code>group1</code> and <code>group2</code>, along with the intercept. The value of <code>group1</code> is <span class="math inline">\(1\)</span> if <code>group=DR</code>, <span class="math inline">\(0\)</span> if <code>group=DRTA</code> and <span class="math inline">\(-1\)</span> if <code>group=TA</code>. Using the invariance property, we find <span class="math inline">\(\mu_{\texttt{DR}} = \beta_0 + \beta_1\)</span>, <span class="math inline">\(\mu_{\texttt{DRTA}}=\beta_0 + \beta_2\)</span> and <span class="math inline">\(\mu_{\texttt{TA}} = \beta_0 - \beta_1 - \beta_2\)</span> (more generally, the intercept minus the sum of all the other mean coefficients). Some algebraic manipulation reveals that <span class="math inline">\(\beta_0 = (\mu_{\texttt{DR}} +\mu_{\texttt{DRTA}}+\mu_{\texttt{TA}})/3\)</span>.</p>
<p>If we removed the intercept, then we could include three dummies for each treatment group and each parameter would correspond to the average. This isn’t recommended in <strong>R</strong> because the software treats models without the intercept differently and some output will be nonsensical (e.g., the coefficient of determination will be wrong).</p>
</div>
<div id="exm-gender-disparity" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.11 (Wage inequality in an American college)</strong></span> We consider a linear regression model for the <code>college</code> data that includes sex, academic rank, field of study and the number of years of service as explanatories.</p>
<p>The postulated model is <span class="math display">\[\begin{align*}
\texttt{salary} &amp;= \beta_0 + \beta_1 \texttt{sex}_{\texttt{woman}} +\beta_2 \texttt{field}_{\texttt{theoretical}} \\&amp;\quad +\beta_3 \texttt{rank}_{\texttt{associate}}
+\beta_4 \texttt{rank}_{\texttt{full}}  +\beta_5 \texttt{service} + \varepsilon.
\end{align*}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Estimated coefficients of the linear model for the <span class="math inline">\(\texttt{college}\)</span> (in USD, rounded to the nearest dollar).</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_3\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_4\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{\beta}_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">86596</td>
<td style="text-align: right;">-4771</td>
<td style="text-align: right;">-13473</td>
<td style="text-align: right;">14560</td>
<td style="text-align: right;">49160</td>
<td style="text-align: right;">-89</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The interpretation of the coefficients is as follows:</p>
<ul>
<li>The estimated intercept is <span class="math inline">\(\widehat{\beta}_0=86596\)</span> dollars; it corresponds to the mean salary of men assistant professors who just started the job and works in an applied domain.</li>
<li>everything else being equal (same field, academic rank, and number of years of service), the estimated salary difference between a woman and is estimated at <span class="math inline">\(\widehat{\beta}_1=-4771\)</span> dollars.</li>
<li><em>ceteris paribus</em>, the salary difference between a professor working in a theoretical field and one working in an applied field is <span class="math inline">\(\beta_2\)</span> dollars: our estimate of this difference is <span class="math inline">\(-13473\)</span> dollars, meaning applied pays more than theoretical.</li>
<li><em>ceteris paribus</em>, the estimated mean salary difference between associate and assistant professors is <span class="math inline">\(\widehat{\beta}_3=14560\)</span> dollars.</li>
<li><em>ceteris paribus</em>, the estimated mean salary difference between full and assistant professors is <span class="math inline">\(\widehat{\beta}_4=49160\)</span> dollars.</li>
<li>within the same academic rank, every additional year of service leads to a mean salary increase of <span class="math inline">\(\widehat{\beta}_5=-89\)</span> dollars.</li>
</ul>
</div>
<div id="exm-interpretation1" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.12 (Parameter interpretation for analysis of covariance)</strong></span> We consider a pre-post model for the error detection task test of <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span>. We fit a linear model with the pre-test score and the experimental condition.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(BSJ92, <span class="at">package =</span> <span class="st">"hecedsm"</span>) <span class="co">#load data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(BSJ92) <span class="co"># Check that categorical variables are factors</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; tibble [66 × 6] (S3: tbl_df/tbl/data.frame)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ group    : Factor w/ 3 levels "DR","DRTA","TA": 1 1 1 1 1 1 1 1 1 1 ...</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ pretest1 : int [1:66] 4 6 9 12 16 15 14 12 12 8 ...</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ pretest2 : int [1:66] 3 5 4 6 5 13 8 7 3 8 ...</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ posttest1: int [1:66] 5 9 5 8 10 9 12 5 8 7 ...</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ posttest2: int [1:66] 4 5 3 5 9 8 5 5 7 7 ...</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ posttest3: int [1:66] 41 41 43 46 46 45 45 32 33 39 ...</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check summary statistics for posttest1</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>BSJ92 <span class="sc">|&gt;</span> <span class="co"># compute group average</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>   <span class="fu">group_by</span>(group) <span class="sc">|&gt;</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>   <span class="fu">summarize</span>(<span class="at">mean_pre =</span> <span class="fu">mean</span>(pretest1),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>             <span class="at">mean_post =</span> <span class="fu">mean</span>(posttest1),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>             <span class="at">diff_impr =</span> mean_post <span class="sc">-</span> mean_pre)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 3 × 4</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   group mean_pre mean_post diff_impr</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   &lt;fct&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 DR       10.5       6.68   -3.82  </span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 DRTA      9.73      9.77    0.0455</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 TA        9.14      7.77   -1.36</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the ANOVA for the difference</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>   posttest1 <span class="sc">-</span> pretest1 <span class="sc">~</span> group,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>   <span class="at">data =</span> BSJ92)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod1) <span class="co"># Mean model coefficients</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)   groupDRTA     groupTA </span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       -3.82        3.86        2.45</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear regression</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>linmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>   posttest1 <span class="sc">~</span> pretest1 <span class="sc">+</span> group,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>   <span class="at">data =</span> BSJ92 <span class="sc">|&gt;</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>      dplyr<span class="sc">::</span><span class="fu">mutate</span>( <span class="co"># mean-center pretest result</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        <span class="at">pretest1 =</span> pretest1 <span class="sc">-</span> <span class="fu">mean</span>(pretest1)))</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod2) <span class="co"># Mean model coefficients</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)    pretest1   groupDRTA     groupTA </span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       6.188       0.693       3.627       2.036</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the ANOVA model for the group as a function of the improvement and using the default treatment parameterization,, the intercept is the average of post-test minus pre-test score for group <code>DR</code>, and the other two coefficients are the difference between groups <code>DRTA</code> and <code>DR</code>, and the difference between groups <code>TA</code> and <code>DR</code>. Thus, the higher average improvement is for <code>DRTA</code>, then <code>TA</code>, then the baseline <code>DR</code>.</p>
<p>Consider next a linear model in which we allow the post-test score to be a linear function of the pre-test. We find that, for each point score on the pre-test, the post-test score increases by 0.693 marks regardless of the group. The <code>DRTA</code> group (respectively <code>TA</code>) has an average, ceteris paribus, that is 3.627 (respectively 2.036) points higher than that of the baseline group <code>DR</code> for two people with the same pre-test score. If we center the continuous covariate <code>pretest1</code>, the intercept <span class="math inline">\(\beta_0\)</span> is the average post-test score of a person from the <code>DR</code> group who scored the overall average of all 66 students in the pre-test.</p>
</div>
</section>
</section>
<section id="parameter-estimation" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">4.2</span> Parameter estimation</h2>
<p>The linear model includes <span class="math inline">\(p+1\)</span> mean parameters and a standard deviation <span class="math inline">\(\sigma\)</span>, which is assumed constant for all observations. Given a design or model matrix <span class="math inline">\(\mathbf{X}\)</span> and a linear model formulation <span class="math inline">\(\mathsf{E}(Y_i) = \mathbf{x}_i\boldsymbol{\beta}\)</span>, we can try to find the parameter vector <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^{p+1}\)</span> that minimizes the mean squared error, i.e., the average squared vertical distance between the fitted values <span class="math inline">\(\widehat{y}_i=\mathbf{x}_i\widehat{\boldsymbol{\beta}}\)</span> and the observations <span class="math inline">\(y_i\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-vertdist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-vertdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-vertdist-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-vertdist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.7: Ordinary residuals <span class="math inline">\(e_i\)</span> (vertical vectors) added to the regression line in the scatter <span class="math inline">\((x, y)\)</span> (left) and the fit of response <span class="math inline">\(y_i\)</span> against fitted values <span class="math inline">\(\widehat{y}_i\)</span>. The ordinary least squares line minimizes the average squared length of the ordinary residuals.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="prp-ols-mle" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.1 (Ordinary least squares)</strong></span> Consider the optimization problem <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}}&amp;=\mathrm{arg min}_{\boldsymbol{\beta} \in \mathbb{R}^{p+1}}\sum_{i=1}^n (y_i-\mathbf{x}_i\boldsymbol{\beta})^2
\\&amp;=(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta}).
\end{align*}\]</span> We can compute the derivative of the right hand side with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, set it to zero and solve for <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>, <span class="math display">\[\begin{align*}
\mathbf{0}_n&amp;=\frac{\partial}{\partial\boldsymbol{\beta}}(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\\
\\&amp;=\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\frac{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}\\
\\&amp;=2\mathbf{X}^\top (\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})
\end{align*}\]</span> using the <a href="http://www.stat.rice.edu/~dobelman/notes_papers/math/Matrix.Calculus.AppD.pdf">chain rule</a>. Distributing the terms leads to the so-called <em>normal equation</em> <span class="math display">\[\begin{align*}
\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}&amp;=\mathbf{X}^\top \boldsymbol{y}.
\end{align*}\]</span> If the <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is full-rank, meaning that it’s columns are not linear combinations of one another, the quadratic form <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible and we obtain the solution to the least square problems, <span id="eq-ols"><span class="math display">\[
\widehat{\boldsymbol{\beta}} = \left(\mathbf{X}^\top \mathbf{X}\right)^{-1}\mathbf{X}^\top \boldsymbol{y}.
\tag{4.1}\]</span></span> This is the <strong>ordinary least squares estimator</strong> (OLS). The explicit solution means that no numerical optimization is needed for linear models.</p>
</div>
<p>We could also consider maximum likelihood estimation. <a href="#prp-ols-mle" class="quarto-xref">Proposition&nbsp;<span>4.1</span></a> shows that, assuming normality of the errors, the least square estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> coincide with the maximum likelihood estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<div id="prp-mle-normal-linmod" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.2 (Maximum likelihood estimation of the normal linear model)</strong></span> The linear regression model specifies that the observations <span class="math inline">\(Y_i \sim \mathsf{normal}(\mathbf{x}_i\boldsymbol{\beta}, \sigma^2)\)</span> are independent. The linear model has <span class="math inline">\(p+2\)</span> parameters (<span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span>) and the log likelihood is, abstracting from constant terms, <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\beta}, \sigma)&amp;\propto-\frac{n}{2} \ln (\sigma^2) -\frac{1}{2\sigma^2}\left\{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})\right\}^2.
\end{align*}\]</span> Maximizing the log likelihood with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span> is equivalent to minimizing the sum of squared errors <span class="math inline">\(\sum_{i=1}^n (y_i - \mathbf{x}_i\boldsymbol{\beta})^2\)</span>, regardless of the value of <span class="math inline">\(\sigma\)</span>, and we recover the OLS estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>. The maximum likelihood estimator of the variance <span class="math inline">\(\widehat{\sigma}^2\)</span> is thus <span class="math display">\[\begin{align*}
\widehat{\sigma}^2=\mathrm{arg max}_{\sigma^2} \ell(\widehat{\boldsymbol{\beta}}, \sigma^2).
\end{align*}\]</span> The profile log likelihood for <span class="math inline">\(\sigma^2\)</span>, excluding constant terms that don’t depend on <span class="math inline">\(\sigma^2\)</span>, is <span class="math display">\[\begin{align*}
\ell_{\mathrm{p}}(\sigma^2)
&amp;\propto-\frac{1}{2}\left\{n\ln\sigma^2+\frac{1}{\sigma^2}(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{\beta}})\right\}.
\end{align*}\]</span> Differentiating each term with respect to <span class="math inline">\(\sigma^2\)</span> and setting the gradient equal to zero yields the maximum likelihood estimator <span class="math display">\[\begin{align*}
\widehat{\sigma}^2&amp;=\frac{1}{n}(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})^\top(\boldsymbol{Y}-\mathbf{X}\hat{\boldsymbol{\beta}})\\&amp;= \frac{1}{n} \sum_{i=1}^n (y_i - \mathbf{x}_i\widehat{\boldsymbol{\beta}})^2\\&amp;= \frac{\mathsf{SS}_e}{n};
\end{align*}\]</span> where <span class="math inline">\(\mathsf{SS}_e\)</span> is the sum of squared residuals. The usual unbiased estimator of <span class="math inline">\(\sigma^2\)</span> calculated by software is <span class="math inline">\(S^2=\mathsf{SS}_e/(n-p-1)\)</span>, where the denominator is the sample size <span class="math inline">\(n\)</span> minus the number of mean parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(p+1\)</span>.</p>
</div>
<div id="prp-info-normal" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.3 (Information matrix for normal linear regression models)</strong></span> The entries of the observed information matrix of the normal linear model are <span class="math display">\[\begin{align*}
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \boldsymbol{\beta}^\top} &amp;= \frac{1}{\sigma^2} \frac{\partial \mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^\top} =  \frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial \boldsymbol{\beta}\partial \sigma^2} &amp;=- \frac{\mathbf{X}^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^4}\\
-\frac{\partial^2 \ell(\boldsymbol{\beta}, \sigma^2)}{\partial (\sigma^2)^2} &amp;= -\frac{n}{2\sigma^4} + \frac{(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})^\top(\boldsymbol{y}-\mathbf{X}\boldsymbol{\beta})}{\sigma^6}.
\end{align*}\]</span> If we evaluate the observed information at the MLE, we get <span class="math display">\[\begin{align*}
j(\widehat{\boldsymbol{\beta}}, \widehat{\sigma^2}) =
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\widehat{\sigma^2}} &amp; \boldsymbol{0}_{p+1} \\  \boldsymbol{0}_{p+1}^\top &amp; \frac{n}{2\widehat{\sigma^4}}
\end{pmatrix}
\end{align*}\]</span> since <span class="math inline">\(\widehat{\sigma}^2=\mathsf{SS}_e/n\)</span> and the residuals are orthogonal to the model matrix. Since <span class="math inline">\(\mathsf{E}(Y \mid \mathbf{X})=\mathbf{X}\boldsymbol{\beta}\)</span>, the Fisher information is <span class="math display">\[\begin{align*}
i(\boldsymbol{\beta}, \sigma^2) =
\begin{pmatrix}
\frac{\mathbf{X}^\top\mathbf{X}}{\sigma^2} &amp; \boldsymbol{0}_{p+1} \\  \boldsymbol{0}_{p+1}^\top &amp; \frac{n}{2\sigma^4}
\end{pmatrix}
\end{align*}\]</span> Since zero off-correlations in normal models amount to independence, the MLE for <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> are independent. Provided the <span class="math inline">\((p+1)\)</span> square matrix <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span> is invertible, the large-sample variance of the ordinary least squares estimator is <span class="math inline">\(\sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\)</span> and that of the MLE of the variance is <span class="math inline">\(2\sigma^4/n\)</span>.</p>
</div>
<div id="prp-software" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.4 (Fitting linear models with software)</strong></span> Although we could build the model matrix ourselves and use the least square formula of <a href="#eq-ols" class="quarto-xref">Equation&nbsp;<span>4.1</span></a>, the numerical routines implemented in software are typically better behaved. The <code>lm</code> function in <strong>R</strong> fits <strong>linear models</strong>, as does <code>glm</code> with the default arguments. Objects of class <code>lm</code> have multiple methods allow you to extract specific objects from <code>lm</code> objects. For example, the functions <code>coef</code>, <code>resid</code>, <code>fitted</code>, <code>model.matrix</code> will return the coefficients <span class="math inline">\(\widehat{\boldsymbol{\beta}},\)</span> the ordinary residuals <span class="math inline">\(\boldsymbol{e},\)</span> the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and the model matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(BSJ92, <span class="at">package =</span> <span class="st">"hecedsm"</span>) <span class="co">#load data</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(BSJ92) <span class="co"># Check that categorical variables are factors</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the linear regression</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>linmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(posttest1 <span class="sc">~</span> pretest1 <span class="sc">+</span> group, </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">data =</span> BSJ92)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(linmod) <span class="co"># beta coefficients</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>vcov_beta <span class="ot">&lt;-</span> <span class="fu">vcov</span>(linmod) <span class="co"># Covariance matrix of betas</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod) <span class="co"># summary table</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>beta_ci <span class="ot">&lt;-</span> <span class="fu">confint</span>(linmod) <span class="co"># Wald confidence intervals for betas</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(linmod) <span class="co"># fitted values</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(linmod) <span class="co"># ordinary residuals</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Check OLS formula</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(linmod) <span class="co"># model matrix</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> college<span class="sc">$</span>salary</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="fu">isTRUE</span>(<span class="fu">all.equal</span>(</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y),</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.numeric</span>(<span class="fu">coef</span>(linmod))</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>summary</code> method is arguably the most useful: it will print mean parameter estimates along with standard errors, <span class="math inline">\(t\)</span> values for the Wald test of the hypothesis <span class="math inline">\(\mathscr{H}_0: \beta_i=0\)</span> and the associated <span class="math inline">\(p\)</span>-values. Other statistics and information about the sample size, the degrees of freedom, etc., are given at the bottom of the table. Note that the <code>lm</code> function uses the unbiased estimator of the variance <span class="math inline">\(\sigma^2\)</span>, denoted <span class="math inline">\(S^2\)</span> in this chapter.</p>
</div>
</section>
<section id="sec-predictions-lm" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-predictions-lm"><span class="header-section-number">4.3</span> Predictions</h2>
<p>When we compute least square estimates, we obtain fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> as <span class="math inline">\(\mathbf{X}\widehat{\boldsymbol{\beta}}\)</span>, where <span class="math inline">\(\mathbf{X}\)</span> denotes the <span class="math inline">\(n \times (p+1)\)</span> model matrix. We can also obtain an estimate of the mean surface for any new row vector of explanatories <span class="math inline">\(\mathbf{x}^* = (1, x^*_1, \ldots, x^*_p)\)</span>, knowing that <span class="math inline">\(\mathsf{E}(Y \mid \mathbf{x}^*)=\mathbf{x}^*\boldsymbol{\beta}\)</span>, by replacing the unknown coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span> by our estimates <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>. This yields the best linear unbiased predictor of the mean.</p>
<p>If we want to predict the value of a new observation, say <span class="math inline">\(Y^*\)</span>, with known explanatories <span class="math inline">\(\mathbf{x}^*\)</span>, the prediction will thus be <span class="math inline">\(\widehat{y}^* = \mathbf{x}^*\widehat{\boldsymbol{\beta}}\)</span> because <span class="math display">\[\begin{align*}
\mathsf{E}(\widehat{Y}^* \mid \mathbf{X}, \mathbf{x}^*) = \mathsf{E}(\mathbf{x}^*\widehat{\boldsymbol{\beta}}\mid \mathbf{X}, \mathbf{x}^*) = \mathbf{x}^*\boldsymbol{\beta}.
\end{align*}\]</span> However, individual observations vary more than averages (which are themselves based on numerous observations). Intuitively, this is due to the added uncertainty of the error term appearing in the model equation: the variability of new predictions is the sum of uncertainty due to the estimators (based on random data) and the intrinsic variance of the observations assuming the new observation is independent of those used to estimate the coefficients, <span class="math display">\[\begin{align*}
\mathsf{Va}(Y^*-\widehat{Y}^* \mid \mathbf{X}, \mathbf{x}^*) &amp;= \mathsf{Va}(Y^*  - \mathbf{x}^*\widehat{\boldsymbol{\beta}} \mid \mathbf{X}, \mathbf{x}^*)
\\&amp;=\mathsf{Va}(Y^* \mid \mathbf{X}, \mathbf{x}^*) + \mathsf{Va}(\mathbf{x}^*\widehat{\boldsymbol{\beta}} \mid \mathbf{X}, \mathbf{x}^*)
\\&amp; = \sigma^2\mathbf{x}^{*\vphantom{\top}}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}^{*\top} + \sigma^2,
\end{align*}\]</span> and we can base the prediction interval on the Student distribution, as <span class="math display">\[\begin{align*}
\frac{Y^*-\mathrm{x}^*\widehat{\boldsymbol{\beta}}}{\sqrt{S^2\{1+\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}\}}}\sim \mathsf{Student}(n-p-1).
\end{align*}\]</span> We obtain <span class="math inline">\(1-\alpha\)</span> <strong>prediction interval</strong> for <span class="math inline">\(Y^*\)</span> by inverting the test statistic, <span class="math display">\[\begin{align*}
\mathrm{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\{1+\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}\}}.
\end{align*}\]</span> Similar calculations yield the formula for pointwise <strong>confidence intervals for the mean</strong>, <span class="math display">\[\begin{align*}
\mathrm{x}^*\widehat{\boldsymbol{\beta}}\pm \mathfrak{t}_{n-p-1}(\alpha/2)\sqrt{S^2\mathrm{x}^*(\mathbf{X}^\top\mathbf{X})^{-1}\mathrm{x}^{*\top}}.
\end{align*}\]</span> The two differ only because of the additional variance of individual observations.</p>
<div id="exm-sokolova-pred" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.13 (Prediction for simple linear regression)</strong></span> Consider the data of <a href="#exm-sokolova" class="quarto-xref">Example&nbsp;<span>4.5</span></a>. We fit a simple linear regression of the form <span class="math inline">\(\texttt{pef} = \beta_0 + \beta_1 \texttt{proportion} + \varepsilon\)</span> with <span class="math inline">\(\varepsilon \sim \mathsf{normal}(0,\sigma^2)\)</span> and observations assumed independent.</p>
<p><a href="#fig-predinterval" class="quarto-xref">Figure&nbsp;<span>4.8</span></a> shows pointwise uncertainty bands for a simple linear regression of the data <span class="citation" data-cites="Sokolova:2023">Sokolova, Krishna, and Döring (<a href="references.html#ref-Sokolova:2023" role="doc-biblioref">2023</a>)</span> as a function of the paper to plastic <code>proportion</code>, with larger values indicating more spurious paper wrapping. The model is not accounting for the fact that our response arises from a bounded discrete distribution with integer values ranging from 1 to 7, and that the ratios tested in the experiment are 0 (no paper), 0.5, 1 and 2. The middle line gives the prediction of individuals as we vary the proportion paper/plastic. Looking at the formulas for the confidence and prediction intervals, it is clear that the bands are not linear (we consider the square root of a function that involves the predictors), but it is not obvious that the uncertainty increases as you move away from the average of the predictors.</p>
<p>This is more easily seen by replicating the potential curves that could have happened with different data: <a href="#fig-predinterval" class="quarto-xref">Figure&nbsp;<span>4.8</span></a> shows generated new potential slopes from the asymptotic normal distribution of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> estimators. The hyperbolic shape is not surprising: we are basically tilting curves from the average <code>pef</code>/<code>proportion</code>, and they have higher potential from deviating the further we are from the average in each direction. The prediction intervals (pale grey) are very large and essentially cover the whole scope of potential values for the perceived environmental friendliness Likert scale, except for a couple of observations. By contrast, the confidence intervals for the mean are quite narrow, as a result of the large sample size. We can see also that the curves do not deviate much from them.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-predinterval" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-predinterval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-predinterval-1.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-predinterval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.8: Prediction with prediction intervals (left) and confidence intervals for the mean (right) for the simple linear regression of perceived environmental friendliness (<code>pef</code>) as a function of the <code>proportion</code> of paper to plastic, with horizontally jittered observations. The plot shows predictions along with pointwise 95% confidence intervals of the mean and the individual predictions. The <span class="math inline">\(y\)</span>-axis has been truncated.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In <strong>R</strong>, the generic <code>predict</code> takes as input a model and a <code>newdata</code> argument contains a data frame with the same structure as the original data used to fit the model (or at least the columns of explanatory variables used).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(SKD23_S2A, <span class="at">package =</span> <span class="st">"hecedsm"</span>) <span class="co"># load data</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>lm_simple <span class="ot">&lt;-</span> <span class="fu">lm</span>(pef <span class="sc">~</span> proportion, <span class="at">data =</span> SKD23_S2A) <span class="co"># fit simple linear regression</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm_simple, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">proportion =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">"prediction"</span>) <span class="co"># prediction intervals</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(lm_simple, </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">proportion =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)),</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">"confidence"</span>) <span class="co"># confidence for mean</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tbl-predints-soko" class="quarto-layout-panel anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-predints-soko-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.3: Predictions with prediction intervals (left) and confidence intervals for the mean (right).
</figcaption>
<div aria-describedby="tbl-predints-soko-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="do-not-create-environment cell caption-top table">
<caption>Prediction intervals</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><code>proportion</code></th>
<th style="text-align: center;">prediction</th>
<th style="text-align: center;">lower</th>
<th style="text-align: center;">upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">-0.168</td>
<td style="text-align: center;">4.98</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">5.24</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">5.51</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">6.04</td>
</tr>
</tbody>
</table>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<table class="do-not-create-environment cell caption-top table">
<caption>Confidence intervals for the mean</caption>
<thead>
<tr class="header">
<th style="text-align: center;">mean</th>
<th style="text-align: center;">lower CI</th>
<th style="text-align: center;">upper CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">2.27</td>
<td style="text-align: center;">2.55</td>
</tr>
<tr class="even">
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">2.57</td>
<td style="text-align: center;">2.77</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2.93</td>
<td style="text-align: center;">2.84</td>
<td style="text-align: center;">3.02</td>
</tr>
<tr class="even">
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">3.30</td>
<td style="text-align: center;">3.62</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</figure>
</div>
</div>
<div id="rem-notation" class="proof remark">
<p><span class="proof-title"><em>Remark 4.4</em> (Notation). </span>It is important to distinguish the equation of the stochastic model, expressed in terms of random variables <span class="math inline">\(\mathsf{E}(Y_i \mid \boldsymbol{x}_i) = \mathbf{x}_i\boldsymbol{\beta} + \varepsilon_i\)</span> and the equation of the fitted values or predictions, <span class="math display">\[\begin{align*}
\widehat{\mathsf{E}(Y_i \mid \mathbf{x}_i)} \mathbf{x}_i\widehat{\boldsymbol{\beta}}
\end{align*}\]</span> The prediction does not involve unknown error terms.</p>
</div>
</section>
<section id="hypothesis-testing" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="hypothesis-testing"><span class="header-section-number">4.4</span> Hypothesis testing</h2>
<p>Hypothesis testing in linear models and analysis of variance proceeds as usual: we compare two nested models, one of which (the null model) is a simplification of a more complex one obtaining by imposing restrictions on the mean coefficients.</p>
<p>Of particular interest are tests of restrictions for components of <span class="math inline">\(\boldsymbol{\beta}\)</span>. The large sample properties of the maximum likelihood estimator imply that <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{\beta}} \stackrel{\cdot}{\sim}\mathsf{normal}_{p+1}\left\{\boldsymbol{\beta}, \sigma^2(\mathbf{X}^\top\mathbf{X})^{-1}\right\}
\end{align*}\]</span> for sufficiently large sample size, and this result is exact for normal data. One can thus easily estimate the standard errors from the matrix upon replacing <span class="math inline">\(\sigma^2\)</span> by an estimator. With normal data, one can show that <span class="math inline">\(\mathsf{SS}_e \sim \sigma^2\chi^2_{n-p-1}\)</span> and <span class="math inline">\(\mathsf{SS}_e\)</span> is independent of <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>
<p>In an inferential setting, it’s often important to test whether the effect of an explanatory variable is significant: if <span class="math inline">\(x_j\)</span> is binary or continuous, the test for <span class="math inline">\(\mathscr{H}_0: \beta_j=0\)</span> corresponds to a null marginal effect for <span class="math inline">\(x_j\)</span>. The null model is a linear regression in which we remove the <span class="math inline">\((j+1)\)</span>st column of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<div id="prp-wald" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.5 (Wald tests in linear regression)</strong></span> Recall that the Wald test statistic for the hypothesis <span class="math inline">\(\mathscr{H}_0: \beta_j=b\)</span> is <span class="math display">\[W = \frac{\widehat{\beta}_j - b}{\mathsf{se}(\widehat{\beta}_j)}.\]</span> The Wald test statistic is reported by most software for the hypothesis <span class="math inline">\(b=0\)</span>. Since <span class="math inline">\(\mathsf{Var}(\widehat{\beta}_j) = \sigma^2 [(\mathbf{X}^\top\mathbf{X})^{-1}]_{j,j}\)</span>, we can estimate standard error from <span class="math inline">\(S^2\)</span> and derive that the distribution of <span class="math inline">\(W\)</span> under the null hypothesis is <span class="math inline">\(\mathsf{Student}(n-p-1)\)</span>. This explains the terminology “<span class="math inline">\(t\)</span> values” in the <code>summary</code> table. In addition to coefficient estimates, it is possible to obtain Wald-based confidence intervals for <span class="math inline">\(\beta_j\)</span>, which are the usual <span class="math inline">\(\widehat{\beta}_j \pm \mathfrak{t}_{n-p-1,\alpha/2} \mathsf{se}(\widehat{\beta}_j)\)</span>, with <span class="math inline">\(\mathfrak{t}_{n-p-1,\alpha/2}\)</span> denoting the <span class="math inline">\(1-\alpha/2\)</span> quantile of the <span class="math inline">\(\mathsf{Student}({n-p-1})\)</span> distribution.</p>
</div>
<div id="exm-sokolova-simple-ttest" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.14</strong></span> Consider the data from <a href="#exm-sokolova" class="quarto-xref">Example&nbsp;<span>4.5</span></a>. If we fit againt the simple linear regression model, we can extract the <span class="math inline">\(p\)</span>-values for the Wald or <span class="math inline">\(t\)</span>-tests. The test for the intercept is of no interest since data are measured on a scale from 1 to 7, so the mean response when <code>proportion=0</code> cannot be zero. The coefficient for <code>proportion</code> suggests a trend of 0.5 point per unit ratio, and this is significantly different from zero, indicating that the <code>pef</code> score changes with the paper to plastic ratio.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm_simple)<span class="sc">$</span>coefficients <span class="co"># t-tests (Wald) for beta=0 with p-values</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value  Pr(&gt;|t|)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)    2.407     0.0723   33.31 2.56e-153</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; proportion     0.526     0.0618    8.51  8.40e-17</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lm_simple) <span class="co"># confidence intervals for betas</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             2.5 % 97.5 %</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 2.266  2.549</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; proportion  0.405  0.648</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>For categorical variables with more than two levels, testing if <span class="math inline">\(\beta_j=0\)</span> is typically not of interest because the coefficient represents the difference between the category <span class="math inline">\(x_j\)</span> and the baseline with the treatment contrast parametrization: these two categories could have a small difference, but the categorical variable as a whole may still be a useful predictor given the other explanatories. The hypothesis of zero contrast is awkward because it implies a null model in which selected categories are merged, but then depends on the reference category. Rather, we wish to compare a model in which all variables are present with one in which the categorical explanatory is omitted.</p>
<div id="prp-ftest" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.6 (<em>F</em>-tests for comparison of nested linear models)</strong></span> Consider the <em>full</em> linear model which contains <span class="math inline">\(p\)</span> predictors, <span class="math display">\[\begin{align*}
\mathbb{M}_1: Y=\beta_0+\beta_1 x_1 + \cdots + \beta_g x_g + \beta_{k+1}x_{k+1} + \ldots + \beta_p x_p + \varepsilon.
\end{align*}\]</span> Suppose without loss of generality that we want to test <span class="math inline">\(\mathscr{H}_0: \beta_{k+1}=\beta_{k+2}=\cdots=\beta_p=0\)</span> (one could permute columns of the model matrix to achieve this configuration). The global hypothesis specifies that <span class="math inline">\((p-k)\)</span> of the <span class="math inline">\(\beta\)</span> parameters are zero. The <em>restricted model</em> corresponding to the null hypothesis contains only the covariates for which <span class="math inline">\(\beta_j \neq 0\)</span>, <span class="math display">\[\begin{align*}
\mathbb{M}_0: Y=\beta_0+\beta_1 x_1 + \ldots + \beta_k x_k + \varepsilon.
\end{align*}\]</span> Let <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_1)\)</span> be the residuals sum of squares for model <span class="math inline">\(\mathbb{M}_1\)</span>, <span class="math display">\[\begin{align*}
\mathsf{SS}_e(\mathbb{M}_1)=\sum_{i=1}^n (Y_i-\widehat{Y}_i^{\mathbb{M}_1})^2,
\end{align*}\]</span> where <span class="math inline">\(\widehat{Y}_i^{\mathbb{M}_1}\)</span> is the <span class="math inline">\(i\)</span>th fitted value from <span class="math inline">\(\mathbb{M}_1\)</span>. Similarly define <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_0)\)</span> for the residuals sum of square of <span class="math inline">\(\mathbb{M}_0\)</span>. Clearly, <span class="math inline">\(\mathsf{SS}_e(\mathbb{M}_0) \geq \mathsf{SS}_e(\mathbb{M}_1)\)</span> (why?)</p>
<p>The <span class="math inline">\(F\)</span>-test statistic is <span class="math display">\[\begin{align*}
F=\frac{\{\mathsf{SS}_e(\mathbb{M}_0)-\mathsf{SS}_e(\mathbb{M}_1)\}/(p-k)}{\mathsf{SS}_e(\mathbb{M}_1)/(n-p-1)}.
\end{align*}\]</span> Under <span class="math inline">\(\mathscr{H}_0\)</span>, the <span class="math inline">\(F\)</span> statistic follows a Fisher distribution (<a href="introduction.html#def-Fdist" class="quarto-xref">Definition&nbsp;<span>1.15</span></a>) with <span class="math inline">\((p-k)\)</span> and <span class="math inline">\((n-p-1)\)</span> degrees of freedom, <span class="math inline">\(\mathsf{Fisher}(p-k, n-p-1)\)</span> — <span class="math inline">\(p-k\)</span> is the number of restrictions, and <span class="math inline">\(n-p-1\)</span> is sample size minus the number of coefficients for the mean of <span class="math inline">\(\mathbb{M}_1\)</span>.</p>
</div>
<p>It turns out that both <span class="math inline">\(F\)</span> and <span class="math inline">\(t\)</span>-statistics are equivalent for testing a single coefficient <span class="math inline">\(\beta_j\)</span>: the <span class="math inline">\(F\)</span>-statistic is the square of the Wald statistic and they lead to the same inference — the <span class="math inline">\(p\)</span>-value for the test are identical. While it may get reported in tables, the test for <span class="math inline">\(\beta_0=0\)</span> is not of interest; we keep the intercept merely to centre the residuals.</p>
<div id="rem-lrtvsF" class="proof remark">
<p><span class="proof-title"><em>Remark 4.5</em> (<em>F</em>-tests versus likelihood ratio tests). </span>For normal linear regression, the likelihood ratio test for comparing models <span class="math inline">\(\mathbb{M}_1\)</span> and <span class="math inline">\(\mathbb{M}_0\)</span> is a function of the sum of squared residuals: the usual formula simplifies to <span class="math display">\[\begin{align*}
R &amp;= n\ln\{\mathsf{SS}_e(\mathbb{M}_0)/\mathsf{SS}_e(\mathbb{M}_1)\}\\
&amp;= n \ln \left( 1+ \frac{p-k}{n-p-1}F\right)
\end{align*}\]</span> Both the likelihood ratio test and the <span class="math inline">\(F\)</span> tests are related via an monotone transformation, and we can use the <span class="math inline">\(\mathsf{Fisher}\)</span> distribution for comparison, rather than the large-sample <span class="math inline">\(\chi^2\)</span> approximation. The <span class="math inline">\(t\)</span>-tests and <span class="math inline">\(F\)</span>-tests presented above could thus both be viewed as particular cases of <a href="@sec-liktests">likelihood-ratio tests</a> but using Student-<span class="math inline">\(t\)</span> versus normal distribution when <span class="math inline">\(p-k=1\)</span>, and <span class="math inline">\(\mathsf{Fisher}\)</span> versus <span class="math inline">\(\chi^2\)</span> when <span class="math inline">\(p-k \ge 1\)</span>. When <span class="math inline">\(n\)</span> is large, results are roughly the same.</p>
</div>
<section id="contrasts" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="contrasts"><span class="header-section-number">4.4.1</span> Contrasts</h3>
<p>Suppose we perform an analysis of variance and the <span class="math inline">\(F\)</span>-test for the (global) null hypothesis that the averages of all groups are equal is very large: we reject the null hypothesis in favor of the alternative, which states that at least one of the group average is different. The follow-up question will be where these differences lie. Indeed, in an experimental context, this implies one or more of the manipulation has a different effect from the others on the mean response. Oftentimes, this isn’t interesting in itself: we could be interested in comparing different options relative to a control group or determine whether specific combinations work better than separately, or find the best treatment by comparing all pairs.</p>
<p>The scientific question of interest that warranted the experiment may lead to a specific set of hypotheses, which can be formulated by researchers as comparisons between means of different subgroups. We can normally express these as <strong>contrasts</strong>. As <a href="https://stat.ethz.ch/~meier">Dr.&nbsp;Lukas Meier</a> puts it, if the global <span class="math inline">\(F\)</span>-test for equality of means is equivalent to a dimly lit room, contrasts are akin to spotlight that let one focus on particular aspects of differences in treatments. Formally speaking, a contrast is a linear combination of averages: in plain English, this means we assign a weight to each group average and add them up, and then compare that summary to a postulated value <span class="math inline">\(a\)</span>, typically zero. Contrasts encode research question of interest: if <span class="math inline">\(c_i\)</span> denotes the weight of group average <span class="math inline">\(\mu_i\)</span> <span class="math inline">\((i=1, \ldots, K)\)</span>, then we can write the contrast as <span class="math inline">\(C = c_1 \mu_1 + \cdots + c_K \mu_K\)</span> with the null hypothesis <span class="math inline">\(\mathscr{H}_0: C=a\)</span> for a two-sided alternative. The sample estimate of the linear contrast is obtained by replacing the unknown population average <span class="math inline">\(\mu_i\)</span> by the sample average of that group, <span class="math inline">\(\widehat{\mu}_i = \overline{y}_{i}\)</span>. We can easily obtain the standard error of the linear combination <span class="math inline">\(C\)</span>: assuming subsample size of <span class="math inline">\(n_1, \ldots, n_K\)</span> and a common variance <span class="math inline">\(\sigma^2\)</span>, the standard error is the square root of <span class="math display">\[\mathsf{Va}(\widehat{C}) = \widehat{\sigma}^2\left(\frac{c_1^2}{n_1} + \cdots + \frac{c_K^2}{n_K}\right).\]</span> We can then build a <span class="math inline">\(t\)</span> statistic as usual by looking at the difference between our postulated value and the observed weighted mean, suitably standardized. If the global <span class="math inline">\(F\)</span>-test leads to rejection of the null, there exists a contrast which is significant at the same level. Whenever the contrasts vectors are orthogonal, the tests will be uncorrelated. Mathematically, if we let <span class="math inline">\(c_{i}\)</span> and <span class="math inline">\(c^{*}_{i}\)</span> denote weights attached to the mean of group <span class="math inline">\(i\)</span> comprising <span class="math inline">\(n_i\)</span> observations, contrasts are orthogonal if <span class="math inline">\(c_{1}c^{*}_{1}/n_1 + \cdots + c_{K}c^{*}_K/n_K = 0\)</span>; if the sample is balanced with the same number of observations in each group, <span class="math inline">\(n/K = n_1 =\cdots = n_K\)</span>, we can consider the dot product of the two contrast vectors and neglect the subsample sizes.</p>
<p>If we have <span class="math inline">\(K\)</span> groups, there are <span class="math inline">\(K-1\)</span> contrasts for pairwise differences, the last one being captured by the sample mean for the overall effect<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. If we care only about difference between groups (as opposed to the overall effect of all treatments), we impose a sum-to-zero constraint on the weights so <span class="math inline">\(c_1 + \cdots + c_K=0\)</span>.</p>
</section>
<section id="examples-of-tests" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="examples-of-tests"><span class="header-section-number">4.4.2</span> Examples of tests</h3>
<div id="exm-moonvaepps-test" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.15 (Testing for amount of donations)</strong></span> Consider <a href="#exm-moon" class="quarto-xref">Example&nbsp;<span>4.9</span></a>, whereby we test for differences between <code>open-ended</code> amounts and pre-specified amounts for proposals (<code>quantity</code>). The test of interest is <span class="math inline">\(\mathscr{H}_0: \beta_1=0\)</span>, where <span class="math inline">\(\beta_1=\mu_{\texttt{oe}} - \mu_{\texttt{qty}}\)</span> is the mean difference between groups. Beyond the fact the difference is statistically significant at the 5% level, we also want to report the <strong>marginal means</strong>, which when we have a single categorical explanatory variable is the group sample mean.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"MV23_S1"</span>, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>MV23_S1 <span class="ot">&lt;-</span> MV23_S1 <span class="sc">|&gt;</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">amount2 =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(amount), <span class="dv">0</span>, amount))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>linmod_MV23 <span class="ot">&lt;-</span> <span class="fu">lm</span>(amount2 <span class="sc">~</span> condition, <span class="at">data =</span> MV23_S1)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald tests with coefficients</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_MV23)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = amount2 ~ condition, data = MV23_S1)</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  -8.70  -6.77  -1.77   3.23  18.23 </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)          6.771      0.377   17.95   &lt;2e-16 ***</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; conditionquantity    1.929      0.517    3.73   0.0002 ***</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 7.61 on 867 degrees of freedom</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.0158, Adjusted R-squared:  0.0147 </span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 13.9 on 1 and 867 DF,  p-value: 0.000205</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Analysis of variance table with F tests</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(linmod_MV23)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Analysis of Variance Table</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Response: amount2</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;            Df Sum Sq Mean Sq F value Pr(&gt;F)    </span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; condition   1    805     805    13.9 0.0002 ***</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals 867  50214      58                   </span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal means</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>(emm <span class="ot">&lt;-</span> emmeans<span class="sc">::</span><span class="fu">emmeans</span>(linmod_MV23, <span class="at">spec =</span> <span class="st">"condition"</span>))</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  condition  emmean    SE  df lower.CL upper.CL</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  open-ended   6.77 0.377 867     6.03     7.51</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  quantity     8.70 0.354 867     8.01     9.40</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Confidence level used: 0.95</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>emm <span class="sc">|&gt;</span> emmeans<span class="sc">::</span><span class="fu">contrast</span>(<span class="at">method =</span> <span class="st">"pairwise"</span>) <span class="co"># contrast vector c(1,-1)</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  contrast                estimate    SE  df t.ratio p.value</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  (open-ended) - quantity    -1.93 0.517 867  -3.730  0.0002</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="exm-teachingtoread" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.16 (Tests and contrasts for reading comprehension methods)</strong></span> We consider now testing for the <a href="#exm-teaching-baumann" class="quarto-xref">Example&nbsp;<span>4.2</span></a> and <a href="#exm-interpretation1" class="quarto-xref">Example&nbsp;<span>4.12</span></a>. The purpose of <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> was to make a particular comparison between treatment groups. From the abstract:</p>
<blockquote class="blockquote">
<p>The primary quantitative analyses involved two planned orthogonal contrasts—effect of instruction (TA + DRTA vs.&nbsp;2 x DRA) and intensity of instruction (TA vs.&nbsp;DRTA).</p>
</blockquote>
<p>With a pre-post model, we will want to compare the means for a common value of <code>pretest1</code>, below taken to be the overall mean of the <code>pretest1</code> score.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans) <span class="co">#load package</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(BSJ92, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>mod_post <span class="ot">&lt;-</span> <span class="fu">lm</span>(posttest1 <span class="sc">~</span> group <span class="sc">+</span> pretest1,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> BSJ92)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(mod_post, <span class="at">type =</span> <span class="dv">3</span>) <span class="co"># F-tests</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Anova Table (Type III tests)</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Response: posttest1</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Sum Sq Df F value  Pr(&gt;F)    </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)      1  1    0.25    0.62    </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; group          143  2   12.17 3.5e-05 ***</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; pretest1       275  1   46.67 4.2e-09 ***</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals      365 62                    </span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>emmeans_post <span class="ot">&lt;-</span> <span class="fu">emmeans</span>(<span class="at">object =</span> mod_post,</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                        <span class="at">specs =</span> <span class="st">"group"</span>) <span class="co"># which variable to keep</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result of the analysis of variance table shows that there are indeed differences between groups. We can thus look at the estimated marginal means, which are the average of each group.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-print-pairwise-baumann" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-print-pairwise-baumann-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.4: Estimated group averages with standard errors and 95% confidence intervals for post-test 1 for average pre-test1 score.
</figcaption>
<div aria-describedby="tbl-print-pairwise-baumann-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">terms</th>
<th style="text-align: right;">marg. mean</th>
<th style="text-align: right;">std. err.</th>
<th style="text-align: right;">dof</th>
<th style="text-align: right;">lower (CI)</th>
<th style="text-align: right;">upper (CI)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DR</td>
<td style="text-align: right;">6.19</td>
<td style="text-align: right;">0.52</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">5.14</td>
<td style="text-align: right;">7.23</td>
</tr>
<tr class="even">
<td style="text-align: left;">DRTA</td>
<td style="text-align: right;">9.81</td>
<td style="text-align: right;">0.52</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">8.78</td>
<td style="text-align: right;">10.85</td>
</tr>
<tr class="odd">
<td style="text-align: left;">TA</td>
<td style="text-align: right;">8.22</td>
<td style="text-align: right;">0.52</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">7.18</td>
<td style="text-align: right;">9.27</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The hypothesis of <span class="citation" data-cites="Baumann:1992">Baumann, Seifert-Kessell, and Jones (<a href="references.html#ref-Baumann:1992" role="doc-biblioref">1992</a>)</span> is <span class="math inline">\(\mathscr{H}_0: \mu_{\mathrm{TA}} + \mu_{\mathrm{DRTA}} = 2 \mu_{\mathrm{DRA}}\)</span> or, rewritten slightly, <span class="math display">\[\begin{align*}
\mathscr{H}_0: - 2 \mu_{\mathrm{DR}} + \mu_{\mathrm{DRTA}} + \mu_{\mathrm{TA}} = 0.
\end{align*}\]</span> with weights <span class="math inline">\((-2, 1, 1)\)</span>; the order of the levels for the treatment are (<span class="math inline">\(\mathrm{DRA}\)</span>, <span class="math inline">\(\mathrm{DRTA}\)</span>, <span class="math inline">\(\mathrm{TA}\)</span>) and it must match that of the coefficients. An equivalent formulation is <span class="math inline">\((2, -1, -1)\)</span> or <span class="math inline">\((1, -1/2, -1/2)\)</span>: in either case, the estimated differences will be different (up to a constant multiple or a sign change). The vector of weights for <span class="math inline">\(\mathscr{H}_0:  \mu_{\mathrm{TA}} = \mu_{\mathrm{DRTA}}\)</span> is (<span class="math inline">\(0\)</span>, <span class="math inline">\(-1\)</span>, <span class="math inline">\(1\)</span>): the zero appears because the first component, <span class="math inline">\(\mathrm{DRA}\)</span> doesn’t appear. The two contrasts are orthogonal since <span class="math inline">\((-2 \times 0) + (1 \times -1) + (1 \times 1) = 0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify the order of the level of the variables</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(BSJ92, <span class="fu">levels</span>(group))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] "DR"   "DRTA" "TA"</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># DR, DRTA, TA (alphabetical)</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>contrasts_list <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Contrasts: linear combination of means, coefficients sum to zero</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"C1: average (DRTA+TA) vs DR"</span> <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>),</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"C2: DRTA vs TA"</span> <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>contrasts_post <span class="ot">&lt;-</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contrast</span>(<span class="at">object =</span> emmeans_post,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>           <span class="at">method =</span> contrasts_list)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>contrasts_summary_post <span class="ot">&lt;-</span> <span class="fu">summary</span>(contrasts_post)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-print-contrasts" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-print-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.5: Estimated contrasts for post-test 1.
</figcaption>
<div aria-describedby="tbl-print-contrasts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">contrast</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">estimate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">std. err.</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">dof</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">stat</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">C1: average (DRTA+TA) vs DR</td>
<td style="text-align: right;">2.83</td>
<td style="text-align: right;">0.64</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">4.40</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">C2: DRTA vs TA</td>
<td style="text-align: right;">1.59</td>
<td style="text-align: right;">0.73</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">2.17</td>
<td style="text-align: right;">0.03</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p>We can look at these differences; since <code>DRTA</code> versus <code>TA</code> is a pairwise difference, we could have obtained the <span class="math inline">\(t\)</span>-statistic directly from the pairwise contrasts using <code>pairs(emmeans_post)</code>.</p>
<p>What is the conclusion of our analysis of contrasts? It looks like the methods involving thinking aloud have a strong impact on reading comprehension relative to only directed reading. The evidence is not as strong when we compare the method that combines directed reading-thinking activity and thinking aloud, but the difference is statistically significant at level 5%.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients and standard errors</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>beta_pre <span class="ot">&lt;-</span> <span class="fu">coefficients</span>(mod_post)[<span class="st">'pretest1'</span>]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>se_pre <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">c</span>(<span class="fu">vcov</span>(mod_post)[<span class="st">'pretest1'</span>, <span class="st">'pretest1'</span>]))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>wald <span class="ot">&lt;-</span> (beta_pre <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>se_pre <span class="co"># Wald statistic, signed version</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># P-value based on Student-t distribution, with n-p-1 dof</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>pval <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="fu">abs</span>(wald), <span class="at">df =</span> mod_post<span class="sc">$</span>df.residual, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Model comparison via 'anova' call</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>mod0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(posttest1 <span class="sc">~</span> <span class="fu">offset</span>(pretest1) <span class="sc">+</span> group, <span class="at">data =</span> BSJ92)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># The 'offset' fixes the term and so this is equivalent to a coefficient of 1</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>aov_tab <span class="ot">&lt;-</span> <span class="fu">anova</span>(mod0, mod_post)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Another potential hypothesis of interest is testing whether the coefficient of <code>pretest1</code> is unity. This amounts to the Wald test <span class="math inline">\(w = (\widehat{\beta}_{\texttt{pretest1}}-1)/\mathsf{se}(\widehat{\beta}_{\texttt{pretest1}})= -3.024\)</span>, or alternatively a model comparison with the <code>anova</code>, which yields a test statistic of <span class="math inline">\(F=9.143\)</span>. Distribution theory shows that if <span class="math inline">\(Z \sim \mathsf{Student}(\nu)\)</span>, then <span class="math inline">\(Z^2 \sim \mathsf{Fisher}(1, \nu)\)</span>, it follows that both tests are equivalent and the <span class="math inline">\(p\)</span>-values are exactly the same.</p>
</div>
<div id="exm-paperorplastic" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.17 (Tests and contrasts for paper vs plastic)</strong></span> Let <span class="math inline">\(\mu_{0}, \mu_{0.5}, \mu_{1}, \mu_2\)</span> denote the true mean of the PEF score as a function of the proportion of paper for the data from <a href="#exm-sokolova" class="quarto-xref">Example&nbsp;<span>4.5</span></a>. There are several tests that could be of interest here, but we focus on contrasts performed by authors and an hypothesis test of linearity as a function of the proportion of plastic. For the latter, we could compare the linear regression model (in which the PEF score increases linearly with the proportion of paper to plastic) against the ANOVA which allows each of the four groups to have different means.</p>
<p>If we use <span class="math inline">\(\boldsymbol{\alpha} \in \mathbb{R}^4\)</span> to denote the parameter vector of the analysis of variance model using the treatment parametrization and <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^2\)</span> for the simple linear regression model, then we have <span class="math display">\[\begin{align*}
\mu_0 &amp;= \beta_0=\alpha_0 \\
\mu_{0.5} &amp;= \beta_0 + 0.5 \beta_1 = \alpha_0 + \alpha_1\\
\mu_1 &amp;= \beta_0 + \beta_1 = \alpha_0 + \alpha_2 \\
\mu_2 &amp;= \beta_0 + 2 \beta_1= \alpha_0 + \alpha_3.
\end{align*}\]</span> The test comparing the simple linear regression with the analysis of variance imposes two simultaneous restrictions, with <span class="math inline">\(\mathscr{H}_0: \alpha_3 = 2\alpha_2= 4\alpha_1\)</span>, so the null distribution is <span class="math inline">\(\mathsf{Fisher}(2, 798)\)</span> or roughly <span class="math inline">\(\chi^2_2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(SKD23_S2A, <span class="at">package =</span> <span class="st">"hecedsm"</span>) <span class="co"># load data</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>linmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(pef <span class="sc">~</span> proportion, <span class="at">data =</span> SKD23_S2A) <span class="co"># fit simple linear regression</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod) <span class="co"># extract intercept and slope</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)  proportion </span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       2.407       0.526</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>anovamod <span class="ot">&lt;-</span> <span class="fu">lm</span>(pef <span class="sc">~</span> <span class="fu">factor</span>(proportion), <span class="co"># one-way ANOVA</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> SKD23_S2A) </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare simple linear regression with ANOVA</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(linmod, anovamod) <span class="co"># is the change in PEF linear?</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Analysis of Variance Table</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 1: pef ~ proportion</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 2: pef ~ factor(proportion)</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Res.Df  RSS Df Sum of Sq    F  Pr(&gt;F)    </span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1    800 1373                              </span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2    798 1343  2      29.3 8.69 0.00018 ***</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Specifying the weights - these are not contrasts!</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">linearHypothesis</span>(<span class="at">model =</span> anovamod, </span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>   <span class="at">hypothesis =</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>), </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">1</span>)))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Linear hypothesis test</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Hypothesis:</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; - 2 factor(proportion)0.5  + factor(proportion)1 = 0</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; - 2 factor(proportion)1  + factor(proportion)2 = 0</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 1: restricted model</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Model 2: pef ~ factor(proportion)</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Res.Df  RSS Df Sum of Sq    F  Pr(&gt;F)    </span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1    800 1373                              </span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2    798 1343  2      29.3 8.69 0.00018 ***</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see from the output that the <span class="math inline">\(F\)</span> tests and the <span class="math inline">\(p\)</span>-values are identical, whether we impose the constraints manually or simply feed the two nested models to the <code>anova</code> method.</p>
<p>The authors were interested in comparing none with other choices: we are interested in pairwise differences, but only relative to the reference <span class="math inline">\(\mu_{0}\)</span>: <span class="math display">\[\begin{align*}
\mu_0 = \mu_{0.5}  &amp; \iff 1\mu_0 - 1\mu_{0.5} + 0\mu_{1} + 0 \mu_{2} = 0\\
\mu_0 = \mu_{1} &amp; \iff 1\mu_0 + 0\mu_{0.5} -1\mu_{1} + 0 \mu_{2} = 0\\
\mu_0 = \mu_{2} &amp; \iff 1\mu_0 + 0\mu_{0.5} + 0\mu_{1} -1 \mu_{2} = 0
\end{align*}\]</span> so contrast vectors <span class="math inline">\((1, -1, 0, 0)\)</span>, <span class="math inline">\((1, 0, -1, 0)\)</span> and <span class="math inline">\((1, 0, 0, -1)\)</span> for the marginal means would allow one to test the hypothesis.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>margmean <span class="ot">&lt;-</span> anovamod <span class="sc">|&gt;</span>  </span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  emmeans<span class="sc">::</span><span class="fu">emmeans</span>(<span class="at">specs =</span> <span class="st">"proportion"</span>) <span class="co"># group means</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>contrastlist <span class="ot">&lt;-</span> <span class="fu">list</span>( <span class="co"># specify contrast vectors</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">refvshalf =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>   <span class="at">refvsone =</span>  <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>   <span class="at">refvstwo =</span>  <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># compute contrasts relative to reference</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>margmean <span class="sc">|&gt;</span> emmeans<span class="sc">::</span><span class="fu">contrast</span>(<span class="at">method =</span> contrastlist)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  contrast  estimate    SE  df t.ratio p.value</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  refvshalf   -0.749 0.131 798  -5.710  &lt;.0001</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  refvsone    -0.901 0.131 798  -6.890  &lt;.0001</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  refvstwo    -1.182 0.129 798  -9.200  &lt;.0001</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The group averages are reported in <a href="#tbl-print-groupmeans-PEF" class="quarto-xref">Table&nbsp;<span>4.6</span></a>, match those reported by the authors in the paper. They suggest an increased perceived environmental friendliness as the amount of paper used in the wrapping increases. We could fit a simple regression model to assess the average change, treating the proportion as a continuous explanatory variable. The estimated slope for the change in PEF score, which ranges from 1 to 7 in increments of 0.25, is 0.53 point per ratio of paper/plastic. There is however strong evidence, given the data, that the change isn’t quite linear, as the fit of the linear regression model is significantly worse than the corresponding linear model.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-print-groupmeans-PEF" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-print-groupmeans-PEF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.6: Estimated group averages of PEF per proportion with standard errors
</figcaption>
<div aria-describedby="tbl-print-groupmeans-PEF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: right;" data-quarto-table-cell-role="th">proportion</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">marg. mean</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">std. err.</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">dof</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">lower (CI)</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">upper (CI)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">2.16</td>
<td style="text-align: right;">0.093</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">1.98</td>
<td style="text-align: right;">2.34</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.5</td>
<td style="text-align: right;">2.91</td>
<td style="text-align: right;">0.093</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">2.73</td>
<td style="text-align: right;">3.09</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">3.06</td>
<td style="text-align: right;">0.092</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">2.88</td>
<td style="text-align: right;">3.24</td>
</tr>
<tr class="even">
<td style="text-align: right;">2.0</td>
<td style="text-align: right;">3.34</td>
<td style="text-align: right;">0.089</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">3.17</td>
<td style="text-align: right;">3.52</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<div class="cell" data-layout-align="center">
<div id="tbl-print-contrast-PEF" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-print-contrast-PEF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.7: Estimated contrasts for differences of PEF to no paper.
</figcaption>
<div aria-describedby="tbl-print-contrast-PEF-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="table do-not-create-environment cell caption-top table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">contrast</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">estimate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">std. err.</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">dof</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">stat</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">refvshalf</td>
<td style="text-align: right;">-0.75</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">-5.71</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">refvsone</td>
<td style="text-align: right;">-0.90</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">-6.89</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">refvstwo</td>
<td style="text-align: right;">-1.18</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">798</td>
<td style="text-align: right;">-9.20</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>


</div>
</div>
</figure>
</div>
</div>
<p>All differences reported in <a href="#tbl-print-contrast-PEF" class="quarto-xref">Table&nbsp;<span>4.7</span></a> are significant and positive, in line with the researcher’s hypothesis.</p>
</div>
<div id="exm-tests-college" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.18 (Testing for the college wage discrimination)</strong></span> Consider the <code>college</code> data example and the associated linear model with <code>rank</code>, <code>sex</code>, years of <code>service</code> and <code>field</code> as covariates.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(college, <span class="at">package =</span> <span class="st">"hecstatmod"</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>mod1_college <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> sex <span class="sc">+</span> field <span class="sc">+</span> rank <span class="sc">+</span> service, <span class="at">data =</span> college)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>mod0_college <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> field <span class="sc">+</span> rank <span class="sc">+</span> service , <span class="at">data =</span> college)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># F-test with ANOVA table comparing nested models</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>aov_tab_college <span class="ot">&lt;-</span> <span class="fu">anova</span>(mod0_college, mod1_college)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Wald t-test - extracted from the output of the 'summary' table</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>wald_college <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod1_college)<span class="sc">$</span>coefficients[<span class="dv">2</span>,]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood ratio test with chi-square approximation</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>pval_lrt <span class="ot">&lt;-</span> <span class="fu">pchisq</span>(<span class="at">q =</span> <span class="fu">as.numeric</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="fu">logLik</span>(mod1_college) <span class="sc">-</span> <span class="fu">logLik</span>(mod0_college))),</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">df =</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The only test of interest here is <span class="math inline">\(\mathscr{H}_0: \beta_{\texttt{sex}} = 0\)</span> against the two-sided alternative <span class="math inline">\(\mathscr{H}_a: \beta_{\texttt{sex}} \neq 0\)</span>. The Wald <span class="math inline">\(t\)</span>-test statistic is <span class="math inline">\(-1.23\)</span>, with a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(0.219\)</span> based on a Student-<span class="math inline">\(t\)</span> distribution with <span class="math inline">\(391\)</span> degrees of freedom. The <span class="math inline">\(p\)</span>-value in the output from the <span class="math inline">\(F\)</span>-test is the same, and that obtained from the likelihood ratio test the same up to two decimal places.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>Table of linear regression coefficients with associated standard errors, Wald tests and <span class="math inline">\(p\)</span>-values based on Student-<span class="math inline">\(t\)</span> distribution.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std. error</th>
<th style="text-align: right;">Wald stat.</th>
<th style="text-align: right;">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">86.596</td>
<td style="text-align: right;">2.96</td>
<td style="text-align: right;">29.25</td>
<td style="text-align: right;">&lt; 0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">sex [woman]</td>
<td style="text-align: right;">-4.771</td>
<td style="text-align: right;">3.878</td>
<td style="text-align: right;">-1.23</td>
<td style="text-align: right;">0.22</td>
</tr>
<tr class="odd">
<td style="text-align: left;">field [theoretical]</td>
<td style="text-align: right;">-13.473</td>
<td style="text-align: right;">2.315</td>
<td style="text-align: right;">-5.82</td>
<td style="text-align: right;">&lt; 0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">rank [associate]</td>
<td style="text-align: right;">14.56</td>
<td style="text-align: right;">4.098</td>
<td style="text-align: right;">3.55</td>
<td style="text-align: right;">&lt; 0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;">rank [full]</td>
<td style="text-align: right;">49.16</td>
<td style="text-align: right;">3.834</td>
<td style="text-align: right;">12.82</td>
<td style="text-align: right;">&lt; 0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">service</td>
<td style="text-align: right;">-0.089</td>
<td style="text-align: right;">0.112</td>
<td style="text-align: right;">-0.8</td>
<td style="text-align: right;">0.43</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
</section>
<section id="factorial-designs-and-interactions" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="factorial-designs-and-interactions"><span class="header-section-number">4.5</span> Factorial designs and interactions</h2>
<p>The additive mean model with linear terms (including categorical variables) specifies that the marginal effect of one variable is independent of others. We may wish to relax this assumption by including <strong>interaction terms</strong>.</p>
<p>Combinations combinations of covariates may affect the response differently than when taking in isolation.</p>
<div id="exm-visualising-interaction" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.19 (Insurance data and complex interactions)</strong></span> A good example of interaction is provided by the <code>insurance</code> dataset. An exploratory data analysis suggested that premiums depended on age, smoker status and body mass index (BMI). <a href="#fig-insuranceinter1" class="quarto-xref">Figure&nbsp;<span>4.9</span></a> shows that the insurance premium depends on smoking: smokers who have a BMI of 30 and above pay a hefty premium, but there is also seemingly a linear increase in the amount charged with BMI. We see no such behaviour for non-smokers.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-insuranceinter1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-insuranceinter1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-insuranceinter1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-insuranceinter1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.9: Graph of insurance charges against body mass index, colored by smoking status.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="def-interaction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.1 (Interaction)</strong></span> An interaction occurs if some explanatory variables, when coupled together, have different impacts than the superposition of each, i.e., if <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span> interact, the marginal effect of <span class="math inline">\(\mathsf{E}(Y \mid \boldsymbol{X})\)</span> with respect to <span class="math inline">\(X_j\)</span> is a function of <span class="math inline">\(X_k\)</span> or vice-versa.</p>
</div>
<p>We will restrict attention to the cases where one or more of the explanatories is a categorical variable (factor).</p>
<div id="exm-intention" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.20 (Intention to buy)</strong></span> We consider a toy model for the <code>interaction</code> data, and model the <code>intention</code> to buy a product as a function of <code>sex</code> and <code>fixation</code>. The base model, without interaction, is <span class="math display">\[\begin{align*}
\texttt{intention}=\beta_0 + \beta_1 \texttt{sex} + \beta_2 \texttt{fixation} + \varepsilon,
\end{align*}\]</span> where is a binary variable taking value unity for female and zero for male. The model assumes that the effect of the continuous variable <span class="math inline">\(\texttt{fixation}\)</span> is the same for the two values of the binary variable. Likewise, the effect of the binary variable is assumed to be the same for all possible values of the continuous variable. We can see this on the plot, as the difference between the lines represents the effect of <span class="math inline">\(\texttt{sex}\)</span>, is the same for all values of <span class="math inline">\(\texttt{fixation}\)</span>; the lines are <em>parallel</em>: see the left panel of <a href="#fig-interaction-slope" class="quarto-xref">Figure&nbsp;<span>4.10</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-interaction-slope" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interaction-slope-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-interaction-slope-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interaction-slope-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.10: Scatterplots and fitted lines for a model with a single continuous and binary explanatory, without (left) and with (right) an interaction term.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In order to add a different slope for men and women, we can create a new variable equal to the product <span class="math inline">\(\texttt{fixation}\times\texttt{sex}\)</span> and add it to the model, <span class="math display">\[\begin{align*}
\texttt{intention} &amp;= \beta_0 + \beta_1 \texttt{sex} + \beta_2\texttt{fixation} \\&amp;\quad + \beta_3 \texttt{fixation}\cdot \texttt{sex} + \varepsilon.
\end{align*}\]</span> Depending on the value of the binary variable , we get <span class="math display">\[\begin{align*}
&amp;\mathsf{E}(\texttt{intention} \mid \texttt{fixation}, \texttt{sex}) \\\quad&amp;=
\begin{cases}
(\beta_0 + \beta_1) + (\beta_2 + \beta_3)\texttt{fixation}, &amp; \texttt{sex}=1,\\
  \beta_0 + \beta_2 \texttt{fixation}, &amp; \texttt{sex}=0.
\end{cases}
\end{align*}\]</span> The interpretation of the coefficients in the model is as usual with the treatment contrast parametrization:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the average buying intention when the fixation time is zero for men,</li>
<li><span class="math inline">\(\beta_1\)</span> is the difference in intercept for women vs men,</li>
<li><span class="math inline">\(\beta_2\)</span> is the unit increase per second of fixation for men,</li>
<li><span class="math inline">\(\beta_3\)</span> is the difference in slope for women vs men.</li>
</ul>
<p>Testing whether the interaction is significant boils down to using the test <span class="math inline">\(\mathscr{H}_0: \beta_3=0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(interaction, <span class="at">package =</span> <span class="st">"hecstatmod"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(intention <span class="sc">~</span> fixation <span class="sc">+</span> sex <span class="sc">+</span> sex<span class="sc">:</span>fixation, </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">data =</span> interaction)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>coefficients</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)     2.741      0.282    9.73 1.02e-16</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; fixation        0.504      0.153    3.29 1.33e-03</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; sex             1.312      0.380    3.45 7.74e-04</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; fixation:sex    2.135      0.200   10.69 5.61e-19</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The model with the interaction is significantly better, meaning that the effect of fixation time on intention to buy varies according to sex.</p>
</div>
<div id="rem-marginality" class="proof remark">
<p><span class="proof-title"><em>Remark 4.6</em> (Marginality). </span>In the model with buying as a function of and time, we would remove the main effect of while keeping the interaction term , even if we fail to reject <span class="math inline">\(\mathscr{H}_0:\beta_2=0\)</span>. Without it, the model would become <span class="math display">\[\begin{align*}
&amp;\mathsf{E}(\texttt{intention} \mid \texttt{fixation}, \texttt{sex}) \\\quad&amp;=
\begin{cases}
(\beta_0 + \beta_1) + \beta_3\texttt{fixation}, &amp; \texttt{sex}=1,\\
  \beta_0, &amp;\texttt{sex}=0;                 
\end{cases}
\end{align*}\]</span><br>
implying that intention to buy is constant for men, regardless of the fixation time. As the choice of baseline is arbitrary, but changing the dummy ( for women, for men), would yield a different model and so potentially different inferences, we never consider removal of the main effect term that is involved in an interaction. The <strong>marginality principle</strong> states that all lower interaction terms should be included.</p>
</div>
<p>The concept of interactions readily extends to categorical variables with <span class="math inline">\(k\)</span> levels/categories. In this case, we need to use the global <span class="math inline">\(F\)</span>-test to check if the interaction is statistically significant.</p>
<div id="def-anova" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.2 (Two-way analysis of variance)</strong></span> An analysis of variance is a linear model in which the mean is a function of categorical explanatory variables. If we have data for all different combinations of factors, the factors are <strong>crossed</strong> and we can consider inclusion of their interactions.</p>
<p>Consider a two-way analysis of variance model. This is a linear model with two factors, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, with respectively <span class="math inline">\(n_a\)</span> and <span class="math inline">\(n_b\)</span> levels. The response <span class="math inline">\(Y_{ijk}\)</span> of the <span class="math inline">\(k\)</span>th measurement in group <span class="math inline">\((a_i, b_j)\)</span> is <span id="eq-twowayasoneway"><span class="math display">\[
\underset{\text{response}\vphantom{b}}{Y_{ijk}} = \underset{\text{subgroup mean}}{\mu_{ij}} + \underset{\text{error term}}{\varepsilon_{ijk}}
\tag{4.2}\]</span></span> where</p>
<ul>
<li><span class="math inline">\(Y_{ijk}\)</span> is the <span class="math inline">\(k\)</span>th replicate for <span class="math inline">\(i\)</span>th level of factor <span class="math inline">\(A\)</span> and <span class="math inline">\(j\)</span>th level of factor <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(\mu_{ij}\)</span> is the average response of measurements in group <span class="math inline">\((a_i, b_j)\)</span></li>
<li><span class="math inline">\(\varepsilon_{ijk}\)</span> are independent error terms with mean zero and standard deviation <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>In a full factorial design with interactions, we can write the mean response as <span class="math inline">\(\mathsf{E}(Y \mid A=a_i, B=b_j) = \mu_{ij}\)</span>. This model can be reduced to a single one-way ANOVA with a single factor having <span class="math inline">\(n_an_b\)</span> levels. This may be useful to specify contrast weights, or when there is an additional control group in an experimental setting. However, preserving the structure helps setting up hypotheses of interest.</p>
<p>We can equivalently express this in terms of an intercept, main effects of either variables, and interaction terms. The additive model, with no interaction, has average for cell <span class="math inline">\((i,j)\)</span> of</p>
<p><span class="math display">\[\begin{align*}
\mathsf{E}(Y_{ij} \mid A = a_i, B=b_j) = \mu + \alpha_i + \beta_j.
\end{align*}\]</span> <!--
- $\mu$ is the average of all subgroup averages, termed overall mean.
- $\alpha_i = \mu_{i.} - \mu$ is the mean of level $A_i$ minus the overall mean.
- $\beta_j  = \mu_{.j} - \mu$ is the mean of level $B_j$ minus the overall mean.
- $(\alpha\beta)_{ij} = \mu_{ij} - \mu_{i.} - \mu_{.j} + \mu$ is the interaction term for $A_i$ and $B_j$ which encodes the effect of both variable not already captured by the main effects.
--></p>
<!--
The model formulation in terms of difference from the global average or main effect ensures that we can test for main effects for factor $A$ by setting $\mathscr{H}_0: \alpha_1 = \cdots = \alpha_{n_a-1}=0$. The $1 +  n_a + n_b$ **sum to zero** constraints,
$$\sum_{i=1}^{n_a} \alpha_i=0, \quad \sum_{j=1}^{n_b} \beta_j=0, \quad  \sum_{j=1}^{n_b} (\alpha\beta)_{ij}=0, \quad \sum_{i=1}^{n_a} (\alpha\beta)_{ij}=0.$$
-->
<p>We can consider model simplifications from bottom up. Removing the interaction leads to a model with <span class="math inline">\(1 + (n_a-1) + (n_b-1)\)</span> parameters, relative to <span class="math inline">\(n_a\times n_b\)</span> for the model with the interaction. We can use an <span class="math inline">\(F\)</span>-test to check for the significance of the latter. If the factors don’t interact, the mean in the cell is given by the sum of the main effects. Only once we have a removed this term can we consider if all row means or column means are the same.</p>
</div>
<p>While formal testing is needed to check for interactions, the concept can be better understood by looking at graphs (at least in a setting where the means are known with little to no uncertainty).</p>
<div id="def-interactionplot" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.3 (Interaction plot)</strong></span> We can try to detect interactions visually by plotting the (mean) response as a function of one of the covariates, using a so-called <strong>interaction plot</strong>. When there are more than two categorical variables, we can use colors, symbols or panels to represent the categories. Lack of interaction in those plots implies parallel lines, but one must account for the uncertainty.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-2by2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2by2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-2by2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2by2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.11: Interaction plots (line graphs) for example patterns for means for each of the possible kinds of general outcomes in a 2 by 2 design. Illustration adapted from Figure 10.2 of <span class="citation" data-cites="Crump.Navarro.Suzuki:2019">Crump, Navarro, and Suzuki (<a href="references.html#ref-Crump.Navarro.Suzuki:2019" role="doc-biblioref">2019</a>)</span> by Matthew Crump (CC BY-SA 4.0 license).
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="def-simple" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.4 (Simple effects and main effects)</strong></span> When interactions do not exist, it makes sense to abstract from one or more variable and consider <strong>marginal effects</strong>, obtained by pooling data from the omitted factors and averaging out. Suppose without loss of generality that we are interested in comparing levels of <span class="math inline">\(A\)</span>. When interactions between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not significant, we can consider lower order terms and report <strong>estimated marginal means</strong> and contrasts between means of <span class="math inline">\(A\)</span>. If the interaction with <span class="math inline">\(B\)</span> has an impact, we can rather compute the subcell average of <span class="math inline">\(A \mid B=b_j\)</span>, and similarly for contrasts. We thus distinguish between the following:</p>
<ul>
<li><strong>simple effects</strong>: difference between levels of one in a fixed combination of others. Simple effects are comparing cell averages within a given row or column.</li>
<li><strong>main effects</strong>: differences relative to average for each condition of a factor. Main effects are row/column averages.</li>
</ul>
</div>
<div id="exm-STC21-twoway" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.21 (Psychological ownership of borrowed money)</strong></span> Supplemental Study 5 from <span class="citation" data-cites="Sharma.Tully.Cryder:2021">Sharma, Tully, and Cryder (<a href="references.html#ref-Sharma.Tully.Cryder:2021" role="doc-biblioref">2021</a>)</span> checks the psychological perception of borrowing money depending on the label. The authors conducted a 2 by 2 between-subject comparison (two-way ANOVA) varying the type of debt (whether the money was advertised as <code>credit</code> or <code>loan</code>) and the type of purchase the latter would be used for (<code>discretionary</code> spending or <code>need</code> for necessary purchases). The response is the average of the likelihood and interest in the product, both measured using a 9 point Likert scale from 1 to 9.</p>
<p>The mean model with an interaction can be written using the treatment contrast parametrization as <span class="math display">\[\begin{align*}
\texttt{likelihood} &amp;= \beta_0 + \beta_1\mathbf{1}_{\texttt{purchase=need}} + \beta_2\mathbf{1}_{\texttt{debttype=loan}} \\&amp;\quad+ \beta_3\mathbf{1}_{\texttt{purchase=need}}\mathbf{1}_{\texttt{debttype=loan}} + \varepsilon
\end{align*}\]</span></p>
<p><span class="citation" data-cites="Sharma.Tully.Cryder:2021">Sharma, Tully, and Cryder (<a href="references.html#ref-Sharma.Tully.Cryder:2021" role="doc-biblioref">2021</a>)</span> fitted a model with two factors, each with two levels, and their interaction. Since there are one global average and two main effect (additional difference in average for both factors <code>debttype</code> and <code>purchase</code>), the interaction involves one degree of freedom since we go from a model with three parameters describing the mean to one that has a different average for each of the four subgroups.</p>
<p>The reason why this is first test to carry out is that if the effect of one factor depends on the level of the other, as shown in <a href="#fig-2by2" class="quarto-xref">Figure&nbsp;<span>4.11</span></a>, then we need to compare the label of debt type separately for each type of purchase and vice-versa using simple effects. If the interaction on the contrary isn’t significant, then we could pool observations instead and average across either of the two factors, resulting in the marginal comparisons with the main effects.</p>
<p>Fitting the model including the interaction between factors ensures that we keep the additivity assumption and that our conclusions aren’t misleading: the price to pay is additional mean parameters to be estimated, which isn’t an issue if you collect enough data, but can be critical when data collection is extremely costly and only a few runs are allowed.</p>
<p>In <strong>R</strong>, we include both factors in a formula as <code>response ~ factorA * factorB</code>, the <code>*</code> symbol indicating that both are allowed to interact, as a shorthand for <code>factorA + factorB + factorA:factorB</code>; in the main effect model, we would use instead <code>+</code> to reflect that the effects of both factors add up.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analysing Supplementary Study 5</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># of Sharma, Tully, and Cryder (2021)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(STC21_SS5, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">aov</span>(likelihood <span class="sc">~</span> purchase<span class="sc">*</span>debttype, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> STC21_SS5)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute means of rows/columns/cells</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">model.tables</span>(mod, <span class="at">type =</span> <span class="st">"means"</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Tables of means</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Grand mean</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      </span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4.88 </span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  purchase </span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     discretionary    need</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             4.182   5.579</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rep       751.000 750.000</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  debttype </span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      credit    loan</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       5.127   4.631</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; rep 753.000 748.000</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  purchase:debttype </span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                debttype</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; purchase        credit loan </span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   discretionary   4.5    3.8</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   rep           392.0  359.0</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   need            5.7    5.4</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   rep           361.0  389.0</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Analysis of variance reveals </span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co"># non-significant interaction</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co"># of purchase and type</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">Anova</span>(mod, <span class="at">type =</span> <span class="dv">2</span>)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Response: likelihood</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                   Sum Sq   Df F value  Pr(&gt;F)    </span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; purchase             752    1   98.21 &lt; 2e-16 ***</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; debttype              92    1   12.04 0.00054 ***</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; purchase:debttype     14    1    1.79 0.18171    </span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals          11467 1497                    </span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since the interaction is not significant, we can only interpret the main effect of fixation. These conditional mean difference are termed marginal effect, because they are obtained by averaging out the other explanatory. The model however estimates the variance based on residuals from the full interaction model with four cell means, so differs from that obtained by (incorrectly) running a model with only <code>purchase</code> as explanatory.</p>
<p>In the analysis of variance table, we focus exclusively on the last line with the sum of squares for <code>purchase:debttype</code>. The <span class="math inline">\(F\)</span> statistic is 1.79; using the <span class="math inline">\(\mathsf{F}\)</span> (1, 1497) distribution as benchmark, we obtain a <span class="math inline">\(p\)</span>-value of 0.18 so there is no evidence the effect of purchase depends on debt type.</p>
<p>We can thus pool data and look at the effect of debt type (<code>loan</code> or <code>credit</code>) overall by combining the results for all purchase types, one of the planned comparison reported in the Supplementary material. To do this in <strong>R</strong> with the <code>emmeans</code> package, we use the <code>emmeans</code> function and we quote the factor of interest (i.e., the one we want to keep) in <code>specs</code>. By default, this will compute the estimate marginal means: the <code>contr = "pairwise"</code> indicates that we want the difference between the two, which gives us the contrasts.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pairwise comparisons within levels of purchase</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple effect</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emmeans</span>(mod, </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">specs =</span> <span class="st">"purchase"</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">contr =</span> <span class="st">"pairwise"</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $emmeans</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  purchase      emmean    SE   df lower.CL upper.CL</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  discretionary   4.17 0.101 1497     3.97     4.36</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  need            5.58 0.101 1497     5.39     5.78</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Results are averaged over the levels of: debttype </span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Confidence level used: 0.95 </span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $contrasts</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  contrast             estimate    SE   df t.ratio p.value</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  discretionary - need    -1.42 0.143 1497  -9.910  &lt;.0001</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Results are averaged over the levels of: debttype</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Interaction plot</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>emmeans<span class="sc">::</span><span class="fu">emmip</span>(mod, </span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>               purchase <span class="sc">~</span> debttype, </span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>               <span class="at">CIs =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-interaction-ST" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interaction-ST-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-interaction-ST-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interaction-ST-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.12: Interaction plots for the <span class="citation" data-cites="Sharma.Tully.Cryder:2021">Sharma, Tully, and Cryder (<a href="references.html#ref-Sharma.Tully.Cryder:2021" role="doc-biblioref">2021</a>)</span> data.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="rem-sumofsquare" class="proof remark">
<p><span class="proof-title"><em>Remark 4.7</em> (Sum of square decomposition). </span>There are different sum of square decompositions (type I, II and III) for the comparison of nested models in analysis of variance tables produced by <code>anova</code>. These test different models using <span class="math inline">\(F\)</span> statistics, with the same denominator based on <span class="math inline">\(S_2\)</span> from the model output, and the numerator is the difference in sum of squares. All of the decompositions agree when the sample size is <strong>balanced</strong>, meaning each cell has the same number of replications <span class="math inline">\(n_r\)</span>, so that the overall number of observations is <span class="math inline">\(n = n_an_bn_r\)</span>.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-ssdecompo" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ssdecompo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.8: Sum of square decompositions in ANOVA tables. Comparison of sum of squares between null, versus alternative model.
</figcaption>
<div aria-describedby="tbl-ssdecompo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">type 1</th>
<th style="text-align: left;">type II</th>
<th style="text-align: left;">type III</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\boldsymbol{A}\)</span></td>
<td style="text-align: left;">intercept vs <span class="math inline">\(A\)</span></td>
<td style="text-align: left;"><span class="math inline">\(B\)</span> vs <span class="math inline">\((A,B)\)</span></td>
<td style="text-align: left;"><span class="math inline">\((B, AB)\)</span> vs <span class="math inline">\((A,B, AB)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\boldsymbol{B}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(A\)</span> vs <span class="math inline">\((A,B)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(A\)</span> vs <span class="math inline">\((A,B)\)</span></td>
<td style="text-align: left;"><span class="math inline">\((A, AB)\)</span> vs <span class="math inline">\((A,B,AB)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\boldsymbol{AB}\)</span></td>
<td style="text-align: left;"><span class="math inline">\((A,B)\)</span> vs <span class="math inline">\((A,B,AB)\)</span></td>
<td style="text-align: left;"><span class="math inline">\((A,B)\)</span> vs <span class="math inline">\((A,B,AB)\)</span></td>
<td style="text-align: left;"><span class="math inline">\((A,B)\)</span> vs <span class="math inline">\((A,B,AB)\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#tbl-ssdecompo" class="quarto-xref">Table&nbsp;<span>4.8</span></a> shows the different sum of squared errors of the models, with the terms in parenthesis indicating which terms are included (<span class="math inline">\(AB\)</span> denotes the interaction).</p>
<p>Type I, the default with the generic <code>anova</code>, uses the order in which terms enter, say <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(AB\)</span>, so compares in the first line the improvement in the mean-only model with <span class="math inline">\(A\)</span>, then in the second line the test for <span class="math inline">\(B\)</span> compares the model with both main effects <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with only <span class="math inline">\(A\)</span>. Since the order in which the factors is specified is arbitrary, this decomposition is arbitrary and not relevant.</p>
<p>The type II decomposition considers terms of the same level in the hierarchy, so the tests for the main effects are <span class="math inline">\(A + B\)</span> vs <span class="math inline">\(A\)</span>, <span class="math inline">\(A+B\)</span> vs <span class="math inline">\(B\)</span> and that of the interaction is <span class="math inline">\(A\times B\)</span> vs <span class="math inline">\(A, B\)</span>. This should be the default option if we wish to consider main effects when the interaction isn’t significant.</p>
<p>The type III decomposition, popularized by SAS and often taken as the default, considers all other terms, so would test main effects as <span class="math inline">\(A + B + A\times B\)</span> vs <span class="math inline">\(B + A\times B\)</span>. This does not respect the marginality principle, so should be avoided. The tests for <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> should not be used.</p>
<p>All three methods agree for the last level with the interaction.</p>
</div>
<p>All of the discussion for a two-way ANOVA extends to higher-dimensional designs for <span class="math inline">\(K\)</span> factors. However, the curse of dimensionality makes it more difficult to collect observations in each cell. Any multiway ANOVA with two or more factor can be collapsed into a single one-way ANOVA: this is notably useful when there is a control group which is not related to the factor levels, as no manipulation takes place. The use of contrasts becomes critical since we can write any test for main effects, interactions using the latter through weighting.</p>
<div id="exm-LKUK24" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.22 (Perceptions of cultural appropriation by ideology)</strong></span> We consider a three-way ANOVA from <span class="citation" data-cites="Lin.Kim.Uduehi.Keinan:2024">Lin et al. (<a href="references.html#ref-Lin.Kim.Uduehi.Keinan:2024" role="doc-biblioref">2024</a>)</span>. Their Study 4 focused on cultural appropriation for soul food recipe cookbook from Chef Dax, who was either black (or not), manipulating the description of the way he obtained the recipes (by peeking without permission in kitchens, by asking permission or no mention for control). Authors postulated that the perception of appropriation would vary by political ideology (liberal or conservative). The study results in a 3 by 2 by 2 three-way ANOVA.</p>
<p>For the <span class="math inline">\(K\)</span>-way ANOVA, we always start with estimating the full model with all <span class="math inline">\(K\)</span>-way interaction (provided there are enough data to estimate the latter, which implies there are repetitions). If the latter is significant, we can fix one or more factor levels and compare the others.</p>
<div class="cell" data-layout-align="center">
<div id="tbl-anova-LKUK24" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-layout-align="center">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-anova-LKUK24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.9: Analysis of variance table (type II decomposition) for the data from Study 4 of Lin et al.&nbsp;(2024).
</figcaption>
<div aria-describedby="tbl-anova-LKUK24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">sum of squares</th>
<th style="text-align: right;">df</th>
<th style="text-align: right;">stat</th>
<th style="text-align: left;">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">politideo</td>
<td style="text-align: right;">48.49</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">21.35</td>
<td style="text-align: left;">&lt;0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">chefdax</td>
<td style="text-align: right;">473.72</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">208.61</td>
<td style="text-align: left;">&lt;0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;">brandaction</td>
<td style="text-align: right;">34.24</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">7.54</td>
<td style="text-align: left;">&lt;0.001</td>
</tr>
<tr class="even">
<td style="text-align: left;">politideo:chefdax</td>
<td style="text-align: right;">65.00</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">28.63</td>
<td style="text-align: left;">&lt;0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;">politideo:brandaction</td>
<td style="text-align: right;">1.56</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.34</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr class="even">
<td style="text-align: left;">chefdax:brandaction</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.14</td>
<td style="text-align: left;">0.87</td>
</tr>
<tr class="odd">
<td style="text-align: left;">politideo:chefdax:brandaction</td>
<td style="text-align: right;">0.66</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: left;">0.86</td>
</tr>
<tr class="even">
<td style="text-align: left;">Residuals</td>
<td style="text-align: right;">1587.33</td>
<td style="text-align: right;">699</td>
<td style="text-align: right;"></td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>If we consider <a href="#tbl-anova-LKUK24" class="quarto-xref">Table&nbsp;<span>4.9</span></a>, we find that there is no three-way interaction and, omitting the latter and focusing on lower-level, a single two-way interaction between political ideology and the race of Chef Dax. We cannot interpret the <span class="math inline">\(p\)</span>-value for the main effect of <code>brandaction</code>, but we could look at the marginal means.</p>
<p>Based on the data, we will collapse data to a one-way ANOVA comparing the three levels of <code>brandaction</code> and a 2 by 2 two-way ANOVA for the other two factors. The results are obtained by averaging over the missing factor, but estimating the standard deviation <span class="math inline">\(\sigma^2\)</span> from the full model.</p>
<p>We are interested in comparing the perception between the race of Chef Dax (black or not, as Southern Soul food cooking is more likely to be associated with cultural appropriation if Chef Dax is not black. We proceed with <code>emmeans</code> by computing the marginal means separately for each of the four subcategories, but compare the race of Chef Dax separately for liberals and conservatives due to the presence of the interaction.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(LKUK24_S4, <span class="at">package =</span> <span class="st">"hecedsm"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(emmeans)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(appropriation <span class="sc">~</span> politideo <span class="sc">*</span> chefdax <span class="sc">*</span> brandaction,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>   <span class="at">data =</span> LKUK24_S4)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal means for political ideology/Chef Dax</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute simple effects, by political ideology</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="fu">emmeans</span>(mod, </span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>         <span class="at">specs =</span> <span class="st">"chefdax"</span>, </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>         <span class="at">by =</span> <span class="st">"politideo"</span>,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>         <span class="at">contrast =</span> <span class="st">"pairwise"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; politideo = conservative:</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  chefdax   emmean     SE  df lower.CL upper.CL</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  not black   2.38 0.1425 699     2.11     2.66</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  black       1.68 0.1494 699     1.38     1.97</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; politideo = liberal:</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  chefdax   emmean     SE  df lower.CL upper.CL</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  not black   3.60 0.0968 699     3.41     3.79</span></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  black       1.57 0.0947 699     1.38     1.75</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Results are averaged over the levels of: brandaction </span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Confidence level used: 0.95</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that the liberals are much more likely to view Chef Dax cookbook as an instance of cultural appropriation if he is not black; there is limited evidence of any difference between conservatives and liberal when Chef Dax is black. Both differences are statistically significative, but the differences (and thus evidence of an effect) is much stronger for left-leaning respondents.</p>
<p>We can look next at the brand action: we expect participants will view peeking less favorably than if Chef Dax asked for permission to publish the recipes. It’s tricky to know the effect of the control, as we are not bringing the point to the attention of participants in this instance.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal mean for brandaction</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>emm_brand <span class="ot">&lt;-</span> <span class="fu">emmeans</span>(mod, <span class="at">specs =</span> <span class="fu">c</span>(<span class="st">"brandaction"</span>)) </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>emm_brand</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  brandaction emmean    SE  df lower.CL upper.CL</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  peeking       2.56 0.107 699     2.35     2.77</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  permission    2.29 0.105 699     2.09     2.50</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  control       2.07 0.108 699     1.86     2.28</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Results are averaged over the levels of: politideo, chefdax </span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Confidence level used: 0.95</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint F test for the main effect of brandaction</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>emm_brand <span class="sc">|&gt;</span> <span class="fu">pairs</span>() <span class="sc">|&gt;</span> <span class="fu">joint_tests</span>()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  model term df1 df2 F.ratio p.value</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  contrast     2 699   5.090  0.0064</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A joint <span class="math inline">\(F\)</span>-test, obtained by collapsing everything to a one-way ANOVA, shows that there are indeed differences. However, note that the averages of the three actions are much smaller than for race.</p>
</div>
<!--
Interactions
Two-way ANOVA - one mean per cell
Crossed factors

Interactions (categorical vs categorical)
Interactions (categorical vs continuous)
Tests for interactions
Remark: what can you estimate + is estimation reliable?
Remark: marginality principle.
Interactions and interaction plots
Simple and marginal effects
Confounding variables



Linearity and interpretation of effects - added variable plots

Colinearity and identifiability
Model assumptions
Residual diagnostics
Plots and remedies/ a primer on other models
-->
</section>
<section id="geometry-of-least-squares" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="geometry-of-least-squares"><span class="header-section-number">4.6</span> Geometry of least squares</h2>
<div id="rem-geometry" class="proof remark">
<p><span class="proof-title"><em>Remark 4.9</em> (Geometry). </span>The vector of fitted values <span class="math inline">\(\widehat{\boldsymbol{y}} =\mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{H}_{\mathbf{X}}\boldsymbol{y}\)</span> is the projection of the response vector <span class="math inline">\(\boldsymbol{y}\)</span> on the linear span generated by the columns of <span class="math inline">\(\mathbf{X}\)</span>. The matrix <span class="math inline">\(\mathbf{H}_{\mathbf{X}} = \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span>, often called hat matrix, is an orthogonal projection matrix, so <span class="math inline">\(\mathbf{H}_{\mathbf{X}}=\mathbf{H}_{\mathbf{X}}^\top\)</span> and <span class="math inline">\(\mathbf{H}_{\mathbf{X}}\mathbf{H}_{\mathbf{X}} = \mathbf{H}_{\mathbf{X}}\)</span> and <span class="math inline">\(\mathbf{H}_{\mathbf{X}}\mathbf{X} = \mathbf{X}\)</span>. Since the vector of residuals <span class="math inline">\(\boldsymbol{e} = (e_1, \ldots, e_n)^\top\)</span>, which appear in the sum of squared errors, is defined as <span class="math inline">\(\boldsymbol{y} - \widehat{\boldsymbol{y}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}=\mathbf{X}\boldsymbol{\beta}\)</span>, simple algebraic manipulations show that the inner product between ordinary residuals and fitted values is zero, since <span class="math display">\[\begin{align*}
\widehat{\boldsymbol{y}}^\top\boldsymbol{e} &amp;= \widehat{\boldsymbol{\beta}}^\top \mathbf{X}^\top (\boldsymbol{y}- \mathbf{X} \widehat{\boldsymbol{\beta}})
\\&amp;= \boldsymbol{y}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y})\\&amp;=\boldsymbol{y}^\top\mathbf{H}_{\mathbf{X}}\boldsymbol{y} - \mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}
\\&amp;= 0
\end{align*}\]</span> where we use the definition of <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and <span class="math inline">\(\boldsymbol{e} = \boldsymbol{y} - \widehat{\boldsymbol{y}}\)</span> on the first line, then substitute the OLS estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{y}\)</span> and distribute terms. Similarly, <span class="math inline">\(\mathbf{X}^\top\boldsymbol{e}=\boldsymbol{0}_{p+1}\)</span>. The ordinary residuals are thus orthogonal to both the model matrix <span class="math inline">\(\mathbf{X}\)</span> and to the fitted values.</p>
<div id="cor-cor" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4.1 (Orthogonality of residuals and fitted values)</strong></span> A direct consequence of this fact is that the sample linear correlation between <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> is zero, so a simple linear regression of <span class="math inline">\(\boldsymbol{e}\)</span> as a function of <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> has zero intercept and zero slope. The same is true for any plot of <span class="math inline">\(\boldsymbol{e}\)</span> against a column of <span class="math inline">\(\mathbf{X}\)</span>. Any additional pattern visible must come from omitted dependence.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-zerocor" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-zerocor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-zerocor-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-zerocor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.13: Plot of residuals against fitted values (left), and against the explanatory variable <code>service</code> (right) for the linear regression of the <code>college</code> data. The intercept and the slope of the simple linear regressions are zero.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="cor-zeromean" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4.2 (Mean of residuals)</strong></span> Since the inner product between the model matrix <span class="math inline">\(\mathbf{X}\)</span> and the residuals <span class="math inline">\(\boldsymbol{e}\)</span> is zero, the sample mean of <span class="math inline">\(\boldsymbol{e}\)</span> must be zero provided that <span class="math inline">\(\mathbf{1}_n\)</span> is in the linear span of <span class="math inline">\(\mathbf{X}\)</span>, which is the case as soon as we include an intercept.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> sex <span class="sc">+</span> field <span class="sc">+</span> rank <span class="sc">+</span> service, <span class="at">data =</span> college)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Zero correlations</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(college, <span class="fu">cor</span>(<span class="fu">resid</span>(mod), service))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 6.04e-17</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">resid</span>(mod), <span class="fu">fitted</span>(mod))</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.19e-16</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean zero errors</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">resid</span>(mod))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1.04e-15</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="rem-invariance" class="proof remark">
<p><span class="proof-title"><em>Remark 4.8</em> (Invariance). </span>One direct consequence of the definition of the estimator in terms of projection matrices is that the fitted values <span class="math inline">\(\widehat{y}_i\)</span> for two model matrices <span class="math inline">\(\mathbf{X}_a\)</span> and <span class="math inline">\(\mathbf{X}_b\)</span>, are the same if they generate the same linear span, as in <a href="#exm-baumann-dummies" class="quarto-xref">Example&nbsp;<span>4.10</span></a>. The interpretation of the coefficients will however change. If we include an intercept term, then we get the same output if the columns of explanatory are mean-centered.</p>
</div>
<p>The value of <span class="math inline">\(\boldsymbol{\beta}\)</span> is such that it will maximize the correlation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\widehat{Y}\)</span>. In the case of a single categorical variable, we will obtain fitted values <span class="math inline">\(\widehat{y}\)</span> that correspond to the sample mean of each group.</p>
</div>
<section id="residuals" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="residuals"><span class="header-section-number">4.6.1</span> Residuals</h3>
<p>Residuals are predictions of the errors <span class="math inline">\(\varepsilon\)</span> and represent the difference between the observed value <span class="math inline">\(Y_i\)</span> and the estimated value on the line. The ordinary residuals are <span class="math display">\[\begin{align*}
e_i=Y_i-\widehat{Y}_i, \qquad i =1, \ldots, n.
\end{align*}\]</span> The sum of the ordinary residuals is always zero by construction if the model includes an intercept, meaning <span class="math inline">\(\overline{e} = 0\)</span>.</p>
<p>Not all observations contribute equally to the adjustment of the fitted hyperplane. The geometry of least squares shows that the residuals are orthogonal to the fitted values, and <span class="math inline">\(\boldsymbol{e} = (\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\boldsymbol{Y}\)</span>, where <span class="math inline">\(\mathbf{H}_{\mathbf{X}}=\mathbf{X}(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\)</span> is an <span class="math inline">\(n \times n\)</span> projection matrix that spans the <span class="math inline">\(p\)</span>-dimensional linear combination of the columns of <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathscr{S}(\mathbf{X})\)</span>. If <span class="math inline">\(\mathsf{Va}(\boldsymbol{Y}) = \sigma^2\mathbf{I}_n\)</span>, it follows that <span class="math inline">\(\mathsf{Va}(\boldsymbol{e})=\sigma^2(\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\)</span> because <span class="math inline">\((\mathbf{I}_n-\mathbf{H}_{\mathbf{X}})\)</span> is a projection matrix, therefore idempotent and symmetric. Because the matrix has rank <span class="math inline">\(n-p\)</span>, the ordinary residuals cannot be independent from one another.</p>
<p>If the errors are independent and homoscedastic, the ordinary residual <span class="math inline">\(e_i\)</span> has variance <span class="math inline">\(\sigma^2(1-h_{i})\)</span>, where the leverage term <span class="math inline">\(h_i =(\mathbf{H}_{\mathbf{X}})_{ii} = \mathbf{x}_i (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th diagonal entry of the projection matrix <span class="math inline">\((\mathbf{H}_{\mathbf{X}})\)</span> and <span class="math inline">\(\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th row of the model matrix corresponding to observation <span class="math inline">\(i\)</span>.</p>
<p>We thus conclude that ordinary residuals do not all have the same standard deviation and they are not independent. This is problematic, as we cannot make meaningful comparisons: points with low leverage are bound to deviate more from the fitted model than others. To palliate to this, we can standardize the residuals so each has the same variance under the null of independent homoscedastic errors — the leverage terms <span class="math inline">\(h_i\)</span> are readily calculated from the model matrix <span class="math inline">\(\mathbf{X}\)</span>. The only remaining question is how to estimate the variance. If we use the <span class="math inline">\(i\)</span>th observation to estimate both the residual and the variance, we introduce additional dependence. A better way is remove the <span class="math inline">\(i\)</span>th observation and refit the model with the <span class="math inline">\(n-1\)</span> remaining observations to get of <span class="math inline">\(s^2_{(-i)}\)</span> (there are tricks and closed-form expressions for these, so one doesn’t need to fit <span class="math inline">\(n\)</span> different linear models). The jacknife studentized residual <span class="math inline">\(r_i = e_i/\{s_{(-i)}(1-h_i)\}\)</span>, also termed externally studentized residuals, are not independent, but they are identically distributed and follow a Student distribution with <span class="math inline">\(n-p-2\)</span> degrees of freedom. These can be obtained in <strong>R</strong> with the command <code>rstudent</code>.</p>
<p>When to use which residuals? By construction, the vector of ordinary residuals <span class="math inline">\(\boldsymbol{e}\)</span> is orthogonal to the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> and also to each column of the model matrix <span class="math inline">\(\mathbf{X}\)</span>: this means a simple linear regression of <span class="math inline">\(\boldsymbol{e}\)</span> with any of these as covariate gives zero intercept and zero slope. However, residual patterns due to forgotten interactions, nonlinear terms, etc. could be picked up from pair plots of ordinary residuals against the explanatories.</p>
<p>While the jackknife studentized residuals <span class="math inline">\(r_i\)</span> are not orthogonal, they are not very different. Use jackknife residuals <span class="math inline">\(\boldsymbol{r}\)</span> to check for equality of variance and distributional assumptions (e.g., using quantile-quantile plots).</p>
<p>One thus typically uses ordinary residuals <span class="math inline">\(\boldsymbol{e}\)</span> for plots of fitted values/explanatories against residuals and otherwise jackknife studentized residuals for any other graphical diagnostic plot.</p>
<div id="def-r2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4.5 (Coefficient of determination)</strong></span> When we specify a model, the error term <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> accounts for the fact no perfect linear relationship characterizes the data (if it did, we wouldn’t need statistic to begin with). Once we have fitted a model, we estimate the variance <span class="math inline">\(\sigma^2\)</span>; one may then wonder which share of the total variance in the sample is explained by the model.</p>
<p>The total sum of squares, defined as the sum of squared residuals from the intercept-only model, serves as comparison — the simplest model we could come up with would involving every observation by the sample mean of the response and so this gives (up to scale) the variance of the response, <span class="math inline">\(\mathsf{SS}_c = \sum_{i=1}^n (y_i - \overline{y})^2\)</span>. We can then compare the variance of the original data with that of the residuals from the model with model matrix <span class="math inline">\(\mathbf{X}\)</span>, defined as <span class="math inline">\(\mathsf{SS}_e =\sum_{i=1}^n e_i^2\)</span> with <span class="math inline">\(e_i = y_i - \widehat{\beta}_0 - \sum_{j=1}^p \widehat{\beta}_jX_j\)</span>. We define the coefficient of determination, or squared multiple correlation coefficient of the model, <span class="math inline">\(R^2\)</span>, as <span class="math display">\[\begin{align*}
R^2 &amp;=1- \frac{\mathsf{SS}_e}{\mathsf{SS}_c} = 1 - \frac{sum_{i=1}^n (y_i - \widehat{y}_i)^2}{\sum_{i=1}^n (y_i - \overline{y})^2}.
\end{align*}\]</span> An alternative decomposition shows that <span class="math inline">\(R^2 = \mathsf{cor}^2(\boldsymbol{y}, \widehat{\boldsymbol{y}})\)</span>, i.e., the coefficient of determination can be interpreted as the square of <a href="#def-correlation-Pearson">Pearson’s linear correlation</a> between the response <span class="math inline">\(\boldsymbol{y}\)</span> and the fitted values <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span>.</p>
</div>
<p>Its important to note that <span class="math inline">\(R^2\)</span> is not a goodness-of-fit criterion, like the log likelihood: some phenomena are inherently noisy and even a good model will fail to account for much of the response’s variability. Moreover, one can inflate the value of <span class="math inline">\(R^2\)</span> by including more explanatory variables and making the model more complex, thereby improving the likelihood and <span class="math inline">\(R^2\)</span>. Indeed, the coefficient is non-decreasing in the dimension of <span class="math inline">\(\mathbf{X}\)</span>, so a model with <span class="math inline">\(p+1\)</span> covariate will necessarily have a higher <span class="math inline">\(R^2\)</span> values than only <span class="math inline">\(p\)</span> of the explanatories. For model comparisons, it is better to employ information criteria or else rely on the predictive performance if this is the purpose of the regression. Lastly, a model with a high <span class="math inline">\(R^2\)</span> may imply high correlation, but <a href="http://www.tylervigen.com/spurious-correlations">the relation may be spurious</a>: linear regression does not yield causal models!</p>
</section>
<section id="collinearity" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="collinearity"><span class="header-section-number">4.6.2</span> Collinearity</h3>
<p>The linearity assumption can be interpreted broadly to mean that all relevant covariates have been included and that their effect is correctly specified in the equation of the mean. Adding superfluous covariates to a model has limited impact: if the (partial) correlation between a column vector <span class="math inline">\(\mathbf{X}_k\)</span> and the response variable <span class="math inline">\(\boldsymbol{Y}\)</span> is zero, then <span class="math inline">\(\beta_k=0\)</span> and the estimated coefficient <span class="math inline">\(\widehat{\beta}_k \approx 0\)</span> because the least square estimators are unbiased. If we include many useless variables, say <span class="math inline">\(k\)</span>, the lack of parsimony can however make interpretation more difficult. The price to pay for including the <span class="math inline">\(k\)</span> additional covariates is an increase in the variance of the estimators <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span>.</p>
<p>It is nevertheless preferable to include more variables than to forget key predictors: if we omit an important predictor, their effect may be picked up by other regressors (termed <strong>confounders</strong>) in the model with are correlated with the omitted variable. The interpretation of the other effects can be severely affected by confounders. For example, the simple linear model (or two-sample <span class="math inline">\(t\)</span>-test) for salary as a function of sex for the <code>college</code> data is invalid because sex is a confounder for rank. Since there are more men than women full professor, the mean salary difference between men and women is higher than it truly is. One way to account for this is to include control variables (such as rank), whose effect we need not be interested in, but that are necessary for the model to be adequate. We could also have used stratification, i.e., tested for wage discrimination within each academic rank. This is the reason why sociodemographic variables (sex, age, education level, etc.) are collected as part of studies.</p>
<p>A linear model is not a <a href="https://xkcd.com/552/">causal model</a>: all it does is capture the linear correlation between an explanatory variable and the response. When there are more than one explanatory, the effect of <span class="math inline">\(x_j\)</span> given what has not already been explained by <span class="math inline">\(\boldsymbol{X}_{-j}\)</span>. Thus, if we fail to reject <span class="math inline">\(\mathscr{H}_0:\beta_j=0\)</span> in favor of the alternative <span class="math inline">\(\mathscr{H}_1: \beta_j \neq 0\)</span>, we can only say that there is no significant <em>linear</em> association between <span class="math inline">\(x_j\)</span> and <span class="math inline">\(Y\)</span> once the effect of other variables included in the model has been accounted for. There are thus two scenarios: either the response is uncorrelated with <span class="math inline">\(x_j\)</span> (uninteresting case, but easy to pick up by plotting both or computing linear correlation), or else there is a strong correlation between <span class="math inline">\(x_j\)</span> and both the response <span class="math inline">\(Y\)</span> as well as (some) of the other explanatory variables <span class="math inline">\(x_1, \ldots, x_p\)</span>. This problem is termed (multi)collinearity.</p>
<p>One potential harm of collinearity is a decrease in the precision of parameter estimators. With collinear explanatories, many linear combinations of the covariates represent the response nearly as well. Due to the (near) lack of identifiability, the estimated coefficients become numerically unstable and this causes an increase of the standard errors of the parameters. The predicted or fitted values are unaffected. Generally, collinearity leads to high estimated standard errors and the regression coefficients can change drastically when new observations are included in the model, or when we include or remove explanatories. The individual <span class="math inline">\(\beta\)</span> coefficients may not be statistically significant, but the global <span class="math inline">\(F\)</span>-test will indicate that some covariates are relevant for explaining the response. This however would also be the case if there are predictors with strong signal, so neither is likely to be useful to detect issues.</p>
<p>The added-variable plot shows the relation between the response <span class="math inline">\(Y\)</span> and an explanatory <span class="math inline">\(x_j\)</span> after accounting for other variables: the slope <span class="math inline">\(\widehat{\beta}_j\)</span> of the simple linear regression is the same of the full model. A similar idea can be used to see how much of <span class="math inline">\(x_j\)</span> is already explained by the other variables. For a given explanatory variable <span class="math inline">\(x_j\)</span>, we define its <strong>variance inflation factor</strong> as <span class="math inline">\(\mathsf{VIF}(j)=(1-R^2(j))^{-1}\)</span>, where <span class="math inline">\(R^2(j)\)</span> is the coefficient of determination of the model obtained by regressing <span class="math inline">\(x_j\)</span> on all the other explanatory variables, i.e., <span class="math display">\[\begin{align*}
x_j = \beta^{\star}_0 + \beta^{\star}_1 x_1 + \cdots + \beta^{\star}_{j-1} x_{j-1} + \beta^{\star}_{j+1} x_{j+1} + \cdots + \beta^{\star}_px_p + \varepsilon^{\star}
\end{align*}\]</span> By definition, <span class="math inline">\(R^2(j)\)</span> represents the proportion of the variance of <span class="math inline">\(x_j\)</span> that is explained by all the other predictor variables. Large variance inflation factors are indicative of problems (typically covariates with <span class="math inline">\(\mathsf{VIF}&gt;10\)</span> require scrutinity, and values in the hundreds or more indicate serious problems).</p>
<p>Added-variable plots can also serve as diagnostics, by means of comparison of the partial residuals with a scatterplot of the pair <span class="math inline">\((Y, x_j)\)</span>; if the latter shows very strong linear relation, but the slope is nearly zero in the added-variable plot, this hints that collinearity is an issue.</p>
<p>What can one do about collinearity? If the goal of the study is to develop a predictive model and we’re not interested in the parameters themselves, then we don’t need to do anything. Collinearity is not a problem for the overall model: it’s only a problem for the individual effects of the variables. Their joint effect is still present in the model, regardless of how the individual effects are combined.</p>
<p>If we are interested in individual parameter estimates, for example, to see how (and to what extent) the predictor variables explain the behaviour of <span class="math inline">\(Y\)</span>, then things get more complicated. Collinearity only affects the variables that are strongly correlated with one another, so we only care if it affects one or more of the variables of interest. There sadly is no good solution to the problem. One could</p>
<ul>
<li>try to obtain more data, so as to reduce the effects of collinearity appearing in specific samples or that are due to small sample size.</li>
<li>create a composite score by somehow combining the variables showing collinearity.</li>
<li>remove one or more of the collinear variables. You need to be careful when doing this not to end up with a misspecified model.</li>
<li>use penalized regression. If <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span> is (nearly) not invertible, this may restore the uniqueness of the solution. Penalties introduce bias, but can reduce the variance of the estimators <span class="math inline">\(\boldsymbol{\beta}\)</span>. Popular choices include ridge regression (with an <span class="math inline">\(l_2\)</span> penalty), lasso (<span class="math inline">\(l_1\)</span> penalty), but these require adjustment in order to get valid inference.</li>
</ul>
<p>Whatever the method, it’s important to understand that it can be very difficult (and sometimes impossible) to isolate the individual effect of a predictor variable strongly correlated with other predictors.</p>
<div id="exm-collegedatcollinear" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.23 (Collinearity in the <code>college</code> data)</strong></span> We consider the <code>college</code> data analysis and include all the covariates in the database, including <code>years</code>, the number of years since PhD. One can suspect that, unless a professor started his or her career elsewhere before moving to the college, they will have nearly the same years of service. In fact, the correlation between the two variables, <code>service</code> and <code>years</code> is 0.91. The variance inflation factor for the five covariates</p>
<p>For categorical variables, the variance inflation factor definition would normally yield for each level a different value; an alternative is the generalized variance inflation factor <span class="citation" data-cites="Fox:1992">(<a href="references.html#ref-Fox:1992" role="doc-biblioref">Fox and Monette 1992</a>)</span>. Here, we are interested in gender disparities, so the fact that both service and field are strongly correlated is not problematic, since the <span class="math inline">\(\mathsf{VIF}\)</span> for <span class="math inline">\(\texttt{sex}\)</span> is not high and the other variables are there to act as control and avoid confounders.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<caption>(Generalized) variance inflation factor for the <span class="math inline">\(\texttt{college}\)</span> data.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">service</th>
<th style="text-align: right;">years</th>
<th style="text-align: right;">rank</th>
<th style="text-align: right;">sex</th>
<th style="text-align: right;">field</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">5.92</td>
<td style="text-align: right;">7.52</td>
<td style="text-align: right;">2.01</td>
<td style="text-align: right;">1.03</td>
<td style="text-align: right;">1.06</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="leverage-and-outliers" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="leverage-and-outliers"><span class="header-section-number">4.6.3</span> Leverage and outliers</h3>
<p>The leverage <span class="math inline">\(h_i\)</span> of observation <span class="math inline">\(i\)</span> measures its impact on the least square fit, since we can write <span class="math inline">\(h_i = \partial \widehat{y}_i/\partial y_i\)</span>. Leverage values tell us how much each point impacts the fit: they are strictly positive, are bounded below by <span class="math inline">\(1/n\)</span> and above by <span class="math inline">\(1\)</span>. The sum of the leverage values is <span class="math inline">\(\sum_{i=1}^n h_i=p+1\)</span>: in a good design, each point has approximately the same contribution, with average weight <span class="math inline">\((p+1)/n\)</span>.</p>
<p>Points with high leverage are those that have unusual combinations of explanatories. An influential observation (<span class="math inline">\(h_i\approx 1\)</span>) pulls the fitted hyperplane towards itself so that <span class="math inline">\(\hat{y}_i \approx y_i\)</span>. As a rule of thumb, points with <span class="math inline">\(h_i&gt; 2(p+1)/n\)</span> should be scrutinized.</p>
<p>It is important to distinguish betwen <strong>influential</strong> observations (which have unusual <span class="math inline">\(\mathbf{x}\)</span> value, i.e., far from the overall mean) and <strong>outliers</strong> (unusual value of the response <span class="math inline">\(y\)</span>). If an observation is both an outlier and has a high leverage, it is problematic.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-outliers" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-outliers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-outliers-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-outliers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.14: Outlier and influential observation. The left panel shows an outlier, whereas the right panel shows an influential variable (rightmost <span class="math inline">\(x\)</span> value).
</figcaption>
</figure>
</div>
</div>
</div>
<p>If influential observations can be detected by inspecting the leverage of each observation, outliers are more difficult to diagnose.</p>
<p>An outlier stands out from the rest of the observations, either because it has an usual response value, or because it falls far from the regression surface. Loosely speaking, an outlier is an unusual values of <span class="math inline">\(Y\)</span> for a given combination of <span class="math inline">\(\mathbf{X}\)</span> that stands out from the rest. Outliers can be detected during the exploratory data analysis or picked-up in residual plots (large values of <span class="math inline">\(|e_i|\)</span> in plots of fitted versus residuals) or added-variable plots. One could potentially test whether an jackknife studentized residual is an outlier (adjusting for the fact we would consider only largest values). One can also consider Cook’s distance, <span class="math inline">\(C_j\)</span>, a statistic giving the scaled distance between the fitted values <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> and the fitted values for the model with all but the <span class="math inline">\(j\)</span>th observation, <span class="math inline">\(\hat{\boldsymbol{y}}^{(-j)}\)</span>, <span class="math display">\[\begin{align*}
C_j = \frac{1}{(p+1)S^2} \sum_{i=1}^n \left\{\hat{y}_i - \hat{y}_{i}^{(-j)}\right\}^2
\end{align*}\]</span> Large values of <span class="math inline">\(C_j\)</span> indicate that its residual <span class="math inline">\(e_j\)</span> is large relative to other observations or else its leverage <span class="math inline">\(h_j\)</span> is high. A rule of thumb is to consider points for which <span class="math inline">\(C_j &gt; 4/(n-p-1)\)</span>. In practice, if two observations are outlying and lie in the same region, their Cook distance will be halved.</p>
<p>Outliers and influential observations should not be disregarded because they don’t comply with the model, but require further investigation. They may motivate further modelling for features not accounted for. It is also useful to check for registration errors in the data (which can be safely discarded).</p>
<p>Except in obvious scenarios, unusual observations should not be discarded. In very large samples, the impact of a single outlier is hopefully limited. Transformations of the response may help reduce outlyingness. Otherwise, alternative objective functions (as those employed in robust regression) can be used; these downweight extreme observations, at the cost of efficiency.</p>
</section>
</section>
<section id="model-assumptions-and-diagnostics" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="model-assumptions-and-diagnostics"><span class="header-section-number">4.7</span> Model assumptions and diagnostics</h2>
<p>So far, we have fit models and tested significance of the parameters without checking the model assumptions. The correctness of statements about the <span class="math inline">\(p\)</span>-values and confidence intervals depend on the (approximate) validity of the model assumptions, which all stem from the distributional assumption for the error, assumed to be independent and identically distributed with <span class="math inline">\(\varepsilon_i \stackrel{\cdot}{\sim} \mathsf{normal}(0, \sigma^2)\)</span>. This compact mathematical description can be broken down into four assumptions.</p>
<ul>
<li>linearity: the mean of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\beta_0 + \beta_1x_1 + \cdots + \beta_p x_p\)</span>.</li>
<li>homoscedasticity: the error variance is constant</li>
<li>independence of the errors/observations conditional on covariates.</li>
<li>normality of the errors</li>
</ul>
<p>This section reviews the assumptions made in order to allow statistical inference using the linear model and different residuals that serve as building blocks for graphical diagnostics. We investigate the consequences of violation of these assumptions and outline potential mitigation strategies, many of which are undertaken in other chapters.</p>
<p>When we perform an hypothesis test, we merely fail to reject the null hypothesis, either because the latter is true or else due to lack of evidence. The same goes for checking the validity of model assumptions: scientific reasoning dictates that we cannot know for certain whether these hold true. Our strategy is therefore to use implications of the linear model assumptions to create graphical diagnostic tools, so as to ensure that there is no gross violation of these hypothesis. However, it is important to beware of over-interpreting diagnostic plots: the human eye is very good at finding spurious patterns.</p>
<p>We review the assumptions in turn and discuss what happens when the assumptions fail to hold.</p>
<section id="independence-assumption" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="independence-assumption"><span class="header-section-number">4.7.1</span> Independence assumption</h3>
<p>Usually, the independence of the observations follows directly from the type of sampling used — this assumption is implicitly true if the observations were taken from a <em>random sample</em> from the population. This is generally not the case for longitudinal data, which contains repeated measures from the same individuals across time. Likewise, time series are bound not to have independent observations. If we want to include all the time points in the analysis, we must take into account the possible dependence (correlation) between observations. If we ignore correlation, the estimated standard errors are too small relative to the truth, so the effective sample size is smaller than number of observations.</p>
<p>What is the impact of dependence between measurements? Heuristically, correlated measurements carry less information than independent ones. In the most extreme case, there is no additional information and measurements are identical, but adding them multiple times unduly inflates the statistic and leads to more frequent rejections.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-plotLevelIndep" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plotLevelIndep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-plotLevelIndep-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plotLevelIndep-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.15: Percentage of rejection of the null hypothesis for the <span class="math inline">\(F\)</span>-test of equality of means for the one way ANOVA with data generated with equal mean and variance from an equicorrelation model (within group observations are correlated, between group observations are independent). The nominal level of the test is 5%.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The lack of independence can also have drastic consequences on inference and lead to false conclusions: <a href="#fig-plotLevelIndep" class="quarto-xref">Figure&nbsp;<span>4.15</span></a> shows an example with correlated samples within group (or equivalently repeated measurements from individuals) with 25 observations per group. The <span class="math inline">\(y\)</span>-axis shows the proportion of times the null is rejected when it shouldn’t be. Here, since the data are generated from the null model (equal mean) with equal variance, the inflation in the number of spurious discoveries, false alarm or type I error is alarming and the inflation is substantial even with very limited correlation between measurements.</p>
<p>The first source of dependence is clustered data, meaning measurements taken from subjects that are not independent from one another (family, groups, etc.) More generally, correlation between observations can arises from space-time dependence, roughly categorized into</p>
<ul>
<li>longitudinal data: repeated measurements are taken from the same subjects (few time points)</li>
<li>time series: observations observed at multiple time periods (many time points).</li>
</ul>
<p>Time series require dedicated models not covered in this course. Because of autocorrelation, positive errors tend to be followed by positive errors, etc. We can plot the residuals as a function of time, and a scatterplot of lagged residuals <span class="math inline">\(e_i\)</span> versus <span class="math inline">\(e_{i-1}\)</span> (<span class="math inline">\(i=2, \ldots, n\)</span>).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-timeresidplot" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-timeresidplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-timeresidplot-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-timeresidplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.16: Lagged residual plots: there is no evidence against independence in the left panel, whereas the right panel shows positively correlated residuals.
</figcaption>
</figure>
</div>
</div>
</div>
<p>However, lagged residuals plots only show dependence at lag one between observations. For time series, we can look instead at a correlogram, i.e., a bar plot of the correlation between two observations <span class="math inline">\(h\)</span> units apart as a function of the lag <span class="math inline">\(h\)</span> <span class="citation" data-cites="Brockwell.Davis:2016">(<a href="references.html#ref-Brockwell.Davis:2016" role="doc-biblioref">Brockwell and Davis 2016</a>, Definition 1.4.4)</span>.</p>
<p>For <span class="math inline">\(y_1, \ldots, y_n\)</span> and constant time lags <span class="math inline">\(h=0, 1, \ldots\)</span> units, the autocorrelation at lag <span class="math inline">\(h\)</span> is <span class="math display">\[\begin{align*}
r(h) = \frac{\gamma(h)}{\gamma(0)}, \qquad \gamma(h) = \frac{1}{n}\sum_{i=1}^{n-|h|} (y_i-\overline{y})(y_{i+h}) - \overline{y})
\end{align*}\]</span></p>
<p>If the series is correlated, the sample autocorrelation will likely fall outside of the pointwise confidence intervals, as shown in <a href="#fig-correlogram" class="quarto-xref">Figure&nbsp;<span>4.17</span></a>. Presence of autocorrelation requires modelling the correlation between observations explicitly using dedicated tools from the time series literature. We will however examine <span class="math inline">\(\mathsf{AR}(1)\)</span> models as part of the chapter on longitudinal data. See <a href="https://otexts.com/fpp2/regression-evaluation.html">Forecasting: Principles and Practice, section 5.3</a> for more details.</p>
<p>When observations are positively correlated, the estimated standard errors reported by the software are too small. This means we are overconfident and will reject the null hypothesis more often then we should if the null is true (inflated Type I error, or false positive).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-correlogram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-correlogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-correlogram-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-correlogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.17: Correlogram of independent observations (left) and the ordinary residuals of the log-linear model fitted to the air passengers data (right). While the mean model of the latter is seemingly correctly specified, there is residual dependence between monthly observations and yearly (at lag 12). The blue lines give approximate pointwise 95% confidence intervals for white noise (uncorrelated observations).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="linearity-assumption" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="linearity-assumption"><span class="header-section-number">4.7.2</span> Linearity assumption</h3>
<p>The second assumption of the linear model is that of linearity, which means that the mean model is correctly specified, all relevant covariates have been included and their effect is correctly specified. To check that the response surface of the linear model is adequate, we plot <span class="math inline">\(e_i\)</span> against <span class="math inline">\(\widehat{y}_i\)</span> or <span class="math inline">\(x_{ij}\)</span> (for <span class="math inline">\(j=1, \ldots, p\)</span>). Since the linear correlation between <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{y}}\)</span> (or <span class="math inline">\(\boldsymbol{e}\)</span> and <span class="math inline">\(\mathbf{X}_j\)</span>) is zero by construction, patterns (e.g., quadratic trend, cycles, changepoints) are indicative of misspecification of the mean model. One can add a smoother to detect patterns. <a href="#fig-regdiaglin" class="quarto-xref">Figure&nbsp;<span>4.18</span></a> shows three diagnostics plots, the second of which shows no pattern in the residuals, but skewed fitted values.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-regdiaglin" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regdiaglin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-regdiaglin-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regdiaglin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.18: Scatterplots of residuals against fitted values. The first two plots show no departure from linearity (mean zero). The third plot shows a clear quadratic pattern, suggesting the mean model is misspecified. Note that the distribution of the fitted value need not be uniform, as in the second panel which shows more high fitted values.
</figcaption>
</figure>
</div>
</div>
</div>
<p>If there is residual structure in plots of ordinary residuals against either (a) the fitted values or (b) the explanatory variables, a more complex model can be adjusted including interactions, nonlinear functions, If the effect of an explanatory variable is clearly nonlinear and complicated, smooth terms could be added (we won’t cover generalized additive models in this course).</p>
<p>Plotting residuals against left-out explanatory variables can also serve to check that all of the explanatory power of the omitted covariate is already explained by the columns of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>If an important variable has been omitted and is not available in the dataset, then the effect of that variable is captured by both the errors (the portion orthogonal to the model matrix <span class="math inline">\(\mathbf{X}\)</span>, i.e., unexplained by the covariates included in the model) and the remaining part is captured by other explanatories of the model that are correlated with the omitted variable. These variables can act as confounders. There is little that can be done in either case unless the data for the omitted variable are available, but subject-specific knowledge may help make sense of the results.</p>
</section>
<section id="constant-variance-assumption" class="level3" data-number="4.7.3">
<h3 data-number="4.7.3" class="anchored" data-anchor-id="constant-variance-assumption"><span class="header-section-number">4.7.3</span> Constant variance assumption</h3>
<p>If the variance of the errors is the same for all observations (homoscedasticity), that of the observations <span class="math inline">\(Y\)</span> is also constant. The most common scenarios for heteroscedasticity are increases in variance with the response, or else variance that depends on explanatory variables <span class="math inline">\(\mathbf{X}\)</span>, most notably categorical variables. For the former, a log-transform (or Box–Cox transformation) can help stabilize the variance, but we need the response to be positive. For the latter, we can explicitly model that variance and we will see how to include different variance per group later on. A popular strategy in the econometrics literature, is to use robust (inflated) estimators of the standard errors such as <a href="https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors">White’s sandwich estimator of the variance</a>.</p>
<p>If the residuals (or observations) are heteroscedastic (non constant variance), the estimated effects of the variables (the <span class="math inline">\(\beta\)</span> parameters) are still valid in the sense that the ordinary least squares estimator <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is unbiased. However, the estimated standard errors of the <span class="math inline">\(\widehat{\beta}\)</span> are no longer reliable and, consequently, the confidence intervals and the hypothesis tests for the model parameters will be incorrect. Indeed, if the variance of the errors differs from one observation to the next, we will estimate an average of the different variance terms. The standard errors of each term are incorrect (too small or too large) and the conclusions of the tests (<span class="math inline">\(p\)</span>-values) will be off because the formulas of both <span class="math inline">\(t\)</span>-test and <span class="math inline">\(F\)</span>-test statistics include estimates of <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<p>Looking at the plot of jackknife studentized residuals against regressors (or fitted values) is instructive — for example, we often see a funnel pattern when there is an increase in variance in the plot of the jackknife studentized residuals against fitted value, or else in boxplots with a categorical variable as in <a href="#fig-diagfitvalhomosce" class="quarto-xref">Figure&nbsp;<span>4.20</span></a>. However, if we want to fit a local smoother to observe trends, it is better to plot the absolute value of the jackknife studentized residuals against regressors or observation number.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-residhomoscedastic" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residhomoscedastic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-residhomoscedastic-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residhomoscedastic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.19: Plot of the absolute value of jackknife studentized residuals against observation number. The left panel is typical of homoscedastic data, whereas the right panel indicates an increase in the variance.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-diagfitvalhomosce" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diagfitvalhomosce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-diagfitvalhomosce-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagfitvalhomosce-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.20: Plot of jackknife studentized residuals against fitted value (left) and categorical explanatory (right). Both clearly display heteroscedasticity.
</figcaption>
</figure>
</div>
</div>
</div>
<p>An obvious extension of the linear model is to allow variance to vary according to explanatories, typically categorical covariates. In a likelihood framework, this is easy to do and we will cover this approach in more detail.</p>
<p>We can perform hypothesis tests for the homogeneity (equal) variance assumption. The most commonly used tests are Bartlett’s test, the likelihood ratio test under the assumption of normally distributed data, with a Bartlett correction to improve the <span class="math inline">\(\chi^2\)</span> approximation to the null distribution. The second most popular is Levene’s test (a more robust alternative, less sensitive to outliers). For both tests, the null distribution is <span class="math inline">\(\mathscr{H}_0: \sigma^2_1 = \cdots = \sigma^2_K\)</span> against the alternative that at least two differ. The Bartlett test statistic has a <span class="math inline">\(\chi^2\)</span> null distribution with <span class="math inline">\(K-1\)</span> degrees of freedom, whereas Levene’s test has an <span class="math inline">\(F\)</span>-distribution with (<span class="math inline">\(K-1\)</span>, <span class="math inline">\(n-K\)</span>) degrees of freedom: it is equivalent to computing the one-way ANOVA <span class="math inline">\(F\)</span>-statistic with the absolute value of the centered residuals, <span class="math inline">\(|y_{ik} - \widehat{\mu}_k|\)</span>, as observations.</p>
<p>What are the impacts of unequal variance if we use the <span class="math inline">\(F\)</span>-test instead? For one, the pooled variance will be based on a weighted average of the variance in each group, where the weight is a function of the sample size. This can lead to size distortion (meaning that the proportion of type I error is not the nominal level <span class="math inline">\(\alpha\)</span> as claimed) and potential loss of power. The following toy example illustrates this.</p>
<div id="exm-heterogeneity" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.24 (Violation of the null hypothesis of equal variance)</strong></span> &nbsp;</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-simuWelchnull" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simuWelchnull-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-simuWelchnull-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simuWelchnull-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.21: Histogram of the null distribution of <span class="math inline">\(p\)</span>-values obtained through simulation using the classical analysis of variance <span class="math inline">\(F\)</span>-test (left) and Welch’s unequal variance alternative (right), based on 10 000 simulations. Each simulated sample consist of 50 observations from a <span class="math inline">\(\mathsf{normal}(0, 1)\)</span> distribution and 10 observations from <span class="math inline">\(\mathsf{normal}(0, 9)\)</span>. The uniform distribution would have 5% in each of the 20 bins used for the display.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We consider for simplicity a problem with <span class="math inline">\(K=2\)</span> groups, which is the two-sample <span class="math inline">\(t\)</span>-test. We simulated 50 observations from a <span class="math inline">\(\mathsf{normal}(0, 1)\)</span> distribution and 10 observations from <span class="math inline">\(\mathsf{normal}(0, 9)\)</span>, comparing the distribution of the <span class="math inline">\(p\)</span>-values for the Welch and the <span class="math inline">\(F\)</span>-test statistics. <a href="#fig-simuWelchnull" class="quarto-xref">Figure&nbsp;<span>4.21</span></a> shows the results. The percentage of <span class="math inline">\(p\)</span>-values less than <span class="math inline">\(\alpha=0.05\)</span> based on 10 000 replicates is estimated to be 4.76% for the Welch statistic, not far from the level. By contrast, we reject 28.95% of the time with the one-way ANOVA global <span class="math inline">\(F\)</span>-test: this is a large share of innocents sentenced to jail based on false premises! While the size distortion is not always as striking, heterogeneity should be accounted in the design by requiring sufficient sample sizes (whenever costs permits) in each group to be able to estimate the variance reliably and using an adequate statistic.</p>
</div>
<p>There are alternative graphical ways of checking the assumption of equal variance, many including the standardized residuals <span class="math inline">\(r_{ik} = (y_{ik} - \widehat{\mu}_k)/\widehat{\sigma}\)</span> against the fitted values <span class="math inline">\(\widehat{\mu}_k\)</span>. We will cover these in later sections.</p>
<p>Oftentimes, unequal variance occurs because the model is not additive. You could use variance-stabilizing transformations (e.g., log for multiplicative effects) to ensure approximately equal variance in each group. Another option is to use a model that is suitable for the type of response you have (including count and binary data). Lastly, it may be necessary to explicitly model the variance in more complex design (including repeated measures) where there is a learning effect over time and variability decreases as a result. Consult an expert if needed.</p>
</section>
<section id="normality-assumption" class="level3" data-number="4.7.4">
<h3 data-number="4.7.4" class="anchored" data-anchor-id="normality-assumption"><span class="header-section-number">4.7.4</span> Normality assumption</h3>
<p>The normality assumption is mostly for convenience: if the errors are assumed normally distributed, then the least square and the maximum likelihood estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> coincide. The maximum likelihood estimators of <span class="math inline">\(\boldsymbol{\beta}\)</span> are asymptotically normal under mild conditions on the model matrix and <span class="math inline">\(t\)</span>-tests are surprisingly robust and unaffected by departure from the normality assumption. This means that inference is valid in large samples, regardless of the distribution of the errors/residuals (even if the null distribution are not exact). It is important to keep in mind that, for categorical explanatory variables, the sample size in each group must be sufficiently large for the central limit theorem to kick in since coefficients represent group average.</p>
<p>Sometimes, transformations can improve normality: if the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate <a href="#sec-transfo" class="quarto-xref"><span>Section 4.8.1</span></a>. This can be assessed by looking at the quantile-quantile plot of the externally studentized residuals. If the response <span class="math inline">\(Y\)</span> is not continuous (including binary, proportion or count data), linear models give misleading answers and generalized linear models are more suitable.</p>
<p>The inference will be valid for large samples even if the errors are not normally distributed by virtue of the central limit theorem. If the errors <span class="math inline">\(\varepsilon_i \sim \mathsf{normal}(0, \sigma^2)\)</span>, then the jacknnife studentized residuals should follow a Student distribution, with <span class="math inline">\(r_i \sim \mathsf{Student}(n-p-2)\)</span>, (identically distributed, but not independent). A Student quantile-quantile plot can thus be used to check the assumption (and for <span class="math inline">\(n\)</span> large, the normal plotting positions could be used as approximation if <span class="math inline">\(n-p&gt; 50\)</span>). One can also plot a histogram of the residuals. Keep in mind that if the mean model is not correctly specified, some residuals may incorporate effect of leftover covariates.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-qqplotresid" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qqplotresid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-qqplotresid-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qqplotresid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.22: Histogram (left) and Student quantile-quantile plot (right) of the jackknife studentized residuals. The left panel includes a kernel density estimate (black), with the density of Student distribution (blue) superimposed. The right panel includes pointwise 95% confidence bands calculated using a bootstrap.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Quantile-quantile plots are discussed in <a href="introduction.html#def-qqplot" class="quarto-xref">Definition&nbsp;<span>1.17</span></a> but their interpretation requires training. For example, <a href="#fig-qqplotsbad" class="quarto-xref">Figure&nbsp;<span>4.23</span></a> shows many common scenarios that can be diagnosed using quantile-quantile plots: discrete data is responsible for staircase patterns, positively skewed data has too high low quantiles and too low high quantiles relative to the plotting positions, heavy tailed data have high observations in either tails and bimodal data leads to jumps in the plot.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-qqplotsbad" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-qqplotsbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-qqplotsbad-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-qqplotsbad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.23: Quantile-quantile plots of non-normal data, showing typical look of behaviour of discrete (top left), heavy tailed (top right), skewed (bottom left) and bimodal data (bottom right).
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-diagplotcollege" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.25 (Diagnostic plots for the <span class="math inline">\(\texttt{college}\)</span> data.)</strong></span> We can look at the <code>college</code> data to see if the linear model assumptions hold.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-diagplotscollege" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diagplotscollege-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-diagplotscollege-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diagplotscollege-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.24: Diagnostic plots for the college data example: ordinary residuals against fitted values (top left), absolute value of the jacknnife studentized residuals against fitted values (top right), box and whiskers plot of jacknnife studentized residuals (bottom left) and detrended Student quantile-quantile plot (bottom right). There is clear group heteroscedasticity.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Based on the plots of <a href="#fig-diagplotscollege" class="quarto-xref">Figure&nbsp;<span>4.24</span></a>, we find that there is residual heteroscedasticity, due to rank. Since the number of years in the first rank is limited and all assistant professors were hired in the last six years, there is less disparity in their income. It is important not to mistake the pattern on the <span class="math inline">\(x\)</span>-axis for the fitted value (due to the large effect of rank and field, both categorical variable) with patterns in the residuals (none apparent). Fixing the heteroscedasticity would correct the residuals and improve the appearance of the quantile-quantile plot.</p>
</div>
</section>
</section>
<section id="extensions-of-the-model" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="extensions-of-the-model"><span class="header-section-number">4.8</span> Extensions of the model</h2>
<section id="sec-transfo" class="level3" data-number="4.8.1">
<h3 data-number="4.8.1" class="anchored" data-anchor-id="sec-transfo"><span class="header-section-number">4.8.1</span> Transformation of the response</h3>
<p>If the response is strictly positive, there are some options that can alleviate lack of additivity, more specifically multiplicative mean-variance relationships.If the data is right-skewed and the response is strictly positive, a log-linear model may be more adequate and the parameters can be interpreted. Theory sometimes dictates a multiplicative model: for example, the Cobb–Douglas production function in economics is <span class="math inline">\(P=\alpha L^{\beta_1}C^{\beta_2}\)</span>, where <span class="math inline">\(P\)</span> stands for production, <span class="math inline">\(L\)</span> for labor and <span class="math inline">\(C\)</span> for capital; all inputs are positive, so taking a log-transform yields a model that is linear in <span class="math inline">\(\beta\)</span>, with <span class="math inline">\(\beta_0=\ln(\alpha)\)</span>.</p>
<p>We can rewrite the log-linear model in the original response scale as <span class="math display">\[\begin{align*}
Y &amp;= \exp\left(\beta_0+\sum_{j=1}^p\beta_jX_j +  \varepsilon \right) \\&amp;= \exp\left(\beta_0+ \sum_{j=1}^p\beta_jX_j\right)\cdot \exp(\varepsilon),
\end{align*}\]</span> and thus <span class="math display">\[\begin{align*}
\mathsf{E}(Y \mid \mathbf{X}) = \exp(\beta_0 +\beta_1 X_1 +\cdots + \beta_pX_p) \times \mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}.
\end{align*}\]</span></p>
<p>If <span class="math inline">\(\varepsilon \mid \mathbf{X} \sim \mathsf{normal}(\mu,\sigma^2)\)</span>, then <span class="math inline">\(\mathsf{E}\{\exp(\varepsilon) \mid \mathbf{X}\}= \exp(\mu+\sigma^2/2)\)</span> and <span class="math inline">\(\exp(\varepsilon)\)</span> follows a log-normal distribution.</p>
<p>An increase of one unit of <span class="math inline">\(X_j\)</span> leads to a <span class="math inline">\(\beta_j\)</span> increase of <span class="math inline">\(\ln Y\)</span> without interaction or nonlinear term for <span class="math inline">\(X_j\)</span>, and this translates into a multiplicative increase of a factor <span class="math inline">\(\exp(\beta_j)\)</span> on the original data scale for <span class="math inline">\(Y\)</span>. Indeed, we can compare the ratio of <span class="math inline">\(\mathsf{E}(Y \mid X_1=x+1)\)</span> to <span class="math inline">\(\mathsf{E}(Y \mid X_1=x)\)</span>, <span class="math display">\[\begin{align*}
\frac{\mathsf{E}(Y \mid X_1=x+1, X_2, \ldots, X_p)}{\mathsf{E}(Y \mid X_1=x,  X_2, \ldots, X_p)} = \frac{\exp\{\beta_1(x+1)\}}{\exp(\beta_1 x)} = \exp(\beta_1).
\end{align*}\]</span> Thus, <span class="math inline">\(\exp(\beta_1)\)</span> represents the ratio of the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1=x+1\)</span> in comparison to that when <span class="math inline">\(X_1=x\)</span>, <em>ceteris paribus</em> (and provided this statement is meaningful). If <span class="math inline">\(\beta_j=0\)</span>, the multiplicative factor one is the identity, whereas negative values of the regression coefficient <span class="math inline">\(\beta_j&lt;0\)</span> leads to <span class="math inline">\(\exp(\beta_j)&lt;1\)</span>. The percentage change is <span class="math inline">\(1-\exp(\beta_j)\)</span> if <span class="math inline">\(\beta_j &lt;0\)</span> and <span class="math inline">\(\exp(\beta_j)-1\)</span> if <span class="math inline">\(\beta_j&gt;0\)</span></p>
<p>Sometimes, we may wish to consider a log transformation of both the response and some of the continuous positive explanatories, when this make sense (a so-called log-log model). Consider the case where both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> is log-transformed, so the equation for the mean on the original data scale reads <span class="math display">\[\begin{align*}
Y= X_1^{\beta_1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\end{align*}\]</span> Taking the derivative of the left hand side with respect to <span class="math inline">\(X_1&gt;0\)</span>, we get <span class="math display">\[\begin{align*}
\frac{\partial Y}{\partial X_1}&amp;= \beta_1 X_1^{\beta_1-1}\exp(\beta_0 + \beta_2X_2 + \cdots + \beta_pX_p + \varepsilon)
\\&amp;= \frac{\beta_1 Y}{X_1}
\end{align*}\]</span> and thus we can rearrange the expression so that <span class="math display">\[\begin{align*}
\frac{\partial X_1}{X_1}\beta_1 = \frac{\partial Y}{Y};
\end{align*}\]</span> this is a partial <strong>elasticity</strong>, so <span class="math inline">\(\beta_1\)</span> is interpreted as a <span class="math inline">\(\beta_1\)</span> percentage change in <span class="math inline">\(Y\)</span> for each percentage increase of <span class="math inline">\(X_1\)</span>, <em>ceteris paribus</em>.</p>
<div id="exm-loglog" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.26 (Log-log model)</strong></span> Consider for example the Cobb–Douglas production function <span class="citation" data-cites="Douglas:1976">(<a href="references.html#ref-Douglas:1976" role="doc-biblioref">Douglas 1976</a>)</span>, which specifies that economic output <span class="math inline">\(Y\)</span> is related to labour <span class="math inline">\(L\)</span> and capital <span class="math inline">\(C\)</span> via <span class="math inline">\(\mathsf{E}(Y \mid L, C) = \beta_0C^{\beta}L^{1-\beta}\)</span> with <span class="math inline">\(\beta \in (0,1)\)</span>. If we take logarithms on both sides (since all arguments are positive), then <span class="math inline">\(\mathsf{E}(\ln Y \mid L, C) = \beta_0^* + \beta_1 \ln C + (1-\beta_1)\ln L\)</span>. We could fit a linear model with response <span class="math inline">\(\ln Y - \ln L\)</span> and explanatory variable <span class="math inline">\(\ln C - \ln L\)</span>, to obtain an estimate of the coefficient <span class="math inline">\(\beta_1\)</span>, while <span class="math inline">\(\beta_0^*=\ln \beta_0\)</span>. A constrained optimization would be potentially necessary to estimate the model parameters of the resulting linear model if the estimates lie outside of the parameter space.</p>
</div>
<div id="prp-boxcox" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4.7 (Box–Cox transformation)</strong></span> If the data are strictly positive, one can consider a Box–Cox transformation, <span class="math display">\[\begin{align*}
y(\lambda)= \begin{cases}
(y^{\lambda}-1)/\lambda, &amp; \lambda \neq 0\\
\ln(y), &amp; \lambda=0.
\end{cases}
\end{align*}\]</span> The cases <span class="math inline">\(\lambda=-1\)</span> (inverse), <span class="math inline">\(\lambda=1\)</span> (identity) and <span class="math inline">\(\lambda=0\)</span> (log-linear model) are perhaps the most important because they yield interpretable models.</p>
<p>If we assume that <span class="math inline">\(\boldsymbol{Y}(\lambda) \sim \mathsf{normal}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_n)\)</span>, then the likelihood is <span class="math display">\[\begin{align*}
L(\lambda, \boldsymbol{\beta}, \sigma; \boldsymbol{y}, \mathbf{X}) &amp;= (2\pi\sigma^2)^{-n/2} J(\lambda, \boldsymbol{y}) \times\\&amp; \quad \exp \left[ - \frac{1}{2\sigma^2}\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}^\top\{\boldsymbol{y}(\lambda) - \mathbf{X}\boldsymbol{\beta}\}\right],
\end{align*}\]</span> where <span class="math inline">\(J\)</span> denotes the Jacobian of the Box–Cox transformation, <span class="math inline">\(J(\lambda, \boldsymbol{y})=\prod_{i=1}^n y_i^{\lambda-1}\)</span>. For each given value of <span class="math inline">\(\lambda\)</span>, the maximum likelihood estimator is that of the usual regression model, with <span class="math inline">\(\boldsymbol{y}\)</span> replaced by <span class="math inline">\(\boldsymbol{y}(\lambda)\)</span>, namely <span class="math inline">\(\widehat{\boldsymbol{\beta}}_\lambda = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{y}(\lambda)\)</span> and <span class="math inline">\(\widehat{\sigma}^2_\lambda = n^{-1}\{ \boldsymbol{y}(\lambda) - \mathbf{X}\widehat{\boldsymbol{\beta}}_\lambda\}^\top\{ \boldsymbol{y}(\lambda) - \mathbf{X}\widehat{\boldsymbol{\beta}}_\lambda\}\)</span>.</p>
<p>The profile log likelihood is <span class="math display">\[\begin{align*}
\ell_{\mathsf{p}}(\lambda) = -\frac{n}{2}\ln(2\pi \widehat{\sigma}^2_\lambda) - \frac{n}{2} + (\lambda - 1)\sum_{i=1}^n \ln(y_i)
\end{align*}\]</span> The maximum profile likelihood estimator is the value <span class="math inline">\(\lambda\)</span> minimizes the sum of squared residuals from the linear model with <span class="math inline">\(\boldsymbol{y}(\lambda)\)</span> as response.</p>
<p>The Box–Cox is not a panacea and should be reserved to cases where the transformation reduces heteroscedasticity (unequal variance) or creates a linear relation between explanatories and response: theory provides a cogent explanation of the data. Rather than an <em>ad hoc</em> choice of transformation, one could choose a log transformation if the value <span class="math inline">\(0\)</span> is included within the 95% confidence interval since this improves interpretability.</p>
</div>
<div id="exm-poisonboxcox" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.27 (Box–Cox transform for the <code>poison</code> data)</strong></span> <span class="citation" data-cites="Box.Cox:1964">Box and Cox (<a href="references.html#ref-Box.Cox:1964" role="doc-biblioref">1964</a>)</span> considered survival time for 48 animals based on a randomized trial; these data are analyzed in Example 8.25 of <span class="citation" data-cites="Davison:2003">Davison (<a href="references.html#ref-Davison:2003" role="doc-biblioref">2003</a>)</span>. Three poisons were administered with four treatments; each factor combination contained four animals, chosen at random. There is strong evidence that both the choice of poison and treatment affect survival time.</p>
<p>We could consider a two-way analysis of variance model for these data without interaction, given the few observations for each combination. The model would be of the form <span class="math display">\[\begin{align*}
Y &amp;= \beta_0 + \beta_1 \texttt{poison}_2 + \beta_2\texttt{poison}_3  +\beta_3\texttt{treatment}_2 \\ &amp;\qquad+ \beta_4\texttt{treatment}_3
+\beta_5\texttt{treatment}_4 + \varepsilon
\end{align*}\]</span></p>
<p>The plot of fitted values against residuals shows that the model is not additive; there is also indications that the variance increases with the mean response. The model is inadequate: lowest survival times are underpredicted, meaning the residuals are positive and likewise the middle responses is positive. A formal test of non-additivity based on constructed variables further point towards non-additivity <span class="citation" data-cites="Davison:2003">(<a href="references.html#ref-Davison:2003" role="doc-biblioref">Davison 2003</a>, Example 8.24)</span>. Overall, the model fit is poor and any conclusion drawn from it dubious.</p>
<p>One could consider using a Box–Cox to find a suitable transformation of the residuals so as to improve normality. AN analysis of residuals in the top four plots of <a href="#fig-poisonplots" class="quarto-xref">Figure&nbsp;<span>4.25</span></a> show evidence of heteroscedasticity as a function of either poison and treatment. This is evident by looking at the plot of ordinary residuals, which displays increase in variance with the survival time. The quantile-quantile plot in the middle right plot shows some evidence of departure from the normality, but the non-linearity and heteroscedasticity obscure this.</p>
<p>The bottom left panel of <a href="#fig-poisonplots" class="quarto-xref">Figure&nbsp;<span>4.25</span></a> shows the profile log likelihood for the Box–Cox transform parameter, suggesting a value of <span class="math inline">\(\lambda=-1\)</span> would be within the 95% confidence interval. This choice has the benefit of being interpretable, as the reciprocal response <span class="math inline">\(Y^{-1}\)</span> corresponds to the speed of action of the poison depending on both poison type and treatment. The diagnostics plot at the bottom right of <a href="#fig-poisonplots" class="quarto-xref">Figure&nbsp;<span>4.25</span></a> also indicate that the model for the reciprocal has no residual structure and the variance appears constant. After fitting the same additive model with main effect only to the reciprocal survival time, there is no more evidence of residual structure and unequal variance.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-poisonplots" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisonplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="linearmodels_files/figure-html/fig-poisonplots-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisonplots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.25: Diagnostic plots for the poison data: ordinary residuals (jittered) for the linear model for survival time as a function of poison and treatment (top), fitted values against residuals (middle left), detrended quantile-quantile plot of residuals (middle right), profile log likelihood of <span class="math inline">\(\lambda\)</span> for the Box–Cox model transformation (bottom left) and fitted values against residuals (bottom right) after reciprocal transformation.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="concluding-remarks" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">4.9</span> Concluding remarks</h2>
<p>Linear regression is the most famous and the most widely used statistical model around. The name may appear reductive, but many tests statistics (<em>t</em>-tests, ANOVA, Wilcoxon, Kruskal–Wallis) <a href="https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf">can be formulated using a linear regression</a>, while <a href="https://threadreaderapp.com/thread/1286420597505892352.html">models as diverse as trees, principal components and deep neural networks are just linear regression model in disguise</a>. What changes under the hood between one fancy model to the next are the optimization method (e.g., ordinary least squares, constrained optimization or stochastic gradient descent) and the choice of explanatory variables entering the model (spline basis for nonparametric regression, indicator variable selected via a greedy search for trees, activation functions for neural networks).</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Baumann:1992" class="csl-entry" role="listitem">
Baumann, James F., Nancy Seifert-Kessell, and Leah A. Jones. 1992. <span>“Effect of Think-Aloud Instruction on Elementary Students’ Comprehension Monitoring Abilities.”</span> <em>Journal of Reading Behavior</em> 24 (2): 143–72. <a href="https://doi.org/10.1080/10862969209547770">https://doi.org/10.1080/10862969209547770</a>.
</div>
<div id="ref-Box.Cox:1964" class="csl-entry" role="listitem">
Box, G. E. P., and D. R. Cox. 1964. <span>“An Analysis of Transformations.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 26 (2): 211–43. <a href="https://doi.org/10.1111/j.2517-6161.1964.tb00553.x">https://doi.org/10.1111/j.2517-6161.1964.tb00553.x</a>.
</div>
<div id="ref-Brockwell.Davis:2016" class="csl-entry" role="listitem">
Brockwell, P. J., and R. A. Davis. 2016. <em>Introduction to Time Series and Forecasting</em>. Springer Texts in Statistics. Springer.
</div>
<div id="ref-Crump.Navarro.Suzuki:2019" class="csl-entry" role="listitem">
Crump, M. J. C., D. J. Navarro, and J. Suzuki. 2019. <em>Answering Questions with Data: Introductory Statistics for Psychology Students</em>. <a href="https://doi.org/10.17605/OSF.IO/JZE52">https://doi.org/10.17605/OSF.IO/JZE52</a>.
</div>
<div id="ref-Davison:2003" class="csl-entry" role="listitem">
Davison, A. C. 2003. <em>Statistical Models</em>. Cambridge University Press.
</div>
<div id="ref-Douglas:1976" class="csl-entry" role="listitem">
Douglas, Paul H. 1976. <span>“The <span>Cobb–Douglas</span> Production Function Once Again: Its History, Its Testing, and Some New Empirical Values.”</span> <em>Journal of Political Economy</em> 84 (5): 903–15. <a href="http://www.jstor.org/stable/1830435">http://www.jstor.org/stable/1830435</a>.
</div>
<div id="ref-Fox:1992" class="csl-entry" role="listitem">
Fox, John, and Georges Monette. 1992. <span>“Generalized Collinearity Diagnostics.”</span> <em>Journal of the American Statistical Association</em> 87 (417): 178–83. <a href="https://doi.org/10.1080/01621459.1992.10475190">https://doi.org/10.1080/01621459.1992.10475190</a>.
</div>
<div id="ref-Lee.Choi:2019" class="csl-entry" role="listitem">
Lee, Kiljae, and Jungsil Choi. 2019. <span>“Image-Text Inconsistency Effect on Product Evaluation in Online Retailing.”</span> <em>Journal of Retailing and Consumer Services</em> 49: 279–88. <a href="https://doi.org/10.1016/j.jretconser.2019.03.015">https://doi.org/10.1016/j.jretconser.2019.03.015</a>.
</div>
<div id="ref-Lin.Kim.Uduehi.Keinan:2024" class="csl-entry" role="listitem">
Lin, Jason D, Nicole You Jeung Kim, Esther Uduehi, and Anat Keinan. 2024. <span>“Culture for Sale: Unpacking Consumer Perceptions of Cultural Appropriation.”</span> <em>Journal of Consumer Research</em>. <a href="https://doi.org/10.1093/jcr/ucad076">https://doi.org/10.1093/jcr/ucad076</a>.
</div>
<div id="ref-Moon.VanEpps:2023" class="csl-entry" role="listitem">
Moon, Alice, and Eric M VanEpps. 2023. <span>“Giving Suggestions: Using Quantity Requests to Increase Donations.”</span> <em>Journal of Consumer Research</em> 50 (1): 190–210. <a href="https://doi.org/10.1093/jcr/ucac047">https://doi.org/10.1093/jcr/ucac047</a>.
</div>
<div id="ref-Sharma.Tully.Cryder:2021" class="csl-entry" role="listitem">
Sharma, Eesha, Stephanie Tully, and Cynthia Cryder. 2021. <span>“Psychological Ownership of (Borrowed) Money.”</span> <em>Journal of Marketing Research</em> 58 (3): 497–514. <a href="https://doi.org/10.1177/0022243721993816">https://doi.org/10.1177/0022243721993816</a>.
</div>
<div id="ref-Sokolova:2023" class="csl-entry" role="listitem">
Sokolova, Tatiana, Aradhna Krishna, and Tim Döring. 2023. <span>“Paper Meets Plastic: The Perceived Environmental Friendliness of Product Packaging.”</span> <em>Journal of Consumer Research</em> 50 (3): 468–91. <a href="https://doi.org/10.1093/jcr/ucad008">https://doi.org/10.1093/jcr/ucad008</a>.
</div>
<div id="ref-Venables:2000" class="csl-entry" role="listitem">
Venables, William N. 2000. <span>“Exegeses on Linear Models.”</span> In <em>S-PLUS User’s Conference</em>. Washington, D.C. <a href="https://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf">https://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>The constraint <span class="math inline">\(c_1 + \cdots + c_K=0\)</span> ensures that linear contrasts are orthogonal to the mean, which has weight <span class="math inline">\(c_i=n_i/n\)</span> and for balanced samples <span class="math inline">\(c_i =1/n\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/lbelzile\.github\.io\/math60604a\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./likelihood.html" class="pagination-link" aria-label="Likelihood-based inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Likelihood-based inference</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link" aria-label="Bibliography">
        <span class="nav-page-text">Bibliography</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>All right reserved (Léo Belzile)</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lbelzile/math60604a/edit/master/linearmodels.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>